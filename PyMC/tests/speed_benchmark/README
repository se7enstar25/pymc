Hi guys.


The fastDisasterSamplerModel defines a model equivalent to that in DisasterSampler, but written as fast as possible within PyMC: The scalar log-probabilities are done in pure Python to avoid gateways and without constant likelihood terms, and the data likelihood is handled using a specialized, unsafe, bare-bones Fortran module. 

The idea is to help us see how fast PyMC could possibly be, and where we have bottlenecks if it's made that fast. Of course PyMC won't achieve its optimal speed out of the box, we'll just want to mention in the documentation that readers will almost certainly be able to get performance gains by writing specialized log-probability functions using f2py. The stuff in this directory will just help us get as much overhead out of the user's way as possible. 


On my laptop, test_fast executes in 20s vs. 65s for test_reg, which samples examples/DisasterModel. Profiler results for test_fast:

     1984181 function calls (1984043 primitive calls) in 49.350 CPU seconds

Ordered by: internal time

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
150000   10.260    0.000   40.040    0.000 SamplingMethods.py:137(step)
150000    7.190    0.000   13.130    0.000 :0(normal)
181869    5.660    0.000    5.660    0.000 fastDisasterModel.py:53(disasters)
275882    5.450    0.000   11.110    0.000 SamplingMethods.py:104(_get_loglike)
150000    3.480    0.000    3.480    0.000 :0(any)
 50000    2.890    0.000    7.080    0.000 Model.py:245(tally)
150000    2.580    0.000    2.580    0.000 memory_trace.py:34(tally)
150000    2.510    0.000   15.920    0.000 SamplingMethods.py:167(propose)
150000    2.460    0.000    5.940    0.000 fromnumeric.py:425(any)
     1    2.210    2.210   49.330   49.330 Model.py:276(sample)
 50014    1.010    0.000    1.610    0.000 Model.py:118(__setattr__)
125882    0.990    0.000    0.990    0.000 :0(random_sample)
 74533    0.520    0.000    0.520    0.000 :0(revert)
 64735    0.500    0.000    0.500    0.000 fastDisasterModel.py:25(switchpoint)
 59520    0.400    0.000    0.400    0.000 fastDisasterModel.py:34(early_mean)
 50517    0.360    0.000    0.360    0.000 :0(isinstance)
 50786    0.340    0.000    0.340    0.000 fastDisasterModel.py:44(late_mean)
 50000    0.280    0.000    0.280    0.000 :0(round)
 50062    0.250    0.000    0.250    0.000 :0(has_key)

The main bottleneck seems to be in OneAtATimeMetropolis.step(), which spends most of its time generating normal RVs (the second item). The third item is the log-likelihood method for the data, which should be the bottleneck. Optimizing the _get_loglike loop in SamplingMethod (item 4) and finding a faster normal RV generator seem to be the most obvious places we could buy improvements, other than the log-probability functions.


Anand
