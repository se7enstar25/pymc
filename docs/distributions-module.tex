% Generated by Sphinx.
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{babel}
%\usepackage{times}
%\usepackage[Bjarne]{fncychap}


PyMC provides 35 built-in probability distributions. For each distribution, PyMC provides:
\begin{itemize}
    \item A function that evaluates its log-probability or log-density, for example \code{normal_like()}.
    \item A function that draws random variables, for example \code{rnormal()}.
    \item A function that computes the expectation associated with the distribution, for example \code{normal_expval()}.
    \item A \code{Stochastic} subclass generated from the distribution, for example \code{Normal}.
\end{itemize}

This section describes the likelihood functions of these distributions.
\index{pymc.distributions (module)}
\hypertarget{module-pymc.distributions}{}
%\declaremodule[pymc.distributions]{}{pymc.distributions}

\small

\section{Discrete distributions}
\index{bernoulli\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.bernoulli_like}{}\begin{funcdesc}{bernoulli\_like}{x, p}
The Bernoulli distribution describes the probability of successes (x=1) and
failures (x=0).
\begin{gather}
\begin{split}f(x \mid p) = p^{x} (1-p)^{1-x}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Series of successes (1) and failures (0). $x=0,1$

\item {} 
\emph{p} : Probability of success. $0 < p < 1$.

\end{itemize}

\item[Example] \leavevmode
\begin{Verbatim}[commandchars=@\[\]]
@PYGaQ[@textgreater[]@textgreater[]@textgreater[] ]bernoulli@_like(@PYGZlb[]@PYGaw[0],@PYGaw[1],@PYGaw[0],@PYGaw[1]@PYGZrb[], @PYGbe[.]@PYGaw[4])
@PYGaa[-2.8542325496673584]
\end{Verbatim}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(x)= p$

\item {} 
$Var(x)= p(1-p)$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{binomial\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.binomial_like}{}\begin{funcdesc}{binomial\_like}{x, n, p}
Binomial log-likelihood.  The discrete probability distribution of the
number of successes in a sequence of n independent yes/no experiments,
each of which yields success with probability p.
\begin{gather}
\begin{split}f(x \mid n, p) = \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} Number of successes, \textgreater{} 0.

\item {} 
\emph{n} : {[}int{]} Number of Bernoulli trials, \textgreater{} x.

\item {} 
\emph{p} : Probability of success in each trial, $p \in [0,1]$.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X)=np$

\item {} 
$Var(X)=np(1-p)$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{categorical\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.categorical_like}{}\begin{funcdesc}{categorical\_like}{x, p}
Categorical log-likelihood. The most general discrete distribution.
\begin{gather}
\begin{split}f(x=i \mid p) = p_i\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
for $i \in 0 \ldots k-1$.
\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} $x \in 0\ldots k-1$

\item {} 
\emph{p} : {[}float{]} $p > 0$, $\sum p = 1$

\end{itemize}

\end{description}
\end{funcdesc}
\index{discrete\_uniform\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.discrete_uniform_like}{}\begin{funcdesc}{discrete\_uniform\_like}{x, lower, upper}
Discrete uniform log-likelihood.
\begin{gather}
\begin{split}f(x \mid lower, upper) = \frac{1}{upper-lower}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} $lower \leq x \leq upper$

\item {} 
\emph{lower} : Lower limit.

\item {} 
\emph{upper} : Upper limit (upper \textgreater{} lower).

\end{itemize}

\end{description}
\end{funcdesc}
\index{geometric\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.geometric_like}{}\begin{funcdesc}{geometric\_like}{x, p}
Geometric log-likelihood. The probability that the first success in a
sequence of Bernoulli trials occurs on the x'th trial.
\begin{gather}
\begin{split}f(x \mid p) = p(1-p)^{x-1}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} Number of trials before first success (x \textgreater{} 0).

\item {} 
\emph{p} : Probability of success on an individual trial, $p \in [0,1]$

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X)=1/p$

\item {} 
$Var(X)=\frac{1-p}{p^2}$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{hypergeometric\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.hypergeometric_like}{}\begin{funcdesc}{hypergeometric\_like}{x, n, m, N}
Hypergeometric log-likelihood. Discrete probability distribution that
describes the number of successes in a sequence of draws from a finite
population without replacement.
\begin{gather}
\begin{split}f(x \mid n, m, N) = \frac{\binom{m}{x}\binom{N-m}{n-x}}{\binom{N}{n}}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} Number of successes in a sample drawn from a population.

\item {} 
\emph{n} : {[}int{]} Size of sample drawn from the population.

\item {} 
\emph{m} : {[}int{]} Number of successes in the population.

\item {} 
\emph{N} : {[}int{]} Total number of units in the population.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}
$E(X) = \frac{n n}{N}$
\end{notice}
\end{funcdesc}
\index{negative\_binomial\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.negative_binomial_like}{}\begin{funcdesc}{negative\_binomial\_like}{x, mu, alpha}
Negative binomial log-likelihood. The negative binomial distribution describes a
Poisson random variable whose rate parameter is gamma distributed. PyMC's chosen
parameterization is based on this mixture interpretation.
\begin{gather}
\begin{split}f(x \mid \mu, \alpha) = \frac{\Gamma(x+\alpha)}{x! \Gamma(\alpha)} (\alpha/(\mu+\alpha))^\alpha (\mu/(\mu+\alpha))^x\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input data (x \textgreater{} 0).

\item {} 
\emph{mu} : mu \textgreater{} 0

\item {} 
\emph{alpha} : alpha \textgreater{} 0

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E[x]=\mu$

\item {} 
In Wikipedia's parameterization,
$r=\alpha$
$p=\alpha/(\mu+\alpha)$
$\mu=r(1-p)/p$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{poisson\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.poisson_like}{}\begin{funcdesc}{poisson\_like}{x, mu}
Poisson log-likelihood. The Poisson is a discrete probability distribution.
It is often used to model the number of events occurring in a fixed period of
time when the times at which events occur are independent. The Poisson
distribution can be derived as a limiting case of the binomial distribution.
\begin{gather}
\begin{split}f(x \mid \mu) = \frac{e^{-\mu}\mu^x}{x!}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} $x \in {0,1,2,...}$

\item {} 
\emph{mu} : Expected number of occurrences during the given interval, $\mu \geq 0$.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(x)=\mu$

\item {} 
$Var(x)=\mu$

\end{itemize}
\end{notice}
\end{funcdesc}


\section{Continuous distributions}
\index{beta\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.beta_like}{}\begin{funcdesc}{beta\_like}{x, alpha, beta}
Beta log-likelihood. The conjugate prior for the parameter :math: \emph{p} of the binomial distribution.
\begin{gather}
\begin{split}f(x \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : 0 \textless{} x \textless{} 1

\item {} 
\emph{alpha} : alpha \textgreater{} 0

\item {} 
\emph{beta} : beta \textgreater{} 0

\end{itemize}

\item[Example] \leavevmode
\begin{Verbatim}[commandchars=@\[\]]
@PYGaQ[@textgreater[]@textgreater[]@textgreater[] ]beta@_like(@PYGbe[.]@PYGaw[4],@PYGaw[1],@PYGaw[2])
@PYGaa[0.18232160806655884]
\end{Verbatim}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X)=\frac{\alpha}{\alpha+\beta}$

\item {} 
$Var(X)=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{cauchy\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.cauchy_like}{}\begin{funcdesc}{cauchy\_like}{x, alpha, beta}
Cauchy log-likelihood. The Cauchy distribution is also known as the
Lorentz or the Breit-Wigner distribution.
\begin{gather}
\begin{split}f(x \mid \alpha, \beta) = \frac{1}{\pi \beta [1 + (\frac{x-\alpha}{\beta})^2]}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{alpha} : Location parameter.

\item {} 
\emph{beta} : Scale parameter \textgreater{} 0.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
Mode and median are at alpha.

\end{itemize}
\end{notice}
\end{funcdesc}
\index{chi2\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.chi2_like}{}\begin{funcdesc}{chi2\_like}{x, nu}
Chi-squared $\chi^2$ log-likelihood.
\begin{gather}
\begin{split}f(x \mid \nu) = \frac{x^{(\nu-2)/2}e^{-x/2}}{2^{\nu/2}\Gamma(\nu/2)}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : \textgreater{} 0

\item {} 
\emph{nu} : {[}int{]} Degrees of freedom ( nu \textgreater{} 0 )

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X)=\nu$

\item {} 
$Var(X)=2\nu$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{degenerate\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.degenerate_like}{}\begin{funcdesc}{degenerate\_like}{x, k}
Degenerate log-likelihood.
\begin{gather}
\begin{split}f(x \mid k) = \left\{ \begin{matrix} 1 \text{ if } x = k \\ 0 \text{ if } x \ne k\end{matrix} \right.\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input value.

\item {} 
\emph{k} : Degenerate value.

\end{itemize}

\end{description}
\end{funcdesc}
\index{exponential\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.exponential_like}{}\begin{funcdesc}{exponential\_like}{x, beta}
Exponential log-likelihood.

The exponential distribution is a special case of the gamma distribution
with alpha=1. It often describes the time until an event.
\begin{gather}
\begin{split}f(x \mid \beta) = \frac{1}{\beta}e^{-x/\beta}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : x \textgreater{} 0

\item {} 
\emph{beta} : Survival parameter (beta \textgreater{} 0).

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X) = \beta$

\item {} 
$Var(X) = \beta^2$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{exponweib\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.exponweib_like}{}\begin{funcdesc}{exponweib\_like}{x, alpha, k, loc=0, scale=1}
Exponentiated Weibull log-likelihood.

The exponentiated Weibull distribution is a generalization of the Weibull
family. Its value lies in being able to model monotone and non-monotone
failure rates.
\begin{gather}
\begin{split}f(x \mid \alpha,k,loc,scale)  & = \frac{\alpha k}{scale} (1-e^{-z^k})^{\alpha-1} e^{-z^k} z^{k-1} \\
z & = \frac{x-loc}{scale}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : x \textgreater{} 0

\item {} 
\emph{alpha} : Shape parameter

\item {} 
\emph{k} : k \textgreater{} 0

\item {} 
\emph{loc} : Location parameter

\item {} 
\emph{scale} : Scale parameter (scale \textgreater{} 0).

\end{itemize}

\end{description}
\end{funcdesc}
\index{gamma\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.gamma_like}{}\begin{funcdesc}{gamma\_like}{x, alpha, beta}
Gamma log-likelihood.

Represents the sum of alpha exponentially distributed random variables, each
of which has mean beta.
\begin{gather}
\begin{split}f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : math:\emph{x ge 0}

\item {} 
\emph{alpha} : Shape parameter (alpha \textgreater{} 0).

\item {} 
\emph{beta} : Scale parameter (beta \textgreater{} 0).

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X) = \frac{\alpha}{\beta}$

\item {} 
$Var(X) = \frac{\alpha}{\beta^2}$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{half\_normal\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.half_normal_like}{}\begin{funcdesc}{half\_normal\_like}{x, tau}
Half-normal log-likelihood, a normal distribution with mean 0 limited
to the domain $x \in [0, \infty)$.
\begin{gather}
\begin{split}f(x \mid \tau) = \sqrt{\frac{2\tau}{\pi}}\exp\left\{ {\frac{-x^2 \tau}{2}}\right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : $x \ge 0$

\item {} 
\emph{tau} : tau \textgreater{} 0

\end{itemize}

\end{description}
\end{funcdesc}
\index{hypergeometric\_like() (in module pymc.distributions)}

\begin{funcdesc}{hypergeometric\_like}{x, n, m, N}
Hypergeometric log-likelihood. Discrete probability distribution that
describes the number of successes in a sequence of draws from a finite
population without replacement.
\begin{gather}
\begin{split}f(x \mid n, m, N) = \frac{\binom{m}{x}\binom{N-m}{n-x}}{\binom{N}{n}}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int{]} Number of successes in a sample drawn from a population.

\item {} 
\emph{n} : {[}int{]} Size of sample drawn from the population.

\item {} 
\emph{m} : {[}int{]} Number of successes in the population.

\item {} 
\emph{N} : {[}int{]} Total number of units in the population.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}
$E(X) = \frac{n n}{N}$
\end{notice}
\end{funcdesc}
\index{inverse\_gamma\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.inverse_gamma_like}{}\begin{funcdesc}{inverse\_gamma\_like}{x, alpha, beta}
Inverse gamma log-likelihood, the reciprocal of the gamma distribution.
\begin{gather}
\begin{split}f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-\alpha - 1} \exp\left(\frac{-\beta}{x}\right)\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : x \textgreater{} 0

\item {} 
\emph{alpha} : Shape parameter (alpha \textgreater{} 0).

\item {} 
\emph{beta} : Scale parameter (beta \textgreater{} 0).

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}
$E(X)=\frac{\beta}{\alpha-1}$  for $\alpha > 1$
$Var(X)=\frac{\beta^2}{(\alpha-1)^2(\alpha)}$  for $\alpha > 2$
\end{notice}
\end{funcdesc}
\index{laplace\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.laplace_like}{}\begin{funcdesc}{laplace\_like}{x, mu, tau}
Laplace (double exponential) log-likelihood.

The Laplace (or double exponential) distribution describes the
difference between two independent, identically distributed exponential
events. It is often used as a heavier-tailed alternative to the normal.
\begin{gather}
\begin{split}f(x \mid \mu, \tau) = \frac{\tau}{2}e^{-\tau |x-\mu|}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : $-\infty < x < \infty$

\item {} 
\emph{mu} : Location parameter :math: \emph{-infty \textless{} mu \textless{} infty}

\item {} 
\emph{tau} : Scale parameter $\tau > 0$

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X) = \mu$

\item {} 
$Var(X) = \frac{2}{\tau^2}$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{logistic\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.logistic_like}{}\begin{funcdesc}{logistic\_like}{x, mu, tau}
Logistic log-likelihood.

The logistic distribution is often used as a growth model; for example,
populations, markets. Resembles a heavy-tailed normal distribution.
\begin{gather}
\begin{split}f(x \mid \mu, tau) = \frac{\tau \exp(-\tau[x-\mu])}{[1 + \exp(-\tau[x-\mu])]^2}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : $-\infty < x < \infty$

\item {} 
\emph{mu} : Location parameter $-\infty < mu < \infty$

\item {} 
\emph{tau} : Scale parameter (tau \textgreater{} 0)

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X) = \mu$

\item {} 
$Var(X) = \frac{\pi^2}{3\tau^2}$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{lognormal\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.lognormal_like}{}\begin{funcdesc}{lognormal\_like}{x, mu, tau}
Log-normal log-likelihood. Distribution of any random variable whose
logarithm is normally distributed. A variable might be modeled as
log-normal if it can be thought of as the multiplicative product of many
small independent factors.
\begin{gather}
\begin{split}f(x \mid \mu, \tau) = \sqrt{\frac{\tau}{2\pi}}\frac{
\exp\left\{ -\frac{\tau}{2} (\ln(x)-\mu)^2 \right\}}{x}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : x \textgreater{} 0

\item {} 
\emph{mu} : Location parameter.

\item {} 
\emph{tau} : Scale parameter (tau \textgreater{} 0).

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}
$E(X)=e^{\mu+\frac{1}{2\tau}}$
$Var(X)=(e^{1/\tau}-1)e^{2\mu+\frac{1}{\tau}}$
\end{notice}
\end{funcdesc}
\index{normal\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.normal_like}{}\begin{funcdesc}{normal\_like}{x, mu, tau}
Normal log-likelihood.
\begin{gather}
\begin{split}f(x \mid \mu, \tau) = \sqrt{\frac{\tau}{2\pi}} \exp\left\{ -\frac{\tau}{2} (x-\mu)^2 \right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input data.

\item {} 
\emph{mu} : Mean of the distribution.

\item {} 
\emph{tau} : Precision of the distribution, which corresponds to $1/\sigma^2$ (tau \textgreater{} 0).

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X) = \mu$

\item {} 
$Var(X) = 1/\tau$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{skew\_normal\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.skew_normal_like}{}\begin{funcdesc}{skew\_normal\_like}{x, mu, tau, alpha}
Azzalini's skew-normal log-likelihood
\begin{gather}
\begin{split}f(x \mid \mu, \tau, \alpha) = 2 \Phi((x-\mu)\sqrt{\tau}\alpha) \phi(x,\mu,\tau)\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where :math: Phi is the normal CDF and :math: phi is the normal PDF.
\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input data.

\item {} 
\emph{mu} : Mean of the distribution.

\item {} 
\emph{tau} : Precision of the distribution (\textgreater{} 0).

\item {} 
\emph{alpha} : Shape parameter of the distribution.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}
See \href{http://azzalini.stat.unipd.it/SN/}{http://azzalini.stat.unipd.it/SN/}
\end{notice}
\end{funcdesc}
\index{t\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.t_like}{}\begin{funcdesc}{t\_like}{x, nu}
Student's T log-likelihood. Describes a zero-mean normal variable whose precision is
gamma distributed. Alternatively, describes the mean of several zero-mean normal
random variables divided by their sample standard deviation.
\begin{gather}
\begin{split}f(x \mid \nu) = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2}) \sqrt{\nu\pi}} \left( 1 + \frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input data.

\item {} 
\emph{nu} : Degrees of freedom.

\end{itemize}

\end{description}
\end{funcdesc}
\index{truncnorm\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.truncnorm_like}{}\begin{funcdesc}{truncnorm\_like}{x, mu, tau, a, b}
Truncated normal log-likelihood.
\begin{gather}
\begin{split}f(x \mid \mu, \tau, a, b) = \frac{\phi(\frac{x-\mu}{\sigma})} {\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})},\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where $\sigma^2=1/\tau$, \emph{phi} is the standard normal PDF and \emph{Phi} is the standard normal CDF.
\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input data.

\item {} 
\emph{mu} : Mean of the distribution.

\item {} 
\emph{tau} : Precision of the distribution, which corresponds to 1/sigma**2 (tau \textgreater{} 0).

\item {} 
\emph{a} : Left bound of the distribution.

\item {} 
\emph{b} : Right bound of the distribution.

\end{itemize}

\end{description}
\end{funcdesc}
\index{uniform\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.uniform_like}{}\begin{funcdesc}{uniform\_like}{x, lower, upper}
Uniform log-likelihood.
\begin{gather}
\begin{split}f(x \mid lower, upper) = \frac{1}{upper-lower}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : $lower \leq x \leq upper$

\item {} 
\emph{lower} : Lower limit.

\item {} 
\emph{upper} : Upper limit (upper \textgreater{} lower).

\end{itemize}

\end{description}
\end{funcdesc}
\index{von\_mises\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.von_mises_like}{}\begin{funcdesc}{von\_mises\_like}{x, mu, kappa}
von Mises log-likelihood.
\begin{gather}
\begin{split}f(x \mid \mu, k) = \frac{e^{k \cos(x - \mu)}}{2 \pi I_0(k)}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where \emph{I\_0} is the modified Bessel function of order 0.
\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : Input data.

\item {} 
\emph{mu} : Mean of the distribution.

\item {} 
\emph{kappa} : Dispersion of the distribution

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X) = \mu$

\end{itemize}
\end{notice}
\end{funcdesc}
\index{weibull\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.weibull_like}{}\begin{funcdesc}{weibull\_like}{x, alpha, beta}
Weibull log-likelihood
\begin{gather}
\begin{split}f(x \mid \alpha, \beta) = \frac{\alpha x^{\alpha - 1}
\exp(-(\frac{x}{\beta})^{\alpha})}{\beta^\alpha}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : $x \ge 0$

\item {} 
\emph{alpha} : alpha \textgreater{} 0

\item {} 
\emph{beta} : beta \textgreater{} 0

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(x)=\beta \Gamma(1+\frac{1}{\alpha})$

\item {} 
$Var(x)=\beta^2 \Gamma(1+\frac{2}{\alpha} - \mu^2)$

\end{itemize}
\end{notice}
\end{funcdesc}


\section{Multivariate discrete distributions}
\index{multivariate\_hypergeometric\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.multivariate_hypergeometric_like}{}\begin{funcdesc}{multivariate\_hypergeometric\_like}{x, m}
The multivariate hypergeometric describes the probability of drawing x{[}i{]}
elements of the ith category, when the number of items in each category is
given by m.
\begin{gather}
\begin{split}\frac{\prod_i \binom{m_i}{x_i}}{\binom{N}{n}}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where $N = \sum_i m_i$ and $n = \sum_i x_i$.
\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : {[}int sequence{]} Number of draws from each category, (x \textless{} m).

\item {} 
\emph{m} : {[}int sequence{]} Number of items in each categoy.

\end{itemize}

\end{description}
\end{funcdesc}
\index{multinomial\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.multinomial_like}{}\begin{funcdesc}{multinomial\_like}{x, n, p}
Multinomial log-likelihood. Generalization of the binomial
distribution, but instead of each trial resulting in ``success'' or
``failure'', each one results in exactly one of some fixed finite number k
of possible outcomes over n independent trials. `x{[}i{]}' indicates the number
of times outcome number i was observed over the n trials.
\begin{gather}
\begin{split}f(x \mid n, p) = \frac{n!}{\prod_{i=1}^k x_i!} \prod_{i=1}^k p_i^{x_i}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{description}
\item[x] \leavevmode{[}(ns, k) int{]}
Random variable indicating the number of time outcome i is 
observed. $\sum_{i=1}^k x_i=n$, $x_i \ge 0$.

\item[n] \leavevmode{[}int{]}
Number of trials.

\item[p] \leavevmode{[}(k,) {]}
Probability of each one of the different outcomes.
$\sum_{i=1}^k p_i = 1)$, $p_i \ge 0$.

\end{description}

\end{description}

\begin{notice}{note}{Note:}\begin{itemize}
\item {} 
$E(X_i)=n p_i$

\item {} 
$Var(X_i)=n p_i(1-p_i)$

\item {} 
$Cov(X_i,X_j) = -n p_i p_j$

\end{itemize}
\end{notice}
\end{funcdesc}


\section{Multivariate continuous distributions}
\index{dirichlet\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.dirichlet_like}{}\begin{funcdesc}{dirichlet\_like}{x, theta}
Dirichlet log-likelihood.

This is a multivariate continuous distribution.
\begin{gather}
\begin{split}f(\mathbf{x}) = \frac{\Gamma(\sum_{i=1}^k \theta_i)}{\prod \Gamma(\theta_i)}\prod_{i=1}^{k-1} x_i^{\theta_i - 1}\cdot\left(1-\sum_{i=1}^{k-1}x_i\right)^\theta_k\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{description}
\item[x] \leavevmode{[}(n, k-1) array {]}
Array of shape (n, k-1) where \emph{n} is the number of samples 
and \emph{k} the dimension. 
$0 < x_i < 1$,  $\sum_{i=1}^{k-1} x_i < 1$
\item[theta] \leavevmode{[}array{]}
An (n,k) or (1,k) array \textgreater{} 0.

\end{description}

\end{description}

\begin{notice}{note}{Note:}
Only the first \emph{k-1} elements of \emph{x} are expected. Can be used as a parent of Multinomial and Categorical
nevertheless.
\end{notice}
\end{funcdesc}
\index{inverse\_wishart\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.inverse_wishart_like}{}\begin{funcdesc}{inverse\_wishart\_like}{X, n, Tau}
Inverse Wishart log-likelihood. The inverse Wishart distribution is the conjugate
prior for the covariance matrix of a multivariate normal distribution.
\begin{gather}
\begin{split}f(X \mid n, T) = \frac{{\mid T \mid}^{n/2}{\mid X \mid}^{(n-k-1)/2} \exp\left\{ -\frac{1}{2} Tr(TX^{-1}) \right\}}{2^{nk/2} \Gamma_p(n/2)}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where $k$ is the rank of X.
\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{X} : Symmetric, positive definite matrix.

\item {} 
\emph{n} : {[}int{]} Degrees of freedom (n \textgreater{} 0).

\item {} 
\emph{Tau} : Symmetric and positive definite matrix.

\end{itemize}

\end{description}

\begin{notice}{note}{Note:}
Step method MatrixMetropolis will preserve the symmetry of Wishart variables.
\end{notice}
\end{funcdesc}
\index{mv\_normal\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.mv_normal_like}{}\begin{funcdesc}{mv\_normal\_like}{x, mu, tau}
Multivariate normal log-likelihood
\begin{gather}
\begin{split}f(x \mid \pi, T) = \frac{|T|^{1/2}}{(2\pi)^{1/2}} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}T(x-\mu) \right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : (n,k)

\item {} 
\emph{mu} : (k) Location parameter sequence.

\item {} 
\emph{Tau} : (k,k) Positive definite precision matrix.

\end{itemize}

\end{description}


\strong{See Also:}


\hyperlink{pymc.distributions.mv_normal_chol_like}{\code{mv\_normal\_chol\_like()}}, \hyperlink{pymc.distributions.mv_normal_cov_like}{\code{mv\_normal\_cov\_like()}}


\end{funcdesc}
\index{mv\_normal\_chol\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.mv_normal_chol_like}{}\begin{funcdesc}{mv\_normal\_chol\_like}{x, mu, sig}
Multivariate normal log-likelihood.
\begin{gather}
\begin{split}f(x \mid \pi, \sigma) = \frac{1}{(2\pi)^{1/2}|\sigma|)} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}(\sigma \sigma^{\prime})^{-1}(x-\mu) \right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : (n,k)

\item {} 
\emph{mu} : (k) Location parameter.

\item {} 
\emph{sigma} : (k,k) Lower triangular matrix.

\end{itemize}

\end{description}


\strong{See Also:}


\hyperlink{pymc.distributions.mv_normal_like}{\code{mv\_normal\_like()}}, \hyperlink{pymc.distributions.mv_normal_cov_like}{\code{mv\_normal\_cov\_like()}}


\end{funcdesc}
\index{mv\_normal\_cov\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.mv_normal_cov_like}{}\begin{funcdesc}{mv\_normal\_cov\_like}{x, mu, C}
Multivariate normal log-likelihood parameterized by a covariance 
matrix.
\begin{gather}
\begin{split}f(x \mid \pi, C) = \frac{1}{(2\pi|C|)^{1/2}} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}C^{-1}(x-\mu) \right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}\begin{description}
\item[Parameters] \leavevmode\begin{itemize}
\item {} 
\emph{x} : (n,k)

\item {} 
\emph{mu} : (k) Location parameter.

\item {} 
\emph{C} : (k,k) Positive definite covariance matrix.

\end{itemize}

\end{description}


\strong{See Also:}


\hyperlink{pymc.distributions.mv_normal_like}{\code{mv\_normal\_like()}}, \hyperlink{pymc.distributions.mv_normal_chol_like}{\code{mv\_normal\_chol\_like()}}


\end{funcdesc}
\index{wishart\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.wishart_like}{}\begin{funcdesc}{wishart\_like}{X, n, Tau}
Wishart log-likelihood. The Wishart distribution is the probability
distribution of the maximum-likelihood estimator (MLE) of the precision
matrix of a multivariate normal distribution. If Tau=1, the distribution
is identical to the chi-square distribution with n degrees of freedom.

For an alternative parameterization based on $C=T{-1}$, see
\emph{wishart\_cov\_like}.
\begin{gather}
\begin{split}f(X \mid n, T) = {\mid T \mid}^{n/2}{\mid X \mid}^{(n-k-1)/2} \exp\left\{ -\frac{1}{2} Tr(TX) \right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where $k$ is the rank of X.
\begin{description}
\item[Parameters] \leavevmode\begin{description}
\item[X] \leavevmode{[}matrix{]}
Symmetric, positive definite.

\item[n] \leavevmode{[}int{]}
Degrees of freedom, \textgreater{} 0.

\item[Tau] \leavevmode{[}matrix{]}
Symmetric and positive definite

\end{description}

\end{description}

\begin{notice}{note}{Note:}
Step method MatrixMetropolis will preserve the symmetry of Wishart variables.
\end{notice}
\end{funcdesc}
\index{wishart\_cov\_like() (in module pymc.distributions)}

\hypertarget{pymc.distributions.wishart_cov_like}{}\begin{funcdesc}{wishart\_cov\_like}{X, n, C}
Wishart log-likelihood. The Wishart distribution is the probability
distribution of the maximum-likelihood estimator (MLE) of the covariance
matrix of a multivariate normal distribution. If C=1, the distribution
is identical to the chi-square distribution with n degrees of freedom.

For an alternative parameterization based on $T=C{-1}$, see
\emph{wishart\_like}.
\begin{gather}
\begin{split}f(X \mid n, C) = {\mid C^{-1} \mid}^{n/2}{\mid X \mid}^{(n-k-1)/2} \exp\left\{ -\frac{1}{2} Tr(C^{-1}X) \right\}\end{split}\notag\\\begin{split}\end{split}\notag
\end{gather}
where $k$ is the rank of X.
\begin{description}
\item[Parameters] \leavevmode\begin{description}
\item[X] \leavevmode{[}matrix{]}
Symmetric, positive definite.

\item[n] \leavevmode{[}int{]}
Degrees of freedom, \textgreater{} 0.

\item[C] \leavevmode{[}matrix{]}
Symmetric and positive definite

\end{description}

\end{description}
\end{funcdesc}
\normalsize
