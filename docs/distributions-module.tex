

    \label{pymc:distributions:arlognormal_like}
    \index{pymc.distributions.arlognormal\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{arlognormal\_like}(\textit{x}, \textit{a}, \textit{sigma}, \textit{rho}, \textit{beta}={\tt 1})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Autoregressive lognormal log-likelihood.
\begin{equation*}\begin{split}x_i & = a_i \exp(e_i) \\e_i & = \rho e_{i-1} + \epsilon_i\end{split}\end{equation*}
where $\epsilon_i \sim N(0,\sigma)$.
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:bernoulli_like}
    \index{pymc.distributions.bernoulli\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{bernoulli\_like}(\textit{x}, \textit{p})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Bernoulli log-likelihood

The Bernoulli distribution describes the probability of successes (x=1) and
failures (x=0).
\begin{equation*}\begin{split}f(x \mid p) = p^{x- 1} (1-p)^{1-x}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Series of successes (1) and failures (0). $x=0,1$
          \item[p]


Probability of success. $0 < p < 1$
        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(x)= p$

\item {} 
$Var(x)= p(1-p)$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:beta_like}
    \index{pymc.distributions.beta\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{beta\_like}(\textit{x}, \textit{alpha}, \textit{beta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Beta log-likelihood.
\begin{equation*}\begin{split}f(x \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


0 {\textless} x {\textless} 1
            {\it (type=float)}

          \item[alpha]


{\textgreater} 0
            {\it (type=float)}

          \item[beta]


{\textgreater} 0
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X)=\frac{\alpha}{\alpha+\beta}$

\item {} 
$Var(X)=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:binomial_like}
    \index{pymc.distributions.binomial\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{binomial\_like}(\textit{x}, \textit{n}, \textit{p})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Binomial log-likelihood.  The discrete probability distribution of the
number of successes in a sequence of n independent yes/no experiments,
each of which yields success with probability p.
\begin{equation*}\begin{split}f(x \mid n, p) = \frac{n!}{x!(n-x)!} p^x (1-p)^{1-x}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Number of successes, {\textgreater} 0.
            {\it (type=float)}

          \item[n]


Number of Bernoulli trials, {\textgreater} x.
            {\it (type=int)}

          \item[p]


Probability of success in each trial, $p \in [0,1]$.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X)=np$

\item {} 
$Var(X)=np(1-p)$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:categorical_like}
    \index{pymc.distributions.categorical\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{categorical\_like}(\textit{x}, \textit{p}, \textit{minval}={\tt 0}, \textit{step}={\tt 1})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Categorical log-likelihood.
Accepts an array of probabilities associated with the histogram,
the minimum value of the histogram (defaults to zero),
and a step size (defaults to 1).
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:cauchy_like}
    \index{pymc.distributions.cauchy\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{cauchy\_like}(\textit{x}, \textit{alpha}, \textit{beta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Cauchy log-likelihood. The Cauchy distribution is also known as the
Lorentz or the Breit-Wigner distribution.
\begin{equation*}\begin{split}f(x \mid \alpha, \beta) = \frac{1}{\pi \beta [1 + (\frac{x-\alpha}{\beta})^2]}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[alpha]


Location parameter.
            {\it (type=float)}

          \item[beta]


Scale parameter {\textgreater} 0.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
Mode and median are at alpha.

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:chi2_like}
    \index{pymc.distributions.chi2\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{chi2\_like}(\textit{x}, \textit{nu})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Chi-squared $\chi^2$ log-likelihood.
\begin{equation*}\begin{split}f(x \mid \nu) = \frac{x^{(\nu-2)/2}e^{-x/2}}{2^{\nu/2}\Gamma(\nu/2)}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xx}

          \item[x]


$\ge 0$
            {\it (type=float)}

          \item[nu]


Degrees of freedom ( $nu > 0$)
            {\it (type=int)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X)=\nu$

\item {} 
$Var(X)=2\nu$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:dirichlet_like}
    \index{pymc.distributions.dirichlet\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{dirichlet\_like}(\textit{x}, \textit{theta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Dirichlet log-likelihood.

This is a multivariate continuous distribution.
\begin{equation*}\begin{split}f(\mathbf{x}) = \frac{\Gamma(\sum_{i=1}^k \theta_i)}{\prod \Gamma(\theta_i)} \prod_{i=1}^k x_i^{\theta_i - 1}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


Where \texttt{n} is the number of samples and \texttt{k} the dimension.
$0 < x_i < 1$,  $\sum_{i=1}^{k-1} x_i < 1$
            {\it (type=(n,k-1) array)}

          \item[theta]


$\theta > 0$
            {\it (type=(n,k) or (1,k) float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} 
There is an implicit k'th value of x, equal to $\sum_{i=1}^{k-1} x_i$.


    \end{boxedminipage}

    \label{pymc:distributions:discrete_uniform_like}
    \index{pymc.distributions.discrete\_uniform\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{discrete\_uniform\_like}(\textit{x}, \textit{lower}, \textit{upper})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

discrete{\_}uniform log-likelihood.
\begin{equation*}\begin{split}f(x \mid lower, upper) = \frac{1}{upper-lower}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


$lower \geq x \geq upper$
            {\it (type=float)}

          \item[lower]


Lower limit.
            {\it (type=float)}

          \item[upper]


Upper limit.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:exponential_like}
    \index{pymc.distributions.exponential\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{exponential\_like}(\textit{x}, \textit{beta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Exponential log-likelihood.

The exponential distribution is a special case of the gamma distribution
with alpha=1. It often describes the duration of an event.
\begin{equation*}\begin{split}f(x \mid \beta) = \frac{1}{\beta}e^{-x/\beta}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxx}

          \item[x]


$x \ge 0$
            {\it (type=float)}

          \item[beta]


Survival parameter $\beta > 0$
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X) = \beta$

\item {} 
$Var(X) = \beta^2$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:exponweib_like}
    \index{pymc.distributions.exponweib\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{exponweib\_like}(\textit{x}, \textit{alpha}, \textit{k}, \textit{loc}={\tt 0}, \textit{scale}={\tt 1})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Exponentiated Weibull log-likelihood.
\begin{equation*}\begin{split}f(x \mid \alpha,k,loc,scale)  & = \frac{\alpha k}{scale} (1-e^{-z^c})^{\alpha-1} e^{-z^c} z^{k-1} \\z & = \frac{x-loc}{scale}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


{\textgreater} 0
            {\it (type=float)}

          \item[alpha]


Shape parameter
            {\it (type=float)}

          \item[k]


{\textgreater} 0
            {\it (type=float)}

          \item[loc]


Location parameter
            {\it (type=float)}

          \item[scale]


Scale parameter {\textgreater} 0.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:gamma_like}
    \index{pymc.distributions.gamma\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{gamma\_like}(\textit{x}, \textit{alpha}, \textit{beta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Gamma log-likelihood.

Represents the sum of alpha exponentially distributed random variables, each
of which has mean beta.
\begin{equation*}\begin{split}f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


$x \ge 0$
            {\it (type=float)}

          \item[alpha]


Shape parameter $\alpha > 0$.
            {\it (type=float)}

          \item[beta]


Scale parameter $\beta > 0$.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:geometric_like}
    \index{pymc.distributions.geometric\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{geometric\_like}(\textit{x}, \textit{p})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Geometric log-likelihood. The probability that the first success in a
sequence of Bernoulli trials occurs after x trials.
\begin{equation*}\begin{split}f(x \mid p) = p(1-p)^{x-1}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Number of trials before first success, {\textgreater} 0.
            {\it (type=int)}

          \item[p]


Probability of success on an individual trial, $p \in [0,1]$
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X)=1/p$

\item {} 
$Var(X)=\frac{1-p}{p^2}$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:gev_like}
    \index{pymc.distributions.gev\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{gev\_like}(\textit{x}, \textit{xi}, \textit{mu}={\tt 0}, \textit{sigma}={\tt 1})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Generalized Extreme Value log-likelihood
\begin{equation*}\begin{split}pdf(x \mid \xi,\mu,\sigma) = \frac{1}{\sigma}(1 + \xi \left[\frac{x-\mu}{\sigma}\right])^{-1/\xi-1}\exp{-(1+\xi \left[\frac{x-\mu}{\sigma}\right])^{-1/\xi}}\end{split}\end{equation*}\begin{equation*}\begin{split}\sigma & > 0,\\x & > \mu-\sigma/\xi \text{ if } \xi > 0,\\x & < \mu-\sigma/\xi \text{ if } \xi < 0\\x & \in [-\infty,\infty] \text{ if } \xi = 0\end{split}\end{equation*}\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:half_normal_like}
    \index{pymc.distributions.half\_normal\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{half\_normal\_like}(\textit{x}, \textit{tau})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Half-normal log-likelihood, a normal distribution with mean 0 and limited
to the domain $x \in [0, \infty)$.
\begin{equation*}\begin{split}f(x \mid \tau) = \sqrt{\frac{2\tau}{\pi}}\exp\left\{ {\frac{-x^2 \tau}{2}}\right\}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxx}

          \item[x]


$x \ge 0$
            {\it (type=float)}

          \item[tau]


$\tau > 0$
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:hypergeometric_like}
    \index{pymc.distributions.hypergeometric\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{hypergeometric\_like}(\textit{x}, \textit{n}, \textit{m}, \textit{N})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Hypergeometric log-likelihood. Discrete probability distribution that
describes the number of successes in a sequence of draws from a finite
population without replacement.
\begin{equation*}\begin{split}f(x \mid n, m, N) = \frac{\binom{m}{x}\binom{N-m}{n-x}}{\binom{N}{n}}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Number of successes in a sample drawn from a population.
$\max(0, draws-failures) \leq x \leq \min(draws, success)$
            {\it (type=int)}

          \item[n]


Size of sample drawn from the population.
            {\it (type=int)}

          \item[m]


Number of successes in the population.
            {\it (type=int)}

          \item[N]


Total number of units in the population.
            {\it (type=int)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} 
$E(X) = \frac{n n}{N}$


    \end{boxedminipage}

    \label{pymc:distributions:inverse_gamma_like}
    \index{pymc.distributions.inverse\_gamma\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{inverse\_gamma\_like}(\textit{x}, \textit{alpha}, \textit{beta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Inverse gamma log-likelihood, the reciprocal of the gamma distribution.
\begin{equation*}\begin{split}f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-\alpha - 1} \exp\left(\frac{-\beta}{x}\right)\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


x {\textgreater} 0
            {\it (type=float)}

          \item[alpha]


Shape parameter, $\alpha > 0$.
            {\it (type=float)}

          \item[beta]


Scale parameter, $\beta > 0$.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} 
$E(X)=\frac{1}{\beta(\alpha-1)}$  for $\alpha > 1$.


    \end{boxedminipage}

    \label{pymc:distributions:lognormal_like}
    \index{pymc.distributions.lognormal\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{lognormal\_like}(\textit{x}, \textit{mu}, \textit{tau})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Log-normal log-likelihood. Distribution of any random variable whose
logarithm is normally distributed. A variable might be modeled as
log-normal if it can be thought of as the multiplicative product of many
small independent factors.
\begin{equation*}\begin{split}f(x \mid \mu, \tau) = \sqrt{\frac{\tau}{2\pi}}\frac{\exp\left\{ -\frac{\tau}{2} (\ln(x)-\mu)^2 \right\}}{x}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxx}

          \item[x]


x {\textgreater} 0
            {\it (type=float)}

          \item[mu]


Location parameter.
            {\it (type=float)}

          \item[tau]


Scale parameter, {\textgreater} 0.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} 
$E(X)=e^{\mu+\frac{1}{2\tau}}$


    \end{boxedminipage}

    \label{distributions:mod_categor_like}
    \index{distributions \textit{(module)}!distributions.mod\_categor\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{mod\_categor\_like}(**\textit{kwds})

\setlength{\parskip}{2ex}
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{distributions:mod_multinom_like}
    \index{distributions \textit{(module)}!distributions.mod\_multinom\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{mod\_multinom\_like}(**\textit{kwds})

\setlength{\parskip}{2ex}
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:multinomial_like}
    \index{pymc.distributions.multinomial\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{multinomial\_like}(\textit{x}, \textit{n}, \textit{p})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Multinomial log-likelihood with k-1 bins. Generalization of the binomial
distribution, but instead of each trial resulting in ``success'' or
``failure'', each one results in exactly one of some fixed finite number k
of possible outcomes over n independent trials. 'x{[}i{]}' indicates the number
of times outcome number i was observed over the n trials.
\begin{equation*}\begin{split}f(x \mid n, p) = \frac{n!}{\prod_{i=1}^k x_i!} \prod_{i=1}^k p_i^{x_i}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Random variable indicating the number of time outcome i is observed,
$\sum_{i=1}^k x_i=n$, $x_i \ge 0$.
            {\it (type=(ns, k) int)}

          \item[n]


Number of trials.
            {\it (type=int)}

          \item[p]


Probability of each one of the different outcomes,
$\sum_{i=1}^k p_i = 1)$, $p_i \ge 0$.
            {\it (type=(k,) float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X_i)=n p_i$

\item {} 
$var(X_i)=n p_i(1-p_i)$

\item {} 
$cov(X_i,X_j) = -n p_i p_j$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:multivariate_hypergeometric_like}
    \index{pymc.distributions.multivariate\_hypergeometric\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{multivariate\_hypergeometric\_like}(\textit{x}, \textit{m})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

The multivariate hypergeometric describes the probability of drawing x{[}i{]}
elements of the ith category, when the number of items in each category is
given by m.
\begin{equation*}\begin{split}\frac{\prod_i \binom{m_i}{x_i}}{\binom{N}{n}}\end{split}\end{equation*}
where $N = \sum_i m_i$ and $n = \sum_i x_i$.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[x]


Number of draws from each category, $< m$
            {\it (type=int sequence)}

          \item[m]


Number of items in each categoy.
            {\it (type=int sequence)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:mv_normal_chol_like}
    \index{pymc.distributions.mv\_normal\_chol\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{mv\_normal\_chol\_like}(\textit{x}, \textit{mu}, \textit{tau})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Multivariate normal log-likelihood
\begin{equation*}\begin{split}f(x \mid \pi, \sigma) = \frac{T^{n/2}}{(2\pi)^{1/2}} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}\sigma \sigma^{\prime}(x-\mu) \right\}\end{split}\end{equation*}
x: (n,k)
mu: (k)
sigma: (k,k)
sigma lower triangular
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:mv_normal_cov_like}
    \index{pymc.distributions.mv\_normal\_cov\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{mv\_normal\_cov\_like}(\textit{x}, \textit{mu}, \textit{C})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Multivariate normal log-likelihood
\begin{equation*}\begin{split}f(x \mid \pi, C) = \frac{T^{n/2}}{(2\pi)^{1/2}} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}C^{-1}(x-\mu) \right\}\end{split}\end{equation*}
x: (n,k)
mu: (k)
C: (k,k)
C positive definite
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:mv_normal_like}
    \index{pymc.distributions.mv\_normal\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{mv\_normal\_like}(\textit{x}, \textit{mu}, \textit{tau})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Multivariate normal log-likelihood
\begin{equation*}\begin{split}f(x \mid \pi, T) = \frac{T^{n/2}}{(2\pi)^{1/2}} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}T(x-\mu) \right\}\end{split}\end{equation*}
x: (n,k)
mu: (k)
tau: (k,k)
tau positive definite
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:negative_binomial_like}
    \index{pymc.distributions.negative\_binomial\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{negative\_binomial\_like}(\textit{x}, \textit{mu}, \textit{alpha})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Negative binomial log-likelihood
\begin{equation*}\begin{split}f(x \mid r, p) = \frac{(x+r-1)!}{x! (r-1)!} p^r (1-p)^x\end{split}\end{equation*}
x {\textgreater} 0, mu {\textgreater} 0, alpha {\textgreater} 0
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:normal_like}
    \index{pymc.distributions.normal\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{normal\_like}(\textit{x}, \textit{mu}, \textit{tau})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Normal log-likelihood.
\begin{equation*}\begin{split}f(x \mid \mu, \tau) = \sqrt{\frac{\tau}{2\pi}} \exp\left\{ -\frac{\tau}{2} (x-\mu)^2 \right\}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxx}

          \item[x]


Input data.
            {\it (type=float)}

          \item[mu]


Mean of the distribution.
            {\it (type=float)}

          \item[tau]


Precision of the distribution, {\textgreater} 0.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(X) = \mu$

\item {} 
$Var(X) = 1/\tau$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:one_over_x_like}
    \index{pymc.distributions.one\_over\_x\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{one\_over\_x\_like}(\textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

returns -np.Inf if x{\textless}0, -np.log(x) otherwise.
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:poisson_like}
    \index{pymc.distributions.poisson\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{poisson\_like}(\textit{x}, \textit{mu})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Poisson log-likelihood. The Poisson is a discrete probability distribution.
It expresses the probability of a number of events occurring in a fixed
period of time if these events occur with a known average rate, and are
independent of the time since the last event. The Poisson distribution can
be derived as a limiting case of the binomial distribution.
\begin{equation*}\begin{split}f(x \mid \mu) = \frac{e^{-\mu}\mu^x}{x!}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xx}

          \item[x]


$x \in {0,1,2,...}$
            {\it (type=int)}

          \item[mu]


Expected number of occurrences that occur during the given interval,
$\mu \geq 0$.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(x)=\mu$

\item {} 
$Var(x)=\mu$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:skew_normal_like}
    \index{pymc.distributions.skew\_normal\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{skew\_normal\_like}(\textit{x}, \textit{mu}, \textit{tau}, \textit{alpha})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Azzalini's skew-normal log-likelihood
\begin{equation*}\begin{split}f(x \mid \mu, \tau, \alpha) = 2 \Phi((x-\mu)\sqrt{tau}\alpha) \phi(x,\mu,\tau)\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


Input data.
            {\it (type=float)}

          \item[mu]


Mean of the distribution.
            {\it (type=float)}

          \item[tau]


Precision of the distribution, {\textgreater} 0.
            {\it (type=float)}

          \item[alpha]


Shape parameter of the distribution.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
See \href{http://azzalini.stat.unipd.it/SN/}{http://azzalini.stat.unipd.it/SN/}

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:truncnorm_like}
    \index{pymc.distributions.truncnorm\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{truncnorm\_like}(\textit{x}, \textit{mu}, \textit{sigma}, \textit{a}, \textit{b})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Truncated normal log-likelihood.
\begin{equation*}\begin{split}f(x \mid \mu, \sigma, a, b) = \frac{\phi(\frac{x-\mu}{\sigma})} {\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})},\end{split}\end{equation*}\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:uniform_like}
    \index{pymc.distributions.uniform\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{uniform\_like}(\textit{x}, \textit{lower}, \textit{upper})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Uniform log-likelihood.
\begin{equation*}\begin{split}f(x \mid lower, upper) = \frac{1}{upper-lower}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


$lower \geq x \geq upper$
            {\it (type=float)}

          \item[lower]


Lower limit.
            {\it (type=float)}

          \item[upper]


Upper limit.
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:uninformative_like}
    \index{pymc.distributions.uninformative\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{uninformative\_like}(\textit{x})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Uninformative log-likelihood. Returns 0 regardless of the value of x.
\setlength{\parskip}{1ex}
    \end{boxedminipage}

    \label{pymc:distributions:weibull_like}
    \index{pymc.distributions.weibull\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{weibull\_like}(\textit{x}, \textit{alpha}, \textit{beta})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Weibull log-likelihood
\begin{equation*}\begin{split}f(x \mid \alpha, \beta) = \frac{\alpha x^{\alpha - 1}\exp(-(\frac{x}{\beta})^{\alpha})}{\beta^\alpha}\end{split}\end{equation*}\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxxxx}

          \item[x]


$x \ge 0$
            {\it (type=float)}

          \item[alpha]


{\textgreater} 0
            {\it (type=float)}

          \item[beta]


{\textgreater} 0
            {\it (type=float)}

        \end{Ventry}

      \end{quote}

\textbf{Note:} \begin{itemize}
\item {} 
$E(x)=\beta \Gamma(1+\frac{1}{\alpha})$

\item {} 
$Var(x)=\beta^2 \Gamma(1+\frac{2}{\alpha} - \mu^2)$

\end{itemize}


    \end{boxedminipage}

    \label{pymc:distributions:wishart_cov_like}
    \index{pymc.distributions.wishart\_cov\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{wishart\_cov\_like}(\textit{X}, \textit{n}, \textit{C})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

PLEASE CHECK THIS DOCSTRING
wishart{\_}like(X, n, C)

Wishart log-likelihood. The Wishart distribution is the probability
distribution of the maximum-likelihood estimator (MLE) of the covariance
matrix of a multivariate normal distribution. If Tau=1, the distribution
is identical to the chi-square distribution with n degrees of freedom.
\begin{equation*}\begin{split}f(X \mid n, T) = {\mid T \mid}^{n/2}{\mid X \mid}^{(n-k-1)/2} \exp\left\{ -\frac{1}{2} Tr(TX) \right\}\end{split}\end{equation*}
where $k$ is the rank of X.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{x}

          \item[X]


Symmetric, positive definite.
            {\it (type=matrix)}

          \item[n]


Degrees of freedom, {\textgreater} 0.
            {\it (type=int)}

          \item[C]


Symmetric and positive definite
            {\it (type=matrix)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \label{pymc:distributions:wishart_like}
    \index{pymc.distributions.wishart\_like \textit{(function)}}

    \vspace{0.5ex}

\hspace{.8\funcindent}\begin{boxedminipage}{\funcwidth}

    \raggedright \textbf{wishart\_like}(\textit{X}, \textit{n}, \textit{Tau})

    \vspace{-1.5ex}

    \rule{\textwidth}{1pt}
\setlength{\parskip}{2ex}

Wishart log-likelihood. The Wishart distribution is the probability
distribution of the maximum-likelihood estimator (MLE) of the precision
matrix of a multivariate normal distribution. If Tau=1, the distribution
is identical to the chi-square distribution with n degrees of freedom.
\begin{equation*}\begin{split}f(X \mid n, T) = {\mid T \mid}^{n/2}{\mid X \mid}^{(n-k-1)/2} \exp\left\{ -\frac{1}{2} Tr(TX) \right\}\end{split}\end{equation*}
where $k$ is the rank of X.
\setlength{\parskip}{1ex}
      \textbf{Parameters}
      \vspace{-1ex}

      \begin{quote}
        \begin{Ventry}{xxx}

          \item[X]


Symmetric, positive definite.
            {\it (type=matrix)}

          \item[n]


Degrees of freedom, {\textgreater} 0.
            {\it (type=int)}

          \item[Tau]


Symmetric and positive definite
            {\it (type=matrix)}

        \end{Ventry}

      \end{quote}

    \end{boxedminipage}

    \index{distributions \textit{(module)}|)}
