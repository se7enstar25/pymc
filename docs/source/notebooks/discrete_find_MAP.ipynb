{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `find_MAP` on models with discrete variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum a posterior(MAP) estimation, can be difficult in models which have discrete stochastic variables. Here we demonstrate the problem with a simple model, and present a few possible work arounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymc3 as mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple model of a survey with one data point. We use a $Beta$ distribution for the $p$ parameter in a binomial. We would like to know both the posterior distribution for p, as well as the predictive posterior distribution over the survey parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applied logodds-transform to p and added transformed p_logodds_ to model.\n"
     ]
    }
   ],
   "source": [
    "alpha = 4\n",
    "beta = 4\n",
    "n = 20\n",
    "yes = 15\n",
    "\n",
    "with mc.Model() as model:\n",
    "    p = mc.Beta('p', alpha, beta)\n",
    "    surv_sim = mc.Binomial('surv_sim', n=n, p=p)\n",
    "    surv = mc.Binomial('surv', n=n, p=p, observed=yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try and use `find_MAP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surv_sim': array(10), 'p_logodds_': array(0.42285684671251805)}\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    print(mc.find_MAP())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_map` defaults to find the MAP for only the continuous variables we have to specify if we would like to use the discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surv_sim': array(14), 'p_logodds_': array(0.7884573537909452)}\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    print(mc.find_MAP(vars=model.vars, disp=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the `disp` variable to display a warning that we are using a non-gradient minimization technique, as discrete variables do not give much gradient information. To demonstrate this, if we use a gradient based minimization, `fmin_bfgs`, with various starting points we see that the map does not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'p_logodds_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-299cdf766633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surv_sim'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mmap_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_MAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin_bfgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],\n\u001b[1;32m      6\u001b[0m                                                       \u001b[0mmap_est\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'surv_sim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiecki/working/projects/pymc/pymc3/tuning/starting.py\u001b[0m in \u001b[0;36mfind_MAP\u001b[0;34m(start, vars, fmin, return_raw, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'fprime'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         r = fmin(logp_o, bij.map(\n\u001b[0;32m---> 84\u001b[0;31m             start), fprime=grad_logp_o, *args, **kwargs)\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbij\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wiecki/working/projects/pymc/pymc3/blocking.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, dpt)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mapt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p_logodds_'"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    for i in range(n+1):\n",
    "        s = {'p':0.5, 'surv_sim':i}\n",
    "        map_est = mc.find_MAP(start=s, vars=model.vars, fmin=mc.starting.optimize.fmin_bfgs)\n",
    "        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],\n",
    "                                                      map_est['surv_sim'],\n",
    "                                                      s['p'],\n",
    "                                                      map_est['p'],\n",
    "                                                      model.logpc(map_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again because the gradient of `surv_sim` provides no information to the `fmin` routine and it is only changed in a few cases, most of which are not correct. Manually, looking at the log proability we can see that the maximum is somewhere around `surv_sim`$=14$ and `p`$=0.7$. If we employ a non-gradient minimization, such as `fmin_powell` (the default when discrete variables are detected), we might be able to get a better estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    for i in range(n+1):\n",
    "        s = {'p':0.5, 'surv_sim':i}\n",
    "        map_est = mc.find_MAP(start=s, vars=model.vars)\n",
    "        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],\n",
    "                                                      map_est['surv_sim'],\n",
    "                                                      s['p'],\n",
    "                                                      map_est['p'],\n",
    "                                                      model.logpc(map_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most starting values this converges to the maximum log likelihood of $\\approx -3.15$, but for particularly low starting values of `surv_sim`, or values near `surv_sim`$=14$ there is still some noise. The scipy optimize package contains some more general 'global' minimization functions that we can utilize. The `basinhopping` algorithm restarts the optimization at places near found minimums. Because it has a slightly different interface to other minimization schemes we have to define a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bh(*args,**kwargs):\n",
    "    result = mc.starting.optimize.basinhopping(*args, **kwargs)\n",
    "    # A `Result` object is returned, the argmin value can be in `x`\n",
    "    return result['x']\n",
    "\n",
    "with model:\n",
    "    for i in range(n+1):\n",
    "        s = {'p':0.5, 'surv_sim':i}\n",
    "        map_est = mc.find_MAP(start=s, vars=model.vars, fmin=bh)\n",
    "        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],\n",
    "                                                      floor(map_est['surv_sim']),\n",
    "                                                      s['p'],\n",
    "                                                      map_est['p'],\n",
    "                                                      model.logpc(map_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `basinhopping` uses a gradient minimization technique, `fmin_bfgs`, resulting in inaccurate predictions many times. If we force `basinhoping` to use a non-gradient technique we get much better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    for i in range(n+1):\n",
    "        s = {'p':0.5, 'surv_sim':i}\n",
    "        map_est = mc.find_MAP(start=s, vars=model.vars, fmin=bh, minimizer_kwargs={\"method\": /\"Powell\"})\n",
    "        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],\n",
    "                                                      map_est['surv_sim'],\n",
    "                                                      s['p'],\n",
    "                                                      map_est['p'],\n",
    "                                                      model.logpc(map_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confident in our MAP estimate we can sample from the posterior, making sure we use the `Metropolis` method for our discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    step1 = mc.step_methods.HamiltonianMC(vars=[p])\n",
    "    step2 = mc.step_methods.Metropolis(vars=[surv_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = mc.sample(25000,[step1,step2],start=map_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mc.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
