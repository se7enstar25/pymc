%!TEX TS-program = pdflatex
%!TEX TS-program = skim
%
%  PyMC User's Guide
%
%  Created by Chris Fonnesbeck on 2006-05-03.
%  Copyright (c) 2006 . All rights reserved.
%
\documentclass[article]{jss}

%% almost as usual
\author{Anand Patil}
\title{A Gaussian process module for \pkg{PyMC}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Anand Patil} %% comma-separated
\Plaintitle{A Gaussian process module for PyMC} %% without formatting
% \Shorttitle{GP's For PyMC} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
<<<<<<< HEAD
  This article introduces a package adding Gaussian process functionality to the Bayesian analysis package \pkg{PyMC}. Gaussian processes (GPs) are probability distributions for functions. In Bayesian statistics, they are often used as priors for functions whose forms are unknown. They can encode many types of knowledge about functions, yet remain much less restrictive than priors based on particular functional forms. GPs are not hard to understand at a conceptual level, but implementing them efficiently on a computer can be complicated. This package implements GPs as a set of \proglang{Python} classes that can conveniently support many types of usage, from intuitive exploration to embedding in larger probability models and fitting with MCMC.
=======
  This article introduces a package adding Gaussian process functionality to the Bayesian analysis package \pkg{PyMC}. Gaussian processes (GPs) are probability distributions for functions. In Bayesian statistics, they are often used as priors for functions whose forms are unknown. They can encode many types of knowledge about functions, yet remain much less restrictive than priors based on particular functional forms. GPs are not hard to understand at a conceptual level, but implementing them efficiently on a computer can require fairly involved linear algebra. This package implements Gaussian processes as a set of \proglang{Python} classes that can support many types of usage, from intuitive exploration to embedding them in larger probability models and fitting with MCMC.
>>>>>>> gppaper
}
\Keywords{gassian process, bayesian, \proglang{Python}}
\Plainkeywords{gaussian process, bayesian, Python} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Anand Patil\\
  Malaria Atlas Project\\
  Department of Zoology\\
  University of Oxford\\
  Oxford, OX1 3PS, UK\\
  E-mail: \email{anand.patil@zoo.ox.ac.uk}
}

% Use utf-8 encoding for foreign characters
%\usepackage[utf8]{inputenc}

% % Setup for fullpage use
% \usepackage{fullpage}
% \usepackage{amsmath}
\usepackage{epsfig}
\usepackage{upquote} 
\usepackage{verbatim} 
% 
% % \usepackage{pdfsync}
% 
% % Flexible citation syntax
% \usepackage{natbib}
% % Uncomment some of the following if you use the features
% %
% 
% % Multipart figures
% %\usepackage{subfigure}
% 
% % More symbols
% \usepackage{amsmath}
% \usepackage{amssymb}
% % \usepackage{latexsym}
% 
% % Package for including code in the document
% \usepackage{listings}
% 
% % Surround parts of graphics with box
% %\usepackage{boxedminipage}
% 
% % This is now the recommended way for checking for PDFLaTeX:
% \usepackage{ifpdf}
% 
% % Enable hyperlinks
% % \usepackage[pdfpagemode=FullScreen,colorlinks=true,linkcolor=red]{hyperref}
% 
% % \ifpdf
% % \usepackage[pdftex]{graphicx}
% % \else
% % \usepackage{graphicx}
% % \fi
% 
% %%% EPYDOC STUFF %%%
\usepackage{underscore}

\begin{document}

\maketitle

\tableofcontents


\section{Introduction}\label{sec:firstlook}

<<<<<<< HEAD
Gaussian processes (GPs) are probability distributions for functions. The statement `random function $f$ has a GP distribution with mean $M$ and covariance $C$' is usually written as follows:
=======
Gaussian processes are probability distributions for functions. The statement `random function $f$ has a Gaussian process distribution with mean $M$ and covariance $C$' is usually written as follows:
>>>>>>> gppaper
\begin{equation}
    f\sim\textup{GP}(M,C).
\end{equation}
The two parameters of the distribution are analogous to the parameters of the normal distribution. $M$ is the mean function, $M(x)=\E(f(x))$. $C$ is the covariance function, $C(x,y)=\COV(f(x),f(y))$. Among other things, $C$ regulates the amount by which $f$ may deviate from $M$ at any input value $x$, the roughnesss of $f$ and the typical lengthscale of changes in $f$.

<<<<<<< HEAD
As with any probability distribution, random values can be drawn from a GP. However, these values (called `realizations') are functions rather than the usual numbers or vectors. This package represents these random values as \code{Realization} objects which, in accordance with intuition, behave like \proglang{Python} functions with a few extra features.

GPs are represented by \code{GaussianProcess} objects. These are \pkg{PyMC} stochastic variables \citep{pymc} (analogous to \pkg{WinBugs}' stochastic nodes \citep{bugs}) valued as \code{Realization} objects. This package enables \pkg{PyMC} to fit models involving \code{GaussianProcess}es using Markov chain Monte Carlo \citep{gamerman}, after which the `dynamic trace' for each \code{GaussianProcess} consists of a sequence of \code{Realization} objects sampled from the target distribution. 

This intuitive object model simplifies and expedites construction and fitting of probability models, as well as predictive simulation. It also makes available an enormous model space, which extends far beyond the standard linear model and generalized linear model families. The \code{GaussianProcess} object `is' a GP in the sense that a \pkg{PyMC} \code{Normal} variable `is' a normal random variable; as is the case for a \code{Normal}, a \code{GaussianProcess}'s descendants' dependencies on it can be defined using the full expressive power of \proglang{Python}.

This package's model-fitting functionality is generic enough to support, in theory, any model that can be defined using \code{GaussianProcess}s and the standard variable classes in \pkg{PyMC}. It is known to perform well for standard situations such as Bayesian geostatistics (see Section~\ref{sec:duffy}), but it cannot provide good performance in all cases. \pkg{PyMC}'s friendly and extensible system of step methods can be used to create bespoke jumping strategies for unusual applications.

To improve performance, all of this package's numerical computations are done in \proglang{C} or \proglang{Fortran} extensions, some of which are provided by the \pkg{NumPy} package \citep{numpybook}. Covariance function evaluations are multithreaded for sufficiently large matrices.

\medskip
All examples can be found in the folder \code{pymc/examples/gp} in the \pkg{PyMC} source tree.
=======
As with any probability distribution, random values can be drawn from a Gaussian process. However, these values (called `realizations') are actually functions rather than the usual numbers or vectors. This package represents these random values as \code{Realization} objects which, in accordance with intuition, behave like \proglang{Python} functions with a few extra features.

Gaussian processes are represented by \code{GaussianProcess} objects. These are \pkg{PyMC} stochastic variables \citep{pymc} (analogous to \pkg{WinBugs}' stochastic nodes \citep{bugs}) valued as \code{Realization} objects. These models can be fit using Markov chain Monte Carlo \cite{gamerman}, after which the `dynamic trace' for each \code{GaussianProcess} consists of a sequence of \code{Realization} objects sampled from the target distribution. 

This intuitive object model simplifies and expedites construction and fitting of probability models, as well as predictive simulation. It also makes available an enormous model space, which extends far beyond the standard linear model and generalized linear model families. The \code{GaussianProcess} object `is' a Gaussian process in the sense that a \pkg{PyMC} \code{Normal} variable `is' a normal random variable; as is the case for a \code{Normal}, a \code{GaussianProcess}'s descendants' dependencies on it can be defined using the full expressive power of \proglang{Python}.

This package's model-fitting functionality is generic enough to support, in theory, any model that can be defined using \code{GaussianProcess}s and the standard variable classes in \pkg{PyMC}. It is known to perform well for standard situations such as Bayesian geostatistics (see chapter \textbf{ref}), but it cannot provide good performance in all cases. \pkg{PyMC}'s friendly and extensible system of step methods can be used to create fine-tuned jumping strategies for particular applications.

To improve performance, all of this package's numerical computations are done in C or Fortran extensions, some of which are provided by the \pkg{NumPy} package \citep{numpybook}. Covariance function evaluations are multithreaded for sufficiently large matrices.

\medskip
This paper introduces  All examples can be found in the folder \code{pymc/examples/gp} in the \pkg{PyMC} source tree.
>>>>>>> gppaper


\section{Creating a Gaussian process}\label{sub:inst}

<<<<<<< HEAD
This section demonstrates creation of a covariance function, a mean function, and finally several random functions drawn from the GP distribution defined by those objects.
=======
This section demonstrates creation of a covariance function, a mean function, and finally several random functions drawn from the Gaussian process distribution defined by those objects.
>>>>>>> gppaper

\subsection{Creating a mean function}\label{subsub:mean}

\begin{figure}
    \centering
        \epsfig{file=figs/mean.pdf,width=8cm}
<<<<<<< HEAD
    \caption{The mean function generated by \code{pymc/examples/gp/mean.py}.}
    \label{fig:mean}
\end{figure}

The mean function of a univariate GP is a univariate function that can be interpreted as a prior guess for the GP. Mean functions are represented by \code{Mean} objects, which are wrappers for ordinary \proglang{Python} functions. The following code (from \code{pymc/examples/gp/Mean.py}) will produce an instance of class \code{Mean} called $M$:
=======
    \caption{The mean function generated by {\sffamily `examples/mean.py'}.}
    \label{fig:mean}
\end{figure}

The mean function of a univariate Gaussian process can be interpreted as a prior guess for the GP, so it is also a univariate function. Mean functions are represented by class \code{Mean}, which is a wrapper for an ordinary \proglang{Python} function. The following code (from \code{pymc/examples/gp/Mean.py}) will produce an instance of class \code{Mean} called $M$:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
from pymc.gp import *
def quadfun(x, a, b, c):
    return (a * x ** 2 + b * x + c)
M = Mean(quadfun, a = 1., b = .5, c = 2.)        
\end{CodeInput}
\end{CodeChunk}

<<<<<<< HEAD
The first argument of \code{Mean}'s init method is the underlying \proglang{Python} function, in this case \code{quadfun}. The extra arguments $a$, $b$  and $c$ will be memorized and passed to \code{quadfun} whenever $M$ is called; the call $M(x)$ in the plotting portion of \code{pymc/examples/gp/Mean.py}, which produces Figure~\ref{fig:mean}, does not need to pass them in.

Mean functions broadcast over their arguments in the same way as \pkg{NumPy} universal functions \citep{numpybook}, which means that the call $M(x)$, where $x$ is a vector, returns the vector
\begin{eqnarray*}
    [M(x_0),\ldots, M(x_{N_x-1})].
\end{eqnarray*}

The last part of the code plots $M(x)$ on $-1<x<1$, and its output is shown in Figure~\ref{fig:mean}. As expected, the plot is a parabola.
=======
The first argument of \code{Mean}'s init method is the underlying \proglang{Python} function, in this case \code{quadfun}. The extra arguments $a$, $b$  and $c$ will be memorized and passed to \code{quadfun} whenever $M$ is called; the call $M(x)$ in the plotting portion of the script does not need to pass them in.

Mean functions broadcast over their arguments in the same way as \href{http://docs.scipy.org/doc/numpy/reference/ufuncs.html}{\pkg{NumPy} universal functions} \citep{numpybook}, which means that the call $M(x)$, where $x$ is a vector, returns the vector
\begin{eqnarray*}
    [M(x_0),\ldots, M(x_{N-1})].
\end{eqnarray*}

The last part of the code plots $M(x)$ on $-1<x<1$, and its output is shown in figure \ref{fig:mean}. As expected, the plot is a parabola.
>>>>>>> gppaper

\subsection{Creating a covariance function}\label{subsub:cov}
\begin{figure}
    \centering
<<<<<<< HEAD
        \epsfig{file=figs/cov.pdf,width=15cm}
    \caption{The covariance function generated by \code{pymc/examples/gp/cov.py}. On the left is the covariance function $C(x,y)$ evaluated over a square: $-1\le x\le 1,\ -1\le y\le 1$. On the right is a slice of the covariance: $C(x,0)$ for $0\le x \le 1$}
=======
        \epsfig{file=figs/cov.pdf,width=12cm}
    \caption{The covariance function generated by {\sffamily `examples/cov.py'}. On the left is the covariance function $C(x,y)$ evaluated over a square: $-1\le x\le 1,\ -1\le y\le 1$. On the right is a slice of the covariance: $C(x,0)$ for $0\le x \le 1$}
>>>>>>> gppaper
    \label{fig:cov}
\end{figure}

Covariance functions are represented by the class \code{Covariance}, which like \code{Mean} is essentially a wrapper for ordinary \proglang{Python} functions. The example in \code{pymc/examples/gp/cov.py} uses the popular Mat\`ern function \citep{banerjee}, which is provided in module \code{cov_funs}. In addition to the two arguments $x$ and $y$, the Mat\`ern function takes three parameters: \code{amp} controls the amount by which realizations may deviate from their mean, \code{diff_degree} controls the roughness of realizations (the degree of differentiability), and \code{scale} controls the lengthscale over which realizations change.

<<<<<<< HEAD
The user is free to write functions to wrap in \code{Covariance} objects. See the full package documentation at \href{http://pymc.googlecode.com/files/GPUserGuide.pdf}{http://pymc.googlecode.com/files/GPUserGuide.pdf} for more information.
=======
The user is free to write functions to wrap in \code{Covariance} objects. See \href{http://code.google.com/p/pymc}{the package documentation} for more information.
>>>>>>> gppaper

The code in \code{pymc/examples/gp/cov.py} will produce an instance of class \code{Covariance} called $C$:
\begin{CodeChunk}
\begin{CodeInput}
from pymc.gp import *
from pymc.gp.cov_funs import matern

C = Covariance(eval_fun = matern.euclidean, diff_degree = 1.4, amp = .4, scale = 1.)
\end{CodeInput}
\end{CodeChunk}

The first argument to \code{Covariance}'s init method is the \proglang{Python} function from which the covariance function will be made. In this case, \code{eval_fun} is \code{matern.euclidean}. Covariance functions' calling conventions are slightly different from ordinary \pkg{NumPy} universal functions' \citep{numpybook} in two ways. First, broadcasting works differently. If $C$ were a \pkg{NumPy} universal function, $C(x,y)$ would return the following array:
    \begin{eqnarray*}
        \begin{array}{ccc}
<<<<<<< HEAD
            [C(x_0,y_0)& \ldots& C(x_{N_x-1},y_{N_x-1})],
        \end{array}
    \end{eqnarray*}
    where $x$ and $y$ would need to be vectors of the same length $N_x$. In fact $C(x,y)$ returns a matrix:
=======
            [C(x_0,y_0)& \ldots& C(x_{N-1},y_{N-1})],
        \end{array}
    \end{eqnarray*}
    where $x$ and $y$ would need to be vectors of the same length. In fact $C(x,y)$ returns a matrix:
>>>>>>> gppaper
    \begin{eqnarray*}
        \left[\begin{array}{ccc}
            C(x_0,y_0)& \ldots& C(x_0,y_{N_y-1})\\
            \vdots&\ddots&\vdots\\
            C(x_{N_x-1},y_0)& \ldots& C(x_{N_x-1},y_{N_y-1})
        \end{array}\right],
    \end{eqnarray*}
<<<<<<< HEAD
    and input arguments $x$ and $y$ can have different lengths $N_x$ and $N_y$. Second, covariance functions can be called with just one argument. $C(x)$ returns
=======
    and input arguments $x$ and $y$ don't need to be the same length. Second, covariance functions can be called with just one argument. $C(x)$ returns
>>>>>>> gppaper
    \begin{eqnarray*}
         [C(x_0,x_0)& \ldots& C(x_{N_x-1},x_{N_x-1})] = \textup{diag}(C(x,x)),
    \end{eqnarray*}
    but is computed much faster than diag$(C(x,x))$ would be.
The extra arguments \code{diff_degree, amp} and \code{scale}, which are required by \code{matern.euclidean}, will be passed to \code{matern.euclidean} by $C$ every time is called.
 
<<<<<<< HEAD
The output of \code{pymc/examples/gp/cov.py} is shown in Figure~\ref{fig:cov}.

\subsubsection{Cholesky algorithms}

The numerical `heavy lifting' required by this package is mostly done by \code{Covariance} and its subclasses. \code{Covariance} itself bases all its computations on the incomplete Cholesky decomposition algorithm used by the \proglang{Matlab} package \pkg{chol_incomplete} \citep{seeger}. \code{Covariance} computes rows of covariance matrices as they are needed, so if the function it wraps tends to produce covariance matrices with only a few large eigenvalues it can approximate the Cholesky decomposition in less than $O(n^2)$ arithmetic operations \citep{predictivechol}.

\code{Covariance} calls back to \proglang{Python} from \proglang{Fortran} every time it needs a new row. If the function it wraps tends to produce full-rank covariance matrices (for which all rows are required), this is inefficient. \code{FullRankCovariance} is a drop-in replacement for \code{Covariance} that is much faster, but fails (with a helpful error message) if it attempts to factor a matrix that is not of full rank. \code{NearlyFullRankCovariance} provides a compromise between the two: it computes covariance matrices in full in \proglang{Fortran}, then factors them using the robust algorithm of \pkg{chol_incomplete}.
=======
The output of \code{examples/cov.py} is shown in figure \ref{fig:cov}.

\subsubsection{Cholesky algorithms}

The numerical `heavy lifting' done by this package is primarily handled by \code{Covariance} and its subclasses. \texttt{Covariance} itself bases all its computations on the incomplete Cholesky decomposition algorithm used by the \proglang{Matlab} package \pkg{chol_incomplete} \citep{seeger}. \code{Covariance} computes rows of covariance matrices as they are needed, so if the function it wraps tends to produce covariance matrices with only a few large eigenvalues it can approximate the Cholesky decomposition in less than $O(n^2)$ arithmetic operations \citep{predictivechol}.

\code{Covariance} calls back to \proglang{Python} from \proglang{Fortran} every time it needs a new row. If the function it wraps tends to produce full-rank covariance matrices (for which all rows are required), this is inefficient. \code{FullRankCovariance} is a drop-in replacement for \code{Covariance} that is much faster, but fails (with a helpful error message) if it attempts to factor a matrix that is not full rank. \code{NearlyFullRankCovariance} provides a compromise between the two: it computes covariance matrices in full in \proglang{Fortran}, then factors them using the robust algorithm of \pkg{chol_incomplete}.
>>>>>>> gppaper

\subsection{Drawing realizations}\label{subsub:realizations}
\begin{figure}
    \centering
<<<<<<< HEAD
        \epsfig{file=figs/realizations.pdf,width=10cm}
    \caption{Three realizations from a GP generated by \code{pymc/examples/gp/realizations.py} displayed with mean and a $\pm$ one standard deviation envelope.}
    \label{fig:realizations}
\end{figure}

The code in \code{pymc/examples/gp/realizations.py} generates a list of \code{Realization} objects, which represent realizations (draws) from the GP defined by $M$ and $C$:
=======
        \epsfig{file=figs/realizations.pdf,width=8cm}
    \caption{Three realizations from a Gaussian process displayed with mean $\pm$ 1 sd envelope. Generated by {\sffamily `examples/realizations.py'}.}
    \label{fig:realizations}
\end{figure}

The code in \texttt{pymc/examples/gp/realizations.py} generates a list of \code{Realization} objects, which represent realizations (draws) from the Gaussian process defined by $M$ and $C$:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
from mean import M
from cov import C
from pymc.gp import *

f_list = [Realization(M,C) for i in range(3)]
\end{CodeInput}
\end{CodeChunk}

<<<<<<< HEAD
The init method of \code{Realization} takes only two required arguments, a \code{Mean} object and a \code{Covariance} object. Each element of \code{f_list} is a GP realization, which is essentially a randomly-generated \proglang{Python} function. Like \code{Mean} objects, \code{Realization} objects use the same broadcasting rules as \pkg{NumPy} universal functions. The call $f(x)$ returns the vector
\begin{eqnarray*}
    [f(x_0)\ldots f(x_{N_x-1})].
\end{eqnarray*}

Each of the three realizations in \code{f_list} is plotted in Figure~\ref{fig:realizations}, superimposed on a $\pm$ 1 standard deviation envelope.


\section{Nonparametric regression: observing GPs}\label{sec:observing}

\begin{figure}
    \centering
        \epsfig{file=figs/obs.pdf,width=10cm}
        % \epsfig{file=figs/cond.pdf,width=7cm}
    \caption{The output of \code{pymc/examples/gp/observations.py}. The uncertain observation of $f$'s value at $o=[-.5,.5]$ has been incorporated to produce a posterior distribution. The observed values are shown as black dots. Realizations from the posterior are more constrained at the observation points than realizations from the prior, shown in Figure~\ref{fig:realizations}.}
    \label{fig:obs}
\end{figure}

Consider the common statistical situation in which a GP prior for an unknown function $f$ is chosen with mean and covariance $M$ and $C$, and the value of $f$ is subsequently observed at $N_o$ input points $[o_0\ldots o_{N_o-1}]$, possibly with uncertainty. If the observation error is normally distributed, it turns out that $f$'s posterior distribution given the new information is another GP, with new mean and covariance functions $M_o$ and $C_o$.
=======
The init method of \code{Realization} takes only two required arguments, a \code{Mean} object and a \code{Covariance} object. Each element of \code{f_list} is a Gaussian process realization, which is essentially a randomly-generated \proglang{Python} function. Like \code{Mean} objects, \code{Realization} objects use the same broadcasting rules as \pkg{NumPy} universal functions. The call $f(x)$ returns the vector
\begin{eqnarray*}
    [f(x_0)\ldots f(x_{N-1})].
\end{eqnarray*}

Each of the three realizations in \code{f_list} is plotted in figure \ref{fig:realizations}, superimposed on a $\pm$ 1 standard deviation envelope.


\section{Nonparametric regression: observing Gaussian processes}\label{sec:observing}

\begin{figure}
    \centering
        \epsfig{file=figs/obs.pdf,width=5cm}
        \epsfig{file=figs/cond.pdf,width=5cm}
    \caption{The output of {\sffamily `examples/observations.py'}: the observed GP with \code{obs_V = .002} (left) and \code{obs_V = 0} (right). Note that in the conditioned case, the $\pm$ 1 SD envelope shrinks to zero at the points where the observations were made, and all realizations pass through the observed values. Compare these plots to those in figure \ref{fig:realizations}.}
    \label{fig:obs}
\end{figure}

Consider the following common statistical situation: A Gaussian process prior for an unknown function $f$ is chosen, then the value of $f$ is observed at $N$ input points $[o_0\ldots o_{N-1}]$, possibly with uncertainty. If the observation error is normally distributed, it turns out that $f$'s posterior distribution given the new information is another Gaussian process, with new mean and covariance functions.
>>>>>>> gppaper

The probability model that represents this situation is as follows:
\begin{equation}
    \label{regprior}
    \left.\begin{array}{l}
        \textup{data}_i \stackrel{\tiny{\textup{ind}}}{\sim} \textup{N}(f(o_i), V_i)\\
        f \sim \textup{GP}(M,C)\\
    \end{array}\right\}\Rightarrow f|\textup{data} \sim \textup{GP}(M_o, C_o).
\end{equation}
<<<<<<< HEAD
Function \code{observe} imposes normally-distributed observations on GP distributions. This function converts $f$'s prior to its posterior by transforming $M$ and $C$ in equation~\ref{regprior} to $M_o$ and $C_o$.
=======
Function \code{observe} imposes normally-distributed observations on Gaussian process distributions. This function converts $f$'s prior to its posterior by transforming $M$ and $C$ in equation \ref{regprior} to $M_o$ and $C_o$:
>>>>>>> gppaper

The code in \code{pymc/examples/gp/observation.py} imposes the observations
\begin{eqnarray*}
    f(-.5) = 3.1\\
    f(.5) = 2.9
\end{eqnarray*}
<<<<<<< HEAD
with observation variance $V=.002$ on the GP defined in \code{mean.py} and \code{cov.py}:
=======
with observation variance $V=.002$ on the GP distribution defined in \code{mean.py} and \code{cov.py}:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
from mean import M
from cov import C
from pymc.gp import *
from numpy import *

<<<<<<< HEAD
o = array([-.5,.5])
V = array([.002,.002])
data = array([3.1, 2.9])
observe(M, C, obs_mesh=o, obs_V=V, obs_vals=data)
=======
obs_x = array([-.5,.5])
V = array([.002,.002])
data = array([3.1, 2.9])
observe(M=M, C=C, obs_mesh=obs_x, obs_V=V, obs_vals=data)
>>>>>>> gppaper

f_list = [Realization(M,C) for i in range(3)]
\end{CodeInput}
\end{CodeChunk}

The function \code{observe} takes a covariance $C$ and a mean $M$ as arguments, and tells them that their `true' realization's value on \code{obs_mesh} has been observed to be \code{obs_vals} with variance \code{obs_V}. 

<<<<<<< HEAD
The output of \code{observation.py}  is shown in Figure~\ref{fig:obs}. Compare this to the analogous figure for the unobserved GP, Figure~\ref{fig:realizations}. The covariance after observation is visualized in Figure~\ref{fig:obscov}. The covariance `tent' of Figure~\ref{fig:cov} has been pressed down at points where $x\approx \pm .5$ and/or $y\approx\pm .5$, which are the values where the observations were made.

\begin{figure}
    \centering
        \epsfig{file=figs/obscov.pdf,width=10cm}
    \caption{The output of \code{pymc/examples/gp/observation.py} shows the covariance function from \code{pymc/examples/gp/cov.py} after observation. The covariance function before observation, visualized in Figure~\ref{fig:cov}, has been pressed down near the observation points $o=[-.5, .5]$.}
    \label{fig:obscov}
\end{figure}

\section{Higher-dimensional Gaussian processes}\label{sec:highdim}

In addition to functions of one variable such as $f(x)$, this package supports GP priors for functions of $d>1$ variables such as $f(\mathbf{x})$, where $\mathbf{x}=[x_0\ldots x_{d-1}]$. This is useful for modeling dynamical or biological functions of many variables as well as for spatial statistics.
=======
The output of \code{observation.py}  is shown in figure \ref{fig:obs}, along with the output with \code{obs_V=0}. Compare these to the analogous figure for the unobserved GP, figure \ref{fig:realizations}. The covariance after observation is visualized in figure \ref{fig:obscov}. The covariance `tent' has been pressed down at points where $x\approx \pm .5$ and/or $y\approx\pm .5$, which are the values where the observations were made.

\begin{figure}
    \centering
        \epsfig{file=figs/obscov.pdf,width=5cm}
    \caption{The covariance function from {\sffamily `observation.py'} after observation. Compare this with the covariance function before observation, visualized in figure \ref{fig:cov} }
    \label{fig:obscov}
\end{figure}

\section{Higher-dimensional GPs}\label{sec:highdim}

In addition to functions of one variable such as $f(x)$, this package supports Gaussian process priors for functions of many variables such as $f(\mathbf{x})$, where $\mathbf{x}=[x_0\ldots x_{n-1}]$. This is useful for modeling dynamical or biological functions of many variables as well as for spatial statistics.
>>>>>>> gppaper

When array is passed into a \code{Mean}, \code{Covariance} or \code{Realization}'s init method or one of these objects is evaluated on an array, the array's last index is understood to iterate over spatial dimension. To evaluate a covariance $C$ on the ordered pairs $(0,1)$, $(2,3)$, $(4,5)$ and $(6,7)$, the user could pass in the following two-dimensional \pkg{NumPy} array:
\begin{verbatim}
[[0,1]
 [2,3]
 [4,5]
 [6,7]]
\end{verbatim}
or the following three-dimensional array:
\begin{verbatim}
[[[0,1]
  [2,3]],

  [4,5]
  [6,7]]]
\end{verbatim}
Either is fine, since in both the last index iterates over elements of the ordered pairs.

The exception to this rule is one-dimensional input arrays. The array
\begin{verbatim}
[0, 1, 2, 3, 4, 5, 6, 7]
\end{verbatim}
is interpreted as an array of eight one-dimensional values, whereas the array
\begin{verbatim}
[[0, 1, 2, 3, 4, 5, 6, 7]]
\end{verbatim}
is interpreted as a single eight-dimensional value according to the convention above.

Means and covariances learn their spatial dimension the first time they are called or observed. Some covariances, such as those specified in geographic coordinates, have an intrinsic spatial dimension. Realizations inherit their spatial dimension from their means and covariances when possible, otherwise they infer it the first time they are called. If one of these objects is subsequently called with an input of a different dimension, it raises an error.

\subsection{Covariance function bundles and coordinate systems}
<<<<<<< HEAD
The examples so far, starting with \code{pymc/examples/gp/cov.py}, have used the covariance function \code{matern.euclidean}. This function is an attribute of the \code{matern} object, which is an instance of class \code{covariance_function_bundle}.
=======
The examples so far, starting with \code{examples/cov.py}, have used the covariance function \code{matern.euclidean}. This function is an attribute of the \code{matern} object, which is an instance of class \code{covariance_function_bundle}.
>>>>>>> gppaper

Instances of \code{covariance_function_bundle} have three attributes, \code{euclidean}, \code{geo_deg} and \code{geo_rad}, which correspond to standard coordinate systems:
\begin{itemize}
    \item \code{euclidean}: $n$-dimensional Euclidean coordinates.
    \item \code{geo_deg}: Geographic coordinates (longitude, latitude) in degrees, with unit radius.
    \item \code{geo_rad}: Geographic coordinates (longitude, latitude) in radians, with unit radius.
\end{itemize}

<<<<<<< HEAD
See the full package documentation at \href{http://pymc.googlecode.com/files/GPUserGuide.pdf}{http://pymc.googlecode.com/files/GPUserGuide.pdf} for information regarding creation and extension of covariance function bundles.
=======
See \href{http://code.google.com/p/pymc}{the package documentation} for information regarding creation and extension of covariance function bundles.
>>>>>>> gppaper

\section{Basis covariances}\label{sec:basis}

\begin{figure}[htbp]
    \centering
<<<<<<< HEAD
        \epsfig{file=figs/basiscov.pdf,width=10cm}
        \caption{The output of \code{pymc/examples/gp/basis_cov.py} shows three realizations of an observed GP whose covariance is an instance of \code{BasisCovariance}. The basis in this case is function \code{fourier_basis} from module \code{cov_funs}. 25 basis functions are used. The realizations are substantially smoother than those in Figure~\ref{fig:obs}.}
=======
        \epsfig{file=figs/basiscov.pdf,width=8cm}
        \caption{Three realizations of an observed Gaussian process whose covariance is an instance of \code{BasisCovariance}. The basis in this case is function \code{fourier_basis} from module \code{cov_funs}. 25 basis functions are used.}
>>>>>>> gppaper
    \label{fig:basiscov}
\end{figure}

It is possible to create random functions from linear combinations of finite sets of basis functions $\{e\}$ with random coefficients $\{c\}$:
\begin{eqnarray*}
    f(x) = M(x) + \sum_{i_0=0}^{n_0-1}\ldots \sum_{i_{N-1}=0}^{n_{N-1}-1} c_{i_1\ldots i_{N-1}} e_{i_1\ldots i_{N-1}}(x), \\
    \{c\}\sim \textup{N}(0,K).
\end{eqnarray*}
<<<<<<< HEAD
It follows that $f$ is a GP with mean $M$ and covariance defined by
=======
It follows that $f$ is a Gaussian process with mean $M$ and covariance defined by
>>>>>>> gppaper
\begin{eqnarray*}
    C(x,y)=\sum_{i_0=0}^{n_0-1}\ldots \sum_{i_{N-1}=0}^{n_{N-1}-1} \sum_{j_0=0}^{n_0-1}\ldots \sum_{j_{N-1}=0}^{n_{N-1}-1} e_{i_0\ldots i_{N-1}}(x) e_{j_0\ldots j_{N-1}}(x) K_{i_0\ldots i_{N-1}, j_0\ldots j_{N-1}},
\end{eqnarray*}
where $K$ is the covariance of the coefficients $c$.

Particularly successful applications of this general idea are:
\begin{description}
    \item[Random Fourier series:] $e_i(x) = \sin(i\pi x/L)$ or $\cos(i\pi x/L)$. See \cite{spanos}.
<<<<<<< HEAD
    \item[GP convolutions:] $e_i(x) = \exp(-(x-\mu_n)^2)$. See \cite{convolution}.
    \item[B-splines:] $e_i(x) = $ a polynomial times an interval indicator. See .
=======
    \item[Gaussian process convolutions:] $e_i(x) = \exp(-(x-\mu_n)^2)$. See \cite{convolution}.
    \item[B-splines:] $e_i(x) = $ a polynomial times an interval indicator. See \href{http://en.wikipedia.org/wiki/Basis_B-spline}{Wikipedia}'s article.
>>>>>>> gppaper
\end{description}
Such representations can be very efficient when there are many observations in a low-dimensional space, but are relatively inflexible in that they generally produce realizations that are infinitely differentiable. In some applications, this tradeoff makes sense.

This package supports basis representations via the \code{BasisCovariance} class:
\begin{verbatim}
    C = BasisCovariance(basis, cov, **basis_params)
\end{verbatim}
The arguments are:
\begin{description}
    \item[\code{basis}:] Must be an array of functions, of any shape. Each basis function will be evaluated at $x$ with the extra parameters. The basis functions should obey the same calling conventions as mean functions: return values should have shape \code{x.shape[:-1]} unless $x$ is one-dimensional, in which case return values should be of the same shape as \code{x}. Note that each function should take the entire input array as an argument.
    \item[\code{cov}:] An array whose shape is either:
        \begin{itemize}
            \item Of the same shape as \code{basis}. In this case the coefficients are assumed independent, and \code{cov[i[0],...,i[N-1]]} (an $N$-dimensional index) simply gives the prior variance of the corresponding coefficient.
            \item Of shape \code{basis.shape * 2}, using \proglang{Python}'s convention for tuple multiplication. In this case \code{cov[i[0],...,i[N-1], j[0],...,j[N-1]]} (a $2N$-dimensional index) gives the covariance of $c_{i_0\ldots i_{N-1}}$ and $c_{j_1\ldots j_{N-1}}$.
        \end{itemize}
        Internally, the basis array is ravelled and this covariance tensor is reshaped into a matrix. This input convention makes it easier to keep track of which covariance value corresponds to which coefficients. The covariance tensor must be symmetric (\code{cov[i[0],...,i[N-1], j[0],...,j[N-1]]} $=$ \code{cov[j[0],...,j[N-1], i[0],...,i[N-1]]}), and positive semidefinite when reshaped to a matrix.
    \item[\code{basis_params}:] Any extra parameters required by the basis functions.
\end{description}

\section{Separable bases}

Many bases, such as Fourier series, can be decomposed into products of functions as follows:
\begin{eqnarray*}
    e_{i_0\ldots i_{N-1}}(x) = \prod_{j=0}^{N-1}e_{i_j}^j(x)
\end{eqnarray*}
Basis covariances constructed using such bases can be represented more efficiently using \code{SeparableBasisCovariance} objects. These objects are constructed just like \code{BasisCovariance} objects, but instead of an $n_0\times \ldots \times n_{N-1}$ array of basis functions they take a nested lists of functions as follows:
\begin{verbatim}
    basis = [ [e[0][0], ... ,e[0][n[0]-1]]
                       ...
              [e[N-1][0], ... ,e[N-1][n[N-1]-1]] ].
\end{verbatim}
<<<<<<< HEAD
For an $N$-dimensional Fourier basis, each of the \code{e}'s would be a sine or cosine; frequency would increase with the second index. As with \code{BasisCovariance}, each basis needs to take the entire input array \code{x} and \code{basis_params} as arguments. See \code{fourier_basis} in \code{pymc/examples/gp/basiscov.py} for an example.

\subsection{Example}

Once created, a \code{BasisCovariance} or \code{SeparableBasisCovariance} object behaves just like a \code{Covariance} object, but it and any \code{Mean} and \code{Realization} objects associated with it will take advantage of the efficient basis representation in their internal computations. An example of \code{SeparableBasisCovariance} usage is given in \code{pymc/examples/gp/basis_cov.py}. Compare its output in Figure~\ref{fig:basiscov} to Figure~\ref{fig:obs}.
=======
For an $N$-dimensional Fourier basis, each of the \code{e}'s would be a sine or cosine; frequency would increase with the second index. As with \code{BasisCovariance}, each basis needs to take the entire input array \code{x} and \code{basis_params} as arguments. See \code{fourier_basis} in \code{examples/gp/basiscov.py} for an example.

\subsection{Example}

Once created, a \code{BasisCovariance} or \code{SeparableBasisCovariance} object behaves just like a \code{Covariance} object, but it and any \code{Mean} and \code{Realization} objects associated with it will take advantage of the efficient basis representation in their internal computations. An example of \code{SeparableBasisCovariance} usage is given in \code{pymc/examples/gp/basis_cov.py}. Compare its output in figure \ref{fig:basiscov} to that in figure \ref{fig:obs}.
>>>>>>> gppaper


\section{Gaussian process submodels}
\label{sec:gp-sub} 

<<<<<<< HEAD
\pkg{PyMC} represents random variables as objects of class \code{Stochastic}. For example, the following code produces a normally-distributed random variable, labelled $a$, with mean zero and precision one:
=======
\pkg{PyMC} \citep{pymc} represents random variables as objects of class \code{Stochastic}. For example, the following code produces a normally-distributed random variable, labelled \code{'a'}, with mean zero and precision one:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
a = pymc.Normal('a',0,1)
\end{CodeInput}
\end{CodeChunk}
<<<<<<< HEAD
The current value of variable $a$ can be queried and updated as follows:
=======
The current value of variable \code{a} can be queried and updated as follows:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
print a.value
a.value = 0.5
\end{CodeInput}
\end{CodeChunk}
<<<<<<< HEAD
The logarithm of $a$'s probability density function, evaluated at its current value, can be obtained via \code{a.logp}. \pkg{PyMC} has a large library of optimized log-probability density/mass functions, and readily incorporates new functions created by users.

\pkg{PyMC} variables can serve as parents of other variables; for example, the following code produces a normally-distributed variable labelled $b$ whose mean is $a$:
=======
The logarithm of \code{a}'s probability density function, evaluated at its current value, can be obtained via \code{a.logp}. PyMC has a large library of optimized log-probability density/mass functions, and readily incorporates new functions created by users.

PyMC variables can serve as parents of other variables; for example, the following code produces a normally-distributed variable labelled \code{'b'} whose mean is \code{a}:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
b = pymc.Normal('b',a,1)    
\end{CodeInput}
\end{CodeChunk}

<<<<<<< HEAD
Any Python function can be used to transform random variables via objects of class \code{Deterministic}. For example, the following code produces a variable labelled $c$ whose value is the square of $b$ 's value:
=======
Any Python function can be used to transform random variables via objects of class \code{Deterministic}. For example, the following code produces a variable labelled \code{'c'} whose value is the square of \code{b} 's value:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
@pymc.deterministic
def c(b=b):
    return b**2
\end{CodeInput}
\end{CodeChunk}
Deterministic variables can be used as parents of other variables, stochastic or deterministic. 

<<<<<<< HEAD
This package equips \pkg{PyMC} with GPs via the \code{GaussianProcess} objects, which are random variables whose values are \code{Realization} objects. Because a GP is an infinite-dimensional variable, it is not practical to compute anything like a probability density for it, and \code{GaussianProcess}es have no \code{logp} attribute.

However, the evaluation $f(x_*)$ of a GP $f$ on a mesh $x_*$ is a simple multivariate normal random variable, which can be endowed with a \code{logp} attribute. For reasons explained in Section~\ref{sec:step-methods}, every \code{GaussianProcess} is wrapped in a \code{GPSubmodel} container object along with a multivariate normal variable representing its evaluation on a constant mesh. The mesh does not affect the mathematical specification of the model (\emph{ie} the posterior), but does have a substantial effect on the performance of the MCMC algorithm.  

\subsection{Example: nonparametric regression with unknown mean and covariance parameters}\label{sub:BasicMCMC}

A GP submodel is created in \code{pymc/examples/gp/PyMCmodel.py} with the following call:
=======
Because it allows any \proglang{Python} function to be used as a log-probability density/mass function or variable transformation, PyMC supports a very large model space. This package enlarges that model space to include Gaussian processes via the \code{GaussianProcess} object, which is a random variable whose value is a \code{Realization} object.

Because a Gaussian process is an infinite-dimensional variable, it is not feasible to compute anything like a probability density function for it \footnote{A density with respect to a base measure could be used, but would introduce new complications. For example, Gaussian process measures with the Mat\`ern covariance function with different degrees of differentiability are not mutually continuous. \textbf{ref}}. \code{GaussianProcess}es therefore have no \code{logp} attribute, and \code{GaussianProcess}es cannot be handled by \pkg{PyMC}'s standard MCMC machinery. 

However, the evaluation $f(x_*)$ of Gaussian process $f$ on mesh $x_*$ is a simple multivariate normal random variable, which has a \code{logp} attribute and can be handled by the standard machinery. If $f(x_*)$ is incorporated in the model as a variable, a minor extension to the standard machinery (implemented by this package) makes it possible to handle $f$ itself as well. Pairs of $f$ and $f(x_*)$ variables are housed in container objects of class \code{GPSubmodel}.

\subsection{Example: nonparametric regression with unknown mean and covariance parameters}\label{sub:BasicMCMC}

A Gaussian process submodel is created in \code{pymc/examples/gp/PyMCmodel.py} with the following call:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
sm = gp.GPSubmodel('sm',M,C,fmesh)
\end{CodeInput}
\end{CodeChunk}
<<<<<<< HEAD
There are two stochastic variables in the submodel: \code{sm.f} and \code{sm.f_eval}. The first is the actual GP $f$: a stochastic variable valued as a \code{Realization} object. The second is $f(x_*)$, where $x_*$ is the constant input argument \code{fmesh}.

Once the GP submodel has been created, other variables can depend on $f$ and $f(x_*)$ in any way that can be expressed in \proglang{Python} code. In \code{PyMCmodel.py}, the observation $d$ depends on $f(x_*)$ in a very standard way: 
=======
There are two stochastic variables in the submodel: \code{sm.f} and \code{sm.f_eval}. The first is the actual Gaussian process $f$: a stochastic variable valued as a \code{Realization} object. The second is $f(x_*)$, where $x_*$ is the input argument \code{fmesh}.

Once the Gaussian process submodel has been created, other variables depend on $f$ and $f(x_*)$ in any way that can be expressed in \proglang{Python} code. In \code{\pkg{PyMC}model.py}, the observation $d$ depends on $f(x_*)$ in a very standard way: 
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
d = pymc.Normal('d',mu=sm.f_eval, tau=1./V, value=init_val, observed=True)
\end{CodeInput}
\end{CodeChunk}
<<<<<<< HEAD
In Section~\ref{sec:duffy}, a \code{GaussianProcess}'s children depend on it in a nonstandard way. 

% The full probability model is shown as a directed acyclic graph in figure \ref{fig:unobservedModel}. It illustrates the conditional independence relationships between the variables in a GP submodel.

The file \code{pymc/examples/gp/MCMC.py} fits the probability model specified in \code{\pkg{PyMC}model.py} using MCMC. The part of the file that actually dispatches the sampling loop is very simple:
=======
In section \textbf{ref}, a \code{GaussianProcess}'s children depend on it in a highly nonstandard way. 

% The full probability model is shown as a directed acyclic graph in figure \ref{fig:unobservedModel}. It illustrates the conditional independence relationships between the variables in a GP submodel.

The file \code{pymc/examples/gp/MCMC.py} fits the probability model created in \code{\pkg{PyMC}model.py} using MCMC. The part of the file that actually dispatches the MCMC is very simple:
>>>>>>> gppaper
\begin{CodeChunk}
\begin{CodeInput}
GPSampler = MCMC(PyMCmodel)
GPSampler.isample(iter=5000,burn=1000,thin=100)    
\end{CodeInput}
\end{CodeChunk}
<<<<<<< HEAD
The file's output is shown in Figure~\ref{fig:MCMCOutput}. After the MCMC loop has terminated, \\\code{GPSampler.trace('sm_f')[:]} yields a `dynamic trace' for \code{sm.f}. This is a sequence of \code{Realization} objects, which can be evaluated on new arrays for plotting. Traces for GP realizations can even be stored on disk via \pkg{PyTables} \citep{tables}, see \cite{pymc}.
=======
The file's output is shown in figure \ref{fig:MCMCOutput}. Note that after the MCMC run \code{GPSampler.trace('sm_f')[:]} yields a sequence of \code{Realization} objects, which can be evaluated on new arrays for plotting. GP realizations can even be tallied on disk using the HDF5 backend via \pkg{PyTables} \citep{tables}, see \cite{pymc}.
>>>>>>> gppaper

% \begin{figure}
%     \centering
%         \epsfig{file=figs/unobservedModel.pdf, width=15cm}
<<<<<<< HEAD
%     \caption{The \pkg{PyMC}-generated directed acyclic graph representation of the extended nonparametric regression model created by \code{pymc/examples/gp/PyMCModel.py}. Ellipses represent \code{Stochastic} objects (variables whose values are unknown even if their parents' values are known), triangles represent \code{Deterministic} objects (variables whose values can be determined if their parents' values are known), and rectangles represent \code{Stochastic} objects with the \code{isdata} flag set to \code{True} (data). Rectangles represent potentials. Arrows point from parent to child. The submodel contains the GP \code{sm\_f} and its evaluation \code{sm\_f\_eval} on input array \code{sm\_mesh}. It also contains the mean \code{sm\_M\_eval} of \code{sm\_f\_eval} and the lower-triangular Cholesky factor \code{sm\_S\_eval} of its covariance matrix, and a potential \code{sm_fr_check} that forces that covariance matrix to remain positive definite. The actual covariance evaluation \code{sm\_C\_eval} is not needed by the model, but it is exposed for use by Gibbs step methods.}
=======
%     \caption{The \pkg{PyMC}-generated directed acyclic graph representation of the extended nonparametric regression model created by \code{pymc/examples/gp/PyMCModel.py}. Ellipses represent \code{Stochastic} objects (variables whose values are unknown even if their parents' values are known), triangles represent \code{Deterministic} objects (variables whose values can be determined if their parents' values are known), and rectangles represent \code{Stochastic} objects with the \code{isdata} flag set to \code{True} (data). Rectangles represent potentials. Arrows point from parent to child. The submodel contains the Gaussian process \code{sm\_f} and its evaluation \code{sm\_f\_eval} on input array \code{sm\_mesh}. It also contains the mean \code{sm\_M\_eval} of \code{sm\_f\_eval} and the lower-triangular Cholesky factor \code{sm\_S\_eval} of its covariance matrix, and a potential \code{sm_fr_check} that forces that covariance matrix to remain positive definite. The actual covariance evaluation \code{sm\_C\_eval} is not needed by the model, but it is exposed for use by Gibbs step methods.}
>>>>>>> gppaper
%     \label{fig:unobservedModel}
% \end{figure}

\begin{figure}
    \centering
<<<<<<< HEAD
        \epsfig{file=figs/gibbsSamples.pdf,width=10cm}
        % \epsfig{file=figs/metroSamples.pdf,width=10cm}
    \caption{The output of \code{pymc/examples/gp/MCMC.py}. The left-hand panel shows all the samples generated for the GP $f$, and the right-hand panel shows the trace of $f(0)$.}
=======
        \epsfig{file=figs/gibbsSamples.pdf,width=8cm}
        % \epsfig{file=figs/metroSamples.pdf,width=10cm}
    \caption{The output of \code{pymc/examples/gp/MCMC.py}. The left-hand panel shows all the samples generated for the Gaussian process $f$, and the right-hand panel shows the trace of $f(0)$.}
>>>>>>> gppaper
    \label{fig:MCMCOutput}
\end{figure}
 

\section{Step methods}
<<<<<<< HEAD
\label{sec:step-methods}
\pkg{PyMC} delegates the task of updating variables in MCMC loops to `step method' objects \citep{pymc}. The most common and familiar of these is \code{Metropolis}, which updates variables using the Metropolis-Hastings algorithm \citep{gamerman}. \code{Metropolis} and related step methods rely on the \code{logp} attributes (see Section~\ref{sec:gp-sub}) of the variables they handle to compute acceptance probabilities for proposed values. 

Because \code{GaussianProcess}es have no \code{logp} attribute, they cannot be handled by the Metropolis-Hastings family of step methods. This package uses a relatively simple work-around that will be described in this section. The parents of $f$ are denoted $P$ and the children $K$ throughout.


\subsection{Step methods that handle parents of Gaussian processes}
If a probability density function for $f$ were available, the Metropolis-Hastings acceptance ratio for a proposed value $P_p$ of the parents \emph{and} a proposed value $f_p$ for $f$ would be:
\begin{eqnarray*}
    \frac{p(K|f_p)\ p(f_p|P_p)\ p(P_p)\ q(P)}{p(K|f)\ p(f|P)\ p(P)\ q(P_p)}
\end{eqnarray*}
where $q$ denotes the proposal density. Similarly, if a value were proposed for $f$ conditional on proposed values for the parents $P_p$ \emph{and} $f_p(x_*)$, the acceptance ratio would become
\begin{eqnarray*}
    \frac{p(K|f_p)\ p(f_p|f(x_*), P_p)\ p(f(x_*) | P_p)\ p(P_p)\ q(f_p|f(x_*),f_p, P_p)\ q(P)}{p(K|f)\ p(f|f(x_*), P)\ p(f(x_*) | P)\ p(P)\ q(f_p|f(x_*),f,P)\ q(P_p)}
=======
\label{sec:step-methods} 
Since $f$ has no \code{logp} attribute, the Metropolis-Hastings family of step methods \citep{pymc} cannot be used to update $f$, $f(x_*)$ or any of their mean or covariance parameters. This package uses a relatively simple work-around that will be described here. Throughout this section, the parents of $f$ are denoted $P$ and the children $K$.


\subsection{Step methods that handle parents of Gaussian processes}
If a probability density function for $f$ were available, the Metropolis-Hastings acceptance ratio for a proposed value $P_p$ of the parents \emph{and} a proposed value $\tilde f$ for $f$ would be:
\begin{eqnarray*}
    \frac{p(K|f_p)\ p(f_p|P_p)\ q(P)}{p(K|f)\ p(f|P)\ q(P_p)}
\end{eqnarray*}
where $q$ denotes the proposal density. Now, suppose we proposed a value for $f$ conditional on the proposed values for the parents $P$ \emph{and} $f(x_*)$. The new acceptance ratio would become
\begin{eqnarray*}
    \frac{p(K|f_p)\ p(f_p|f(x_*), P_p)\ p(f(x_*) | P_p)\ q(f_p|f(x_*),f_p, P_p)\ q(P)}{p(K|f)\ p(f|f(x_*), P)\ p(f(x_*) | P)\ q(f_p|f(x_*),f,P)\ q(P_p)}
>>>>>>> gppaper
\end{eqnarray*}
 The problematic terms are the ones with $f$ or $f_p$ in the consequent position:
\begin{eqnarray*}
    p(f_p|f(x_*), P),\\ q(f|f(x_*),f_p,P),\\ p(f|f(x_*), P),\\ q(f_p|f(x_*),f,P_p),
\end{eqnarray*}
These can be made to cancel by choosing a proposal distribution as follows:
\begin{eqnarray*}
    q(f_p|f(x_*),f,P_p) = p(f_p|f(x_*), P).
\end{eqnarray*}
In other words, if $f$ is proposed from its prior distribution conditional on $f(x_*)$ and its parents whenever $f(x_*)$ is proposed, the intractable terms don't have to be computed. This argument can be made more rigorous by replacing $f$ with its evaluation at all the points at which its value would ever be needed.

\smallskip
<<<<<<< HEAD
To summarize, any Metropolis-Hastings step method can handle the parents of $f$ or $f(x_*)$, if it proposes values for $f$ jointly with its target variable as outlined above. This minor alteration in jumping strategy is implemented by the function \\\code{wrap_metropolis_for_gp_parents}, which takes a subclass of \code{Metropolis} as an argument and returns a new step method class with altered \code{propose} and \code{reject} methods. The function automatically produces modified versions of all Metropolis step methods implemented by \pkg{PyMC} \citep{pymc}. The modified step methods are automatically assigned to parents of GPs.

\subsection{Choosing a mesh} 

The mesh points $x_*$ are the points where Metropolis-Hastings step methods can `grab' the value of $f$ to moderate the variance of its proposal distribution. If $x_*$ is an empty array, $f$'s value will be proposed from its prior, and rejection rates are likely to be quite large. If $x_*$ is too dense, on the other hand, computation of the log-probability of $f(x_*)$ will be expensive, as its cost scales as the cube of the number of points in the mesh. This continuum is illustrated in Figure~ \ref{fig:meshpropose}. Finding the happy medium requires some experimentation.
=======
To summarize, any Metropolis-Hastings step method can handle the parents of $f$, as well as $f(x_*)$, if it proposes values for $f$ jointly with its target variable as outlined above. 

This minor alteration in jumping strategy is implemented by the function \code{wrap_metropolis_for_gp_parents}, which takes a subclass of \code{Metropolis} as an argument and returns a new step method class with altered \code{propose} and \code{reject} methods. The function automatically produces modified versions of all Metropolis step methods in \pkg{PyMC}'s step method registry (\code{Metropolis}, \code{AdaptiveMetropolis}, etc.) \citep{pymc}. The modified step methods are automatically assigned to parents of Gaussian processes.

\subsection{Choosing a mesh} 

The mesh points $x_*$ are the points where Metropolis-Hastings step methods can `grab' the value of $f$ to moderate the variance of its proposal distribution. If $x_*$ is an empty array, $f$'s value will be proposed from its prior, and rejection rates are likely to be quite large. If $x_*$ is too dense, on the other hand, computation of the log-probability of $f(x_*)$ will be expensive, as it scales as the cube of the number of points in the mesh.This continuum is illustrated in figure \ref{fig:meshpropose}. Finding the happy medium requires some experimentation.
>>>>>>> gppaper

If $f$'s children depend on its value only via its evaluation on the mesh, the likelihood terms $p(K|f_p)$ and $p(K|f)$ will cancel. In other words, if the mesh is chosen so that $p(K|f)=p(K|f(x_*))$ then the proposed value of $f$ will have no bearing on the acceptance probability of the proposed value of $f(x_*)$ or of the parents $P$. This is the situation in \code{\pkg{PyMC}Model.py}. Such a mesh choice will generally improve the acceptance rate.

\begin{figure}
    \centering
<<<<<<< HEAD
        \epsfig{file=figs/nomeshpropose.pdf,width=5cm}
        \epsfig{file=figs/lightmeshpropose.pdf,width=5cm}
        \epsfig{file=figs/densemeshpropose.pdf,width=5cm}
    \caption{Several possible proposals of $f$ (curves) given proposed values for $f(x_*)$ (heavy dots) with no mesh (left), a sparse mesh (middle), and a dense mesh (right). Proposal distributions'  $\pm$ one standard deviation envelopes are shown as shaded regions, with means shown as broken lines. With no mesh, $f$ is proposed from its prior and the acceptance rate will usually be very low. A denser mesh permits a high degree of control over $f$ and hence better acceptance ratios, but computing the log-probability is more expensive.}
=======
        \epsfig{file=figs/nomeshpropose.pdf,width=4cm}
        \epsfig{file=figs/lightmeshpropose.pdf,width=4cm}
        \epsfig{file=figs/densemeshpropose.pdf,width=4cm}
    \caption{Several possible proposals of $f$ (curves) given proposed values for $f(x_*)$ (heavy dots) with no mesh (top), a sparse mesh (middle), and a dense mesh (bottom). Proposal distributions' envelopes are shown as shaded regions, with means shown as broken lines. With no mesh, $f$ is proposed from its prior and the acceptance rate will be very low. A denser mesh permits a high degree of control over $f$, but computing the log-probability will be more expensive.}
>>>>>>> gppaper
    \label{fig:meshpropose}
\end{figure}

\subsection{Gibbs steps} 
<<<<<<< HEAD
\label{sec:gp-gibbs} 
=======
>>>>>>> gppaper
If all of $f$'s children, $K$, depend on it as follows:
\begin{eqnarray*}
    K_i|f \stackrel{\textup{\tiny ind}}{\sim} \textup{Normal}(f(x_{*i}),V_i)
\end{eqnarray*}
then $f(x_*)$ can be handled by the \code{GPEvaluationGibbs} step method. This step method is used in \code{MCMC.py}:
\begin{CodeChunk}
\begin{CodeInput}
GPSampler.use_step_method(gp.GPEvaluationGibbs, GPSampler.submod, \
    GPSampler.V, GPSampler.d)
\end{CodeInput}
\end{CodeChunk}
<<<<<<< HEAD
The initialization arguments are the GP submodel that contains $f$, the observation variance of $f$'s children, and the children, in this case the vector-valued normal variable $d$. 

\code{GPEvaluationGibbs} covers the standard submodel encountered in geostatistics, but there are many conjugate situations to which it does not apply. If necessary, special step methods can be written to handle these situations. If \code{GPEvaluationGibbs} is not assigned manually, $f(x_*)$ will generally be handled by a wrapped version of \code{AdaptiveMetropolis} \citep{pymc}, which implements the algorithm of Haario, Saksman and Tamminenn \citep{haario}.


\section{Example: Duffy negativity prevalence in Africa}
\label{sec:duffy} 

\cite{Howes} recently presented global maps of the prevalence of Duffy negativity, a genetic trait in humans that confers resistance to infection by the malaria parasite \emph{Plasmodium vivax} \citep{duffy-vivax}. The geostatistical analysis that produced the maps was implemented using this package, and demonstrates its ability to support a model space that extends outside the standard generalized linear model family. 

The original implementation is freely available on the internet \citep{duffy-code, generic-mbg}. Code implementing a simplified version of the same analysis is included in the folder \\\code{pymc/examples/gp/more_examples/Duffy}. The current section will focus on this simplified version. Files \code{model.py} and \code{mcmc.py} contain the model specification and the model-fitting and predictive simulation procedure, respectively. The datafile \code{duffy-jittered.csv} contains a simulated dataset designed to produce a similar posterior distribution to the dataset collected by \cite{Howes} in Africa and the Arabian peninsula. 

\subsection{Scientific background and data}
\label{subsec:duffy-data} 
The Duffy negativity trait, as modelled by \cite{Howes}, is controlled by two genetic loci. The first differentiates the two main Duffy alleles, known as \emph{FY*A} and \emph{FY*B}. These alleles encode proteins known as Fy$^\textup{a}$ and Fy$^\textup{b}$, respectively. The second region can `silence' the alleles, in which case they are denoted \emph{FY*A}$^{ES}$ and \emph{FY*B}$^{ES}$. \emph{FY*A}$^{ES}$ is much less common than \emph{FY*B}$^{ES}$. Roughly speaking, the silenced alleles do not encode proteins. A person is called `Duffy negative' if they do not express either protein Fy$^\textup{a}$ or Fy$^\textup{b}$, that is if each of their two chromosomes contains either \emph{FY*A}$^{ES}$ or \emph{FY*B}$^{ES}$. 

For clarity, in the remainder of this section the event that the $A$ variant is present at the first locus in a given chromosome will be denoted $a$, and the event that the $B$ variant is present by $b$. Similarly, the event that the silencing variant is present at the second locus will be denoted $s$. The alleles will be denoted accordingly; allele \emph{FY*A} will be denoted $a\neg s$, allele \emph{FY*B}$^{SE}$ by $bs$, etc. Genotypes will be denoted as ordered pairs of alleles: $(bs,a \neg s)$, etc.

\cite{Howes} collected a comprehensive database of Duffy blood group surveys from the literature. The surveys, conducted at various times and places and for various reasons, fell into five categories:
\begin{description}
    \item[genotypic] These studies examined participants' actual DNA, and indicated whether they contained \emph{FY*A}$^{ES}$, \emph{FY*B}$^{ES}$ or both. That is, they could distinguish all ten genotypes $(bs,bs)$, $(as,bs)$, $(b\neg s,bs)$, \ldots
    \item[promoter] These studies also examined participants' DNA directly, but only determined whether the silencing variant was present. They determined whether each chromosome contained a member of the set $\{bs,as\}$ or the set $\{b\neg s, a\neg s\}$, but could not distinguish the members of these sets.
    \item[phenotypic] These studies examined the proteins in participants' blood rather than their actual DNA, directly assaying the medical definition of Duffy negativity. As such, they could only tell whether Fy$^\textup{a}$ and Fy$^\textup{b}$ were expressed. That means they determined whether at least one chromosome contained $b\neg s$ and whether at least one chromosome contained $a\neg s$.
    \item[a-phenotypic] These studies also examined proteins, but only determined whether Fy$^\textup{a}$ was expressed. That means they determined only whether at least one chromosome contained $a\neg s$.
    \item[b-phenotypic] Similarly to \textbf{a-phenotypic}, these studies only determined whether at least one chromosome contained $b\neg s$.
\end{description}

\subsection{Statistical model}
The frequency of Duffy negativity can be computed directly from \textbf{genotypic}, \textbf{promoter} and \textbf{phenotypic} surveys. If the entire dataset consisted of such observations the standard spatial generalized linear model framework could be applied \citep{diggle}. The other datatypes are not direct observations of the quantity to be predicted, but they are closely related and can be expected to contain valuable information that should be incorporated in the statistical analysis.

However, incorporating all of these datatypes in a geostatistical analysis requires a departure from the generalized linear model family. A simplified version of the model for these fields chosen by \cite{Howes}, and implemented in \code{model.py}, is given here with priors for scalar parameters suppressed for brevity:
\begin{equation}
    \label{eq:duffy-model} 
    \begin{array}{r}
        \phi_b,\theta_b,\phi_s,\theta_s,p_1,V_b,V_s\beta\sim\ldots\\
        C_{\phi,\theta}(x,y)=\phi\exp\left[{-\frac{d(x,y)}{\theta}}\right]\\\\
        M_b : x\mapsto \beta1_{x\in r}\\
        f_b|\phi_b,\theta_b,\beta\sim \textup{GP}(M_b, C_{\phi_b,\theta_b}) \\
        \tilde f_b(x) | f_b(x) \stackrel{\textup{\tiny ind}}{\sim} \textup{N}(f_b(x),V_b) \\\\
        M_s : x\mapsto 0\\
        f_s|\phi_s,\theta_s\sim \textup{GP}(M_s, C_{\phi_s,\theta_s})\\
        \tilde f_s(x) | f_s(x) \stackrel{\textup{\tiny ind}}{\sim} \textup{N}(f_s(x),V_s)\\\\
        p(b;x) = \textup{logit}^{-1}(\tilde f_b(x))\\
        p(s|b;x)= \textup{logit}^{-1}(\tilde f_s(x))\\
        p(s|a;x)= p_1
    \end{array}
\end{equation}  
The notation $p(b;x)$ indicates the frequency of the $b$ variant at location $x$. The $s$ variant is found almost exclusively in conjunction with the $b$ variant, so $p(s|a;x)$ is modelled as a simple (small) constant rather than a random field. The function $d(x,y)$ gives the great-circle distance between $x$ and $y$. \cite{Howes} use the Mat\`ern covariance function, but this example uses a simple exponential for shorter mixing time. The region $r$, defined by \cite{Howes}, includes Africa south of the Sahara. The $b$ variant is known to be relatively common in this region. 

The Hardy-Weinberg model \citep{gillespie}, which assumes that the alleles in each of an individual's two chromosomes are `chosen' independently, makes it possible to write down likelihoods for each of the datatypes listed in Section~\ref{subsec:duffy-data} at survey locations $o$ conditional on $p(b;o)$, $p(s|b;o)$ and $p(s|a;o)$. These are implemented using \pkg{PyMC}'s \code{Binomial} and \code{Multinomial} classes in \code{model.py} to complete model~\ref{eq:duffy-model}. \cite{Howes} give full details.

\subsection{Model fitting and prediction}
The portion of \code{mcmc.py} that creates the model and runs the sampling loop is quite concise: 
\begin{CodeInput}
DuffySampler = pymc.MCMC(model.make_model(**data, db='hdf5', dbcomplevel=1, \
    dbcomplib='zlib', dbname='duffydb.hdf5')

DuffySampler.use_step_method(pymc.gp.GPEvaluationGibbs, DuffySampler.sp_sub_b, \
    DuffySampler.V_b, DuffySampler.tilde_fb)

DuffySampler.use_step_method(pymc.gp.GPEvaluationGibbs, DuffySampler.sp_sub_s, \
    DuffySampler.V_s, DuffySampler.tilde_fs)

DuffySampler.isample(50000,10000,100)    
\end{CodeInput}
The first line creates the variables specified in \code{model.py}, and hands them to an MCMC sampler called \code{DuffySampler}. The database backend chosen is \code{hdf5}, so dynamic traces will be stored in an on-disk \pkg{PyTables} database under compression \citep{pymc}. The middle two lines choose the \code{GPEvaluationGibbs} step method, described in Section~\ref{sec:gp-gibbs}, for updating the \code{Gaussian process}es. The final line calls for 50k MCMC iterations, with every 100 iterations after the first 10k saved to the database.

\bigskip
Most of the code in \code{mcmc.py} is devoted to estimating the mean and standard deviation of the posterior predictive distribution of Duffy negativity prevalence at unsampled locations $x$ in Africa and the Arabian peninsula (Figure~\ref{fig:duffymaps}). For each sample from the joint posterior of the variables in model~\ref{eq:duffy-model}, 
the pointwise mean and variance 
\begin{equation}
    \label{eq:map-moments} 
    \begin{array}{l}
        \E(f_b(x_i)|f_b(o), \phi_b, \theta_b, \beta)\\
        \VAR(f_b(x_i)|f_b(o), \phi_b, \theta_b, \beta)\\\\
        \E(f_s(x_i)|f_s(o), \phi_s, \theta_s)\\
        \VAR(f_s(x_i)|f_s(o), \phi_s, \theta_s)
    \end{array}
\end{equation}
are computed for each pixel $x_i$ in the output map. Independent samples of $f_b(x_i)$ and $f_s(x_i)$ at each $x_i$ are sufficient for mapping, and are much cheaper to produce than joint samples of $f_b(x)$ and $f_s(x)$, so the conditional covariances of $f_b(x)$ and $f_s(x)$ are not needed.

Samples for $\tilde f_b(x_i)$ and $\tilde f_s(x_i)$ are obtained for each element $x_i$ of $x$ using equation~\ref{eq:map-moments}, and the last three lines of model~\ref{eq:duffy-model} are used to convert these to posterior predictive samples of the frequency of the silencing variant, $p(s;x_i)$, and thence the frequency of the Duffy negativite phenotype, $p(s;x_i)^2$. The posterior predictive mean and variance of Duffy negativity are estimated from these samples, and these quantities are displayed on a map.

\bigskip
The algorithm just described is implemented simply and conveniently in \code{mcmc.py} thanks to the \code{remember} method of \pkg{PyMC} \code{MCMC} objects, and the \code{point_eval} function provided by this package. The \code{remember} method of \code{DuffySampler} is used to reset its constituent variables to each of the $n$ stored samples from the joint posterior distribution:
\begin{CodeInput}
for i in xrange(n):
    DuffySampler.remember(0,i)
\end{CodeInput}
Inside the loop, the pointwise means and variances of equation~\ref{eq:map-moments} are computed efficiently and in a multithreaded fashion (on multi-core systems, assuming the environment variable \code{OMP_NUM_THREADS} is set to a value greater than 1) using \code{point_eval}:
\begin{CodeInput}
    Msurf_b, Vsurf_b = pymc.gp.point_eval(DuffySampler.sp_sub_b.M_obs.value, \
        DuffySampler.sp_sub_b.C_obs.value, dpred)
    
    Msurf_s, Vsurf_s = pymc.gp.point_eval(DuffySampler.sp_sub_s.M_obs.value, \
        DuffySampler.sp_sub_s.C_obs.value, dpred)
\end{CodeInput}
The `nugget' variances $V_b$ and $V_s$ are added to the variances in equation~\ref{eq:map-moments} to obtain the conditional variances of $\tilde f_s$ and $\tilde f_b$:        
\begin{CodeInput} 
    Vsurf_b += DuffySampler.V_b.value
    Vsurf_s += DuffySampler.V_s.value
\end{CodeInput}
 Samples from these distributions are drawn, and converted to posterior predictive samples for the frequencies $p(s|b;x_i)$ and $p(b;x_i)$ at each pixel $x_i$:
\begin{CodeInput}
    freq_b = pymc.invlogit(Msurf_b +pymc.rnormal(0,1)*np.sqrt(Vsurf_b))
    freq_s = pymc.invlogit(Msurf_s +pymc.rnormal(0,1)*np.sqrt(Vsurf_s))
\end{CodeInput}
Finally, these samples are converted to a posterior predictive sample for Duffy negativity prevalence $p(s;x_i)^2=[p(s|b;x_i)p(b;x_i)+p(s|a;x_i)p(a;x_i)]^2$:
\begin{CodeInput}
    samp_i = (freq_b*freq_s+(1-freq_b)*DuffySampler.p1.value)**2
\end{CodeInput}
These samples are accumulated to produce estimates of the first two moments of the posterior predictive distribution of $p(s;x_i)^2$ for each $x_i$, and these moments are used to fill in the maps of the posterior predictive mean and standard deviation shown in Figure~\ref{fig:duffymaps}.

\begin{figure}
\begin{center}
    \epsfig{file=figs/duffymean.pdf, width=7cm} 
    \epsfig{file=figs/duffyvar.pdf, width=7cm}     
    \caption{The mean and variance of the posterior predictive distribution of Duffy negativity prevalence, $p(s;x)^2$ in the notation of model~\ref{eq:duffy-model}, in Africa and the Arabian peninsula. These maps were produced by \code{pymc/examples/gp/more_examples/Duffy/mcmc.py}. The locations of (simulated) observations are shown as red dots.}
    \label{fig:duffymaps}
\end{center}    
\end{figure}

% \begin{figure}
%     \centering
%         \epsfig{file=figs/elevmean.pdf, width=7cm}
%         \epsfig{file=figs/elevvar.pdf, width=7cm}
%     \caption{The posterior mean and variance surfaces for the $v$ variable of the Walker lake example. The posterior variance is relatively small in the neighborhood of observations, but large in regions where no observations were made.}
%     \label{fig:walker}
% \end{figure}
% \begin{figure}
%     \centering
%         \epsfig{file=figs/elevdraw0.pdf, width=7cm}
%         \epsfig{file=figs/elevdraw1.pdf, width=7cm}
%     \caption{Two realizations from the posterior distribution of the $v$ surface for the Walker Lake example. Elevation is measured in meters.}
%     \label{fig:walkerreal}
% \end{figure}
% Bayesian geostatistics is demonstrated in the folder \code{pymc/examples/gp/more_examples/Geostatistics}. File \code{getdata.py} downloads the Walker Lake dataset of Isaaks and Srivastava \citep{isaaks} from the internet and manipulates the $x$ and $y$ coordinates into the array format described in section \ref{sec:highdim}. File \code{model.py} contains the geostatistical model specification, which is
% 
% \begin{eqnarray*}
%     d|f \sim \textup{Normal}(f(x),V)\\
%     f|M,C \sim \textup{GP}(M,C) \\
%     M:x\rightarrow m\\
%     C:x,y,\mathtt{amp},\mathtt{scale},\mathtt{diff\_degree}\rightarrow \mathtt{matern.euclidean}(x,y;\mathtt{amp},\mathtt{scale},\mathtt{diff\_degree})\\
%     p(m)\propto 1\\
%     \mathtt{amp}\sim \textup{Exponential}(7e-5) \\
%     \mathtt{scale}\sim \textup{Exponential}(4e-3) \\
%     \mathtt{diff\_degree}\sim \textup{Uniform}(.5,2)\\ 
%     V\sim \textup{Exponential}(5e-9)\\
% \end{eqnarray*}
% 
% File \code{mcmc.py} fits the model and produces output maps.  The output of \code{mcmc.py} is shown in figures \ref{fig:walker} and \ref{fig:walkerreal}. Figure \ref{fig:walker} shows the posterior mean and variance of the $v$ variable of the dataset, which is a function of elevation (see \cite{isaaks}, appendix A).

\section{Conclusion}

This package is built around the \code{Realization} object, which represents random mathematical functions as random \proglang{Python} functions. This is arguably the most natural and intuitive representation possible within the \proglang{Python} programming language, which itself is widely regarded as unusually human-friendly. 

This design choice has concrete advantages that go beyond aesthetics. The Duffy negativity example in Section~\ref{sec:duffy} implements a probability model (model~\ref{eq:duffy-model}) that is not a member of the spatial generalized linear model family, whose nonstandard likelihood function involves transformations of two GPs. It would not be possible to implement in a model-based geostatistics package that fits spatial generalized linear models. However, this package focuses on providing a \pkg{Python} representation of Gaussian processes that closely resembles the mathematical objects rather than on covering any particular model family. Because model~\ref{eq:duffy-model} is straightforward to express in terms of those mathematical objects, it falls comfortably within this package's scope. The highly declarative implementation in \code{pymc/examples/gp/more_examples/Duffy/model.py} is little more than a transliteration from mathematical notation to \pkg{Python}. 

This package inherits the flexibility of \pkg{PyMC}, as described in Section~\ref{sec:gp-sub}. Because \pkg{PyMC} allows any \proglang{Python} function to be used to transform variables in probability models, and \proglang{Python} (like all modern programming languages) allows functions to be passed to other functions as arguments, and \code{GaussianProcess} objects are function-valued random variables, this package supports construction of a very wide variety of probability models that involve scalar-valued GPs. In keeping with the extensible spirit of \pkg{PyMC}, this package accomodates user-specified mean and covariance functions, and provides support for automatic combination of covariance functions and distance metrics.

Strenuous efforts at optimization have resulted in good performance for `standard' GP-based analyses such as Bayesian geostatistics. For example, \cite{map} recently used it to conduct a fully Bayesian analysis involving a very expensive covariance function evaluated at 4,291 input locations. The library of covariance functions provided by the package is implemented in Fortran, and can take advantage of SMP systems. The linear algebra functionality is provided by \pkg{NumPy}, which can be configured to make use of optimized, multithreaded \pkg{BLAS} \citep{blas} and \pkg{LAPACK} \citep{lapack}  implementations. 

However, there are many use cases for which this package cannot achieve performance on par with hand-optimized algorithms. For example, in many reversible-jump MCMC applications (e.g. \cite{gramacy}) the set of input locations under consideration changes as the MCMC progresses. It would be possible to create these models and start to fit them using this package, but the acceptance rate would typically be much lower than that enjoyed by a true reversible-jump MCMC algorithm for reasons explained in Section~\ref{sec:step-methods}. It remains to be seen whether this and related performance problems can be overcome without either diluting the conceptual fidelity of the object model or incurring an unmanageable amount of additional complexity.

More tutorial material, as well as documentation of several additional features and the incomplete Cholesky decomposition algorithms, can be found in the full package documentation at \href{http://pymc.googlecode.com/files/GPUserGuide.pdf}{http://pymc.googlecode.com/files/GPUserGuide.pdf}.
=======
The initialization arguments are the Gaussian process submodel that contains $f$, the observation variance of $f$'s children, and the children, in this case the vector-valued normal variable $d$. 

\code{GPEvaluationGibbs} covers the standard submodel encountered in geostatistics, but there are many conjugate situations to which it does not apply. If necessary, special step methods can be written to handle these situations. If \code{GPEvaluationGibbs} is not assigned manually, $f(x_*)$ will generally be handled by a wrapped version of \code{AdaptiveMetropolis} \citep{pymc}, which implements the algorithm of Haario, Saksman and Tamminenn \textbf{ref}.





\section{Geostatistical example}\label{sub:geostat}
\begin{figure}
    \centering
        \epsfig{file=figs/elevmean.pdf, width=5cm}
        \epsfig{file=figs/elevvar.pdf, width=5cm}
    \caption{The posterior mean and variance surfaces for the $v$ variable of the Walker lake example. The posterior variance is relatively small in the neighborhood of observations, but large in regions where no observations were made.}
    \label{fig:walker}
\end{figure}
\begin{figure}
    \centering
        \epsfig{file=figs/elevdraw0.pdf, width=5cm}
        \epsfig{file=figs/elevdraw1.pdf, width=5cm}
    \caption{Two realizations from the posterior distribution of the $v$ surface for the Walker Lake example. Elevation is measured in meters.}
    \label{fig:walkerreal}
\end{figure}
Bayesian geostatistics is demonstrated in the folder \code{pymc/examples/gp/more_examples/Geostatistics}. File \code{getdata.py} downloads the Walker Lake dataset of Isaaks and Srivastava \citep{isaaks} from the internet and manipulates the $x$ and $y$ coordinates into the array format described in section \ref{sec:highdim}. File \code{model.py} contains the geostatistical model specification, which is

\begin{eqnarray*}
    d|f \sim \textup{Normal}(f(x),V)\\
    f|M,C \sim \textup{GP}(M,C) \\
    M:x\rightarrow m\\
    C:x,y,\mathtt{amp},\mathtt{scale},\mathtt{diff\_degree}\rightarrow \mathtt{matern.euclidean}(x,y;\mathtt{amp},\mathtt{scale},\mathtt{diff\_degree})\\
    p(m)\propto 1\\
    \mathtt{amp}\sim \textup{Exponential}(7e-5) \\
    \mathtt{scale}\sim \textup{Exponential}(4e-3) \\
    \mathtt{diff\_degree}\sim \textup{Uniform}(.5,2)\\ 
    V\sim \textup{Exponential}(5e-9)\\
\end{eqnarray*}

File \code{mcmc.py} fits the model and produces output maps.  The output of \code{mcmc.py} is shown in figures \ref{fig:walker} and \ref{fig:walkerreal}. Figure \ref{fig:walker} shows the posterior mean and variance of the $v$ variable of the dataset, which is a function of elevation (see \cite{isaaks}, appendix A).

\section{Conclusion}

This package is built around the \code{Realization} object, which represents random mathematical functions as random \proglang{Python} functions. This is arguably the most natural and intuitive representation possible within the \proglang{Python} programming language, which itself is widely regarded as unusually human-friendly.

This package inherits the flexibility of PyMC, as described in section \ref{sec:gp-sub}. Because \pkg{PyMC} allows any \proglang{Python} function to be used to transform variables in probability models, and \proglang{Python} (like all modern programming languages) allows functions to be passed to other functions as arguments, and \code{GaussianProcess} objects are function-valued random variables, this package supports construction of a wides variety of probability models that involve scalar-valued Gaussian processes. The Duffy negativity example in section \ref{sub:geostat} implements a probability model that is not a member of the generalized linear model family, whose nonstandard likelihood function involves transformations of two Gaussian processes. In keeping with the extensible spirit of PyMC, the package accomodates user-specified covariance functions, and provides support for automatic combination of covariance functions and distance metrics.

Strenuous efforts at optimization have resulted in good performance for `standard' Gaussian process-based analyses such as Bayesian geostatistics. For example, \cite{map} recently used it to conduct a fully Bayesian MCMC analysis involving a very expensive covariance function evaluated at 4,291 input locations. The library of covariance functions provided by the package is implemented in Fortran, and can take advantage of SMP systems. The linear algebra functionality is provided by \code{NumPy}, which can be configured to make use of optimized, multithreaded BLAS and LAPACK \textbf{cite} implementations. 

However, there are many use cases for which this package cannot achieve performance on par with hand-optimized algorithms. For example, the \code{mesh} attribute of \texttt{GPSubmodel} objects is fixed on creation, but in many reversible-jump MCMC applications (e.g. \cite{gramacy} and the toy example above) the set of input locations under consideration changes as the MCMC progresses. It would be possible to fit these models using \texttt{GPSubmodel}, but the acceptance rate would typically be much lower than that enjoyed by a true reversible-jump MCMC algorithm for reasons explained in section \textbf{ref}. It remains to be seen whether this and related performance problems can be overcome without either diluting the conceptual fidelity of the object model or incurring an unacceptable level of program complexity.

More tutorial material, as well as documentation of several additional features and the incomplete Cholesky decomposition algorithms, can be found in the \href{http://pymc.googlecode.com/files/GPUserGuide.pdf}{package documentation}.
>>>>>>> gppaper

% \nocite{*}
% \bibliographystyle{plain}v
\bibliography{jss-gp}

\end{document}
