%!TEX TS-program = pdflatex
%!TEX TS-program = skim
%
%  PyMC User's Guide
%
%  Created by Chris Fonnesbeck on 2006-05-03.
%  Copyright (c) 2006 . All rights reserved.
%
\documentclass[]{manual}

% Use utf-8 encoding for foreign characters
%\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{pdfsync} 

% Flexible citation syntax
\usepackage{natbib}
% Uncomment some of the following if you use the features
%

% Multipart figures
%\usepackage{subfigure}

% More symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}

% Package for including code in the document
\usepackage{listings}

% Surround parts of graphics with box
%\usepackage{boxedminipage}

% This is now the recommended way for checking for PDFLaTeX:
%\usepackage{ifpdf}

% Enable hyperlinks
\usepackage[pdfpagemode=FullScreen,colorlinks=true,linkcolor=red]{hyperref}

%\ifpdf
%\usepackage[pdftex]{graphicx}
%\else
%\usepackage{graphicx}
%\fi

%%% EPYDOC STUFF %%%
\usepackage{underscore}
\usepackage[english]{babel}
\usepackage{alltt, parskip, boxedminipage}
\usepackage{makeidx, multirow, longtable, amssymb}% tocbibind}
\usepackage{fullpage}
\usepackage[usenames]{color}
\usepackage{ifthen}
\usepackage{ae}
\usepackage{aeguill}
\usepackage{shortvrb}
\usepackage{ucs}
\usepackage{tabularx}
\input{epydocheader}
%%% END OF EPYDOC STUFF %%%


\title{PyMC Gaussian process module \\User's guide}
\author{ Anand Patil }
\pdfoutput=1
% \date


%%%%%%%%%%%%%%% Commands from rst2latex %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\rubric}[1]{\subsection*{~\hfill {\it #1} \hfill ~}}
\newcommand{\titlereference}[1]{\textsl{#1}}
\newlength{\locallinewidth}
\setlength{\locallinewidth}{7in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document} 

\maketitle

\tableofcontents

\chapter{Introduction}\label{cha:introduction} % (fold)

Gaussian processes (GPs) are probability distributions for functions. In Bayesian statistics, they are often used as priors for functions whose forms are unknown because they can encode many types of knowledge about functions, yet remain much less restrictive than priors based on particular functional forms. GPs are not hard to understand at a conceptual level, but implementing them efficiently on a computer can require fairly involved linear algebra.

This package implements Gaussian processes as a set of Python classes that can support many types of usage, from intuitive exploration to embedding in larger probability models and fitting with MCMC.

% \section{Prerequisites}\label{sec:prerequisites}
% \begin{itemize}
%     \item Familiarity with \citetitle[www.python.org]{Python}. Several online tutorials can be found by clicking around the Python website, including \citetitle[www.ibiblio.org/obp/thinkCSpy/]{How to think like a computer scientist: Learning with Python} by Downey, Elkner and Meyers. More experienced programmers may prefer \citetitle[docs.python.org/tut/]{another tutorial} by Python's author. Python is generally regarded as having unusually good documentation for open-source software, and it is also considered one of the easiest general-purpose programming languages to learn.
%     \item Familiarity with Numerical Python, or \citetitle[www.scipy.org/numpy]{NumPy}. This package provides array and matrix objects and associated functions that are much faster than their equivalents in pure Python, and also more convenient for scientific and numerical work. If you're familiar with a matrix language such as Matlab or R, learning numpy shouldn't be too difficult. However, there are a few differences that can function as `gotchas' early on. Although numpy itself is available free of charge, its documentation costs \$40 and is available from \citetitle[www.scipy.org/Documentation]{the Scientific Python website}.
%     \item A conceptual understanding of Bayesian statistics. Some familiarity with the normal distribution would help. Gelman et al. \cite{gelman} and Berry \cite{berry} are good general references, as is \citetitle[http://www.ams.ucsc.edu/~draper/draper-BHM-2005.pdf]{Draper's book}. For a colorful online tutorial on the logical aspects of Bayesian statistics, see \citetitle[http://www.yudkowsky.net/bayes/bayes.html]{An Intuitive Explanation of Bayesian Reasoning}.
% \end{itemize}

% chapter introduction (end)

\chapter{Tutorial I: The basics}\label{cha:basics} % (fold)

All the code in the tutorial is in the folder \file{pymc/examples/gp} in the PyMC source tree.

\section{Mathematical functions and Python functions}\label{sec:functions}
A mathematical function is a rule that associates a single output value with each input value in a set \cite{rudin}. Examples are:
\begin{eqnarray*}
    f(x) = x^2 & \textup{for each value $x$ in the real numbers}  \\
    f(x) = \sin(x)& \textup{for each value $x$ in the real numbers}\\
    f(x,y) = x^2 - (y-1)^2 & \textup{for each pair of values $x$, each in the real numbers.}
\end{eqnarray*}
Note the important distinction between the actual function $f$ and the evaluation $f(x)$ of the function at a particular point. Each evaluation of a function is just a single output value, but a function itself is essentially an (often infinite) table of values. If you `look up' an input value in the table by evaluating the function, you'll find the corresponding output value.

One convenient way to visualize a function is by graphing it. The graphs of the first two functions above are curves. Their input and output values are single numbers, so the functions themselves are infinite tables of ordered pairs. The graph of the third function, on the other hand, is a surface. Its input value is a two-vector and its output value is a number, so the function itself is an infinite table of ordered triples. Ordered triples have to be plotted in three-dimensional space, so the graph ends up being a surface.

Python functions can emulate mathematical functions. Python representations of the mathematical functions above are:
\begin{verbatim}
def f(x):
    return x ** 2
    
def f(x):
    return sin(x)
    
def f(x,y):
    return x ** 2 - (y-1) ** 2
\end{verbatim}
These Python functions act like very large tables of values. If you `look up' an input value by passing it in as an argument, the function will tell you the corresponding output value. It would be very inefficient to store output values corresponding to each input value on a computer, so the functions have to figure out the output value corresponding to arbitrary input values on demand.


\section{A first look at Gaussian processes}\label{sec:firstlook}

Gaussian processes are probability distributions for mathematical functions. The statement `random function $f$ has a Gaussian process distribution with mean $M$ and covariance $C$' is usually written as follows:
\begin{equation}
    f\sim\textup{GP}(M,C).
\end{equation}
Gaussian processes have two parameters, which are analogous to the parameters of the normal distribution:
\begin{itemize}
    \item $M$ is the mean function. Like the mean parameter of the normal distribution, $M$ gives the central tendency for $f$. In Bayesian statistics, $M$ is usually considered a prior guess for $f$.
    \item $C$ is the covariance function. $C$ takes twice as many arguments as $f$; if $f$ is a function of one variable, $C$ is a function of two variables. Its role is harder to understand than that of the mean function, but among other things it regulates:
    \begin{itemize}
        \item the amount by which $f$ may deviate from $M$ at any input value $x$,
        \item the smoothness of $f$,
        \item the wiggliness of $f$.
    \end{itemize}
Also, $C(x,y)$ gives the covariance of $f(x)$ and $f(y)$, and $C(x,x)$ gives the variance of $f(x)$.
\end{itemize}
Section \ref{sec:cov} will look at covariance functions in more depth; for the time being don't worry about them too much.

As with any probability distribution, random values can be drawn from a Gaussian process. However, these values (called `realizations') are actually mathematical functions rather than the usual numbers or vectors. On the computer, the random values we draw will essentially be Python functions, with a few extra features.

\subsection{What are Gaussian processes good for?}\label{sub:applications}
Mathematical functions are ubiquitous in science. A very short list of examples:
\begin{itemize}
    \item Functional responses in predator-prey dynamics \cite{mathecol}. These functions associate a value for rate of prey capture with each value of predator population size.
    \item Transmission functions in epidemiology \cite{andersonmay}. These functions associate a value for rate of new infections with each value of infected and uninfected population size.
    \item Transfer functions in engineering \cite{duffy}. These functions associate a value for a ratio of Laplace transformed input and output signals with each value of the Laplace input variable $s$.
    \item Utility functions in microeconomics \cite{microecon}. These functions associate a value for a person's satisfaction with each portfolio of goods.
    \item Action potentials in neuroscience \cite{neuro}. These functions associate a value for transmembrane potential with each value of time since depolarization.
    \item The annual mean temperature at each point on the earth's surface in earth and atmospheric sciences.
\end{itemize}

The problem of estimating functions is equally widespread, because researchers frequently want to find out what each of the above functions is. In some cases, the phenomena underlying a function are simple and well-understood enough that the function can be derived up to a handful of parameters. For example, in Newtonian mechanics the height of a rock is known to be a parabolic function of the time since it was thrown, and the problem of inferring its trajectory reduces to the problem of inferring its initial height, its initial velocity and the acceleration due to gravity.

In many other cases it's not possible to deduce nearly as much about a function a priori. In some cases several candidate forms exist, but it's not possible to rule all of them out or even to ascertain that they are the only possibilities. As flexible and convenient probability distributions on function spaces, Gaussian processes are useful for Bayesian inference of functions without the need for reducing the problem to inference of a set of parameters.

\subsection{Instantiating a Gaussian process}\label{sub:inst}

In the following subsections we will instantiate objects representing a covariance function, a mean function, and finally several random functions drawn from the GP distribution defined by those objects.

\subsubsection{Instantiating a mean function}\label{subsub:mean}

\begin{figure}
    \centering
        \epsfig{file=figs/mean.pdf,width=10cm}
    \caption{The mean function generated by {\sffamily `examples/mean.py'}.}
    \label{fig:mean}
\end{figure}

The first component we will create is a mean function, represented by class \class{Mean}. The mean function of a univariate GP can be interpreted as a prior guess for the GP, so it's a univariate function also. The \class{Mean} class is a wrapper for an ordinary Python function. We will use the parabolic function
\begin{equation}
    M(x) = ax^2 + bx + c.
\end{equation}

The following code will produce an instance of class \class{Mean} called M:
\verbatiminput{../../examples/gp/mean.py}

The first argument to \class{Mean}'s init method is the underlying Python function, in this case \function{quadfun}. The extra arguments \code{a}, \code{b}  and \code{c} will be memorized and passed to \function{quadfun} whenever M is called; the call \texttt{M(x)} in the plotting portion of the script doesn't need to pass them in.

Mean functions broadcast over their arguments in the same way as numpy universal functions \cite{numpybook}, which means that \texttt{M(x)} will return the vector 
\begin{eqnarray*}
    \texttt{[M(x[0]),\ldots, M(x[N-1])]}.
\end{eqnarray*}

The last part of the code plots \texttt{M(x)} on $-1<\texttt{x}<1$, and its output is shown in figure \ref{fig:mean}. As expected, the plot is a parabola.

\subsubsection{Instantiating a covariance function}\label{subsub:cov}
\begin{figure}
    \centering
        \epsfig{file=figs/cov.pdf,width=15cm}
    \caption{The covariance function generated by {\sffamily `examples/cov.py'}. On the left is the covariance function $C(x,y)$ evaluated over a square: $-1\le x\le 1,\ -1\le y\le 1$. On the right is a slice of the covariance: $C(x,0)$ for $0\le x \le 1$}
    \label{fig:cov}
\end{figure}

GP covariance functions are represented by the class \class{Covariance}, which like \class{Mean} is essentially a wrapper for ordinary Python functions. In this example we will use the popular Mat\`ern covariance function, which is provided in module \module{cov_funs}. In addition to the two arguments \texttt{x} and \texttt{y}, this function takes three tunable parameters: \code{amp} controls the amount by which realizations may deviate from their mean, \code{diff_degree} controls the smoothness of realizations (the degree of differentiability), and \code{scale} controls the wiggliness of realizations.

You're free to write your own functions to wrap in \class{Covariance} objects. See section \ref{sec:usercov} for more information.

The code in \file{examples/cov.py} will produce an instance of class \class{Covariance} called C.
\verbatiminput{../../examples/gp/cov.py}

The first argument to \class{Covariance}'s init method, \function{eval_fun}, gives the Python function from which the covariance function will be made. In this case, \function{eval_fun} is \function{matern.euclidean}. The extra arguments \code{diff_degree, amp} and \code{scale} will be passed to \function{matern.euclidean} every time C is called.

At this stage, the covariance function C exposes a very simple user interface. In fact, it behaves a lot like the ordinary Python function \texttt{matern.euclidean} it wraps, except that like \texttt{Mean} it `memorizes' the parameters \code{diff_degree, amp} and \code{scale} so that you don't need to pass them in when you call it. Covariance functions' calling conventions are slightly different than ordinary numpy universal functions' \cite{numpybook}:
\begin{enumerate}
    \item Broadcasting works differently. If C were a numpy universal function, \texttt{C(x,y)} would return the following array:
    \begin{eqnarray*}
        \begin{array}{ccc}
            \texttt{[C(x[0],y[0])}& \ldots& \texttt{C(x[N-1],y[N-1])]},
        \end{array}
    \end{eqnarray*}
    where \texttt{x} and \texttt{y} would need to be vectors of the same length. In fact \texttt{C(x,y)} returns a matrix:
    \begin{eqnarray*}
        \left[\begin{array}{ccc}
            \texttt{C(x[0],y[0])}& \ldots& \texttt{C(x[0],y[Ny-1])}\\
            \vdots&\ddots&\vdots\\
            \texttt{C(x[Nx-1],y[0])}& \ldots& \texttt{C(x[Nx-1],y[Ny-1])}
        \end{array}\right],
    \end{eqnarray*}
    and input arguments \texttt{x} and \texttt{y} don't need to be the same length.
    \item You can call covariance functions with just one argument. \texttt{C(x)} returns
    \begin{eqnarray*}
         \texttt{[C(x[0],x[0])}& \ldots& \texttt{C(x[Nx-1],x[Nx-1])]} = \texttt{diag(C(x,x))},
    \end{eqnarray*}
    but is computed much faster than \texttt{diag(C(x,x))} would be.
\end{enumerate}

Most of the code in \file{examples/cov.py} is devoted to output, which is shown in figure \ref{fig:cov}. It plots the covariance function \texttt{C(x,x)} evaluated over a square, and also the `slice' \texttt{C(x,0)} over an interval. You'll notice that the graph of the full covariance function resembles a rounded A-frame tent.

\subsubsection{Drawing realizations}\label{subsub:realizations}
\begin{figure}
    \centering
        \epsfig{file=figs/realizations.pdf,width=10cm} 
    \caption{Three realizations from a Gaussian process displayed with mean $\pm$ 1 sd envelope. Generated by {\sffamily `examples/realizations.py'}.}
    \label{fig:realizations}
\end{figure}

Finally, let's generate some realizations (draws) from the Gaussian process defined by M and C and take a look at them. The following code will generate a list of instances of class \class{Realization}:
\verbatiminput{../../examples/gp/realizations.py}

    The init method of \class{Realization} takes only two required arguments, a \class{Mean} object and a \class{Covariance} object. Each element of \code{f_list} is a Gaussian process realization, which is essentially a randomly-generated Python function. Like \class{Mean} objects, \class{Realization} objects use the same broadcasting rules as numpy universal functions. Typing \texttt{f(x)} will return the vector 
\begin{eqnarray*}
    [\texttt{f(x[0])}\ldots \texttt{f(x[N-1])}].
\end{eqnarray*}
    

The plotting portion of the code calls the function \function{plot_envelope}, which summarizes some aspects of the distribution. The dashdot black line in the middle is M, and the gray band is the $\pm 1$ standard deviation envelope for f, generated by C. Each of the three realizations in \texttt{f_list} is a callable function, and they are plotted superimposed on the envelope. The plot output is shown in figure \ref{fig:realizations}. 


\section{The role of the covariance function}\label{sec:cov} 
The following covariance functions for Euclidean coordinates are included in the module \module{cov_funs}:
\begin{itemize}
    \item \texttt{matern.euclidean}
    \item \texttt{sphere.euclidean}
    \item \texttt{pow_exp.euclidean}
    \item \texttt{gaussian.euclidean} (The name `gaussian' doesn't imply that this one is particularly special.)
    \item \texttt{quadratic.euclidean}
\end{itemize} 
See section 2.1.3 of \citetitle[http://www.statsnetbase.com/ejournals/books/book_summary/summary.asp?id=1285]{Banerjee et al.} \cite{banerjee} for more information on each of these. Each covariance function takes at least two parameters, called \texttt{amp} and \texttt{scale}. The following covariance functions take extra parameters:
\begin{description}
    \item[\texttt{matern}:] \texttt{diff_degree}
    \item[\texttt{pow_exp}:] \texttt{pow}
    \item[\texttt{quadratic}:] \texttt{phi}. 
\end{description}

In this section we'll focus on the Mat\`ern family, as it is generally considered the state of the art. Its popularity is due to the fact that it has three parameters, each of which clearly controls one of three important properties of realizations: roughness, wiggliness and amplitude.

In this section we'll set the mean function to zero (more precisely, a function whose output value is zero regardless of input) in order to focus on the covariance. This:
\begin{eqnarray*}
    f\sim\textup{GP}(M,C)
\end{eqnarray*}
is equivalent to this:
\begin{eqnarray*}
    f = M + g, & g\sim \textup{GP}(0,C), 
\end{eqnarray*}
so it's not difficult to adapt the intuition we gain in this section to GPs with nontrivial mean functions.

The covariance functions listed above are \emph{stationary} and \emph{isotropic}. Intuitively, that means our a priori expectation of how $f$ will deviate from its mean doesn't vary with location or with the direction in which we look (for functions of several variables). Section \ref{sec:usercov} describes how these restrictions can be relaxed.

All the figures in this section were produced using the file \file{examples/cov_params.py}. You can follow along by editing the line of that file which reads
\begin{verbatim}
C = Covariance(eval_fun = matern.euclidean, diff_degree = 1.4, amp = 1., scale = 1.)
\end{verbatim}

The actual formulas for the covariance functions are given in section \ref{sec:usercov}, but the Mat\`ern formula is fairly inscrutable (though its Fourier transform is much more readable \cite{stein}). This section will try to help you understand it graphically.

\subsection*{The \texttt{amp} and \texttt{scale} parameters}\label{sub:ampscale}

These parameters are common to all covariance functions provided by this package, not just \texttt{matern.euclidean}. Please see section \ref{sec:usercov} if you're planning on writing your own covariance functions, as this package provides utilities that will endow them with these parameters.

As mentioned in section \ref{subsub:cov}, the covariance function plotted in figure \ref{fig:cov} resembles a rounded A-frame tent. The width of this tent controls how tightly nearby evaluations of a realization f will be coupled to each other. If the tent is wide, \texttt{f(x)} and \texttt{f(y)} will tend to have similar values when \texttt{x} and \texttt{y} are close to one another. If the tent is narrower, \texttt{f(x)} and \texttt{f(y)} won't be as tightly correlated. The height of the tent controls the overall amplitude of f's deviation from its mean.

The mathematical definition of the covariance function is as follows:
\begin{equation}
    \label{covdef} 
    C(x,y)=\textup{cov}(f(x), f(y)).
\end{equation}
That implies the following:
\begin{eqnarray*}
    \textup{var}(f(x))=C(x,x).
\end{eqnarray*}
By the definition of variance, for any real number $a$
\begin{eqnarray*}
    \textup{var}( a f(x))=a^2 C(x,x).
\end{eqnarray*}

The covariance function is multiplied by \texttt{amp}$^\texttt{2}$, and this effectively multiplies realizations by \texttt{amp}. In other words, a larger \texttt{amp} parameter means that realizations will deviate further from their mean. The effects of changing the \texttt{amp} parameter are illustrated in figure \ref{fig:amp}.  
\begin{figure}
    \centering
        \epsfig{file=figs/d14a1s1.pdf, width=8cm}
        \epsfig{file=figs/d14a4s1.pdf, width=8cm}
    \caption{Mat\`ern covariances with \texttt{diff_degree=1.4}, \texttt{scale=1}, and \texttt{amp=1} (left) and 4 (right), and corresponding realizations. The amplitude of realizations tends to scale with \texttt{amp}, and the amplitude of the covariance function scales with \texttt{amp}$^2$. Note the scales on the plots of the covariance functions.}
    \label{fig:amp}
\end{figure}

By a trivial substitution,
\begin{eqnarray*}
    \textup{cov}(f(sx),f(sy))=C(sx,sy).
\end{eqnarray*}
Whenever a call \texttt{C(x,y)} is made, the input arrays \texttt{x} and \texttt{y} are multiplied by the \texttt{scale} parameter before being passed to the underlying covariance function. In our one-dimensional example this effectively stretches the realizations in the \texttt{x} direction. If \texttt{scale} is large, the function will be correlated over a larger distance and will not `wiggle' as quickly. Figure \ref{fig:scale} illustrates the effects of the scale parameter.
\begin{figure}
    \centering
        \epsfig{file=figs/d14a1s1close.pdf, width=5cm}
        \epsfig{file=figs/d14a1s4.pdf, width=5cm}
        \epsfig{file=figs/d14a1s4far.pdf, width=5cm}        
    \caption{Mat\`ern covariances with \texttt{diff_degree=1.4}, \texttt{amp=1} and \texttt{scale=1} (left) and \texttt{scale=4} (center, right), and corresponding realizations. A larger value of \texttt{scale} stretches both covariance functions and realizations (center), so that realizations don't wiggle as rapidly. When the stretched functions are plotted on commensurately stretched axes (right), they look like the unstretched functions again.}
    \label{fig:scale}
\end{figure}

\subsection{The \texttt{diff_degree} parameter}\label{sub:diffdegree}
The \texttt{diff_degree} parameter, usually denoted $\nu$, is unique (amongst the covariance functions provided by this package) to the Mat\`ern family of covariance functions. It controls the sharpness of the ridge of the covariance function, which controls the roughness/smoothness of realizations.

More specifically, look at the slices \texttt{C(x,0)} that are shown in the upper right-hand panels of the subfigures in figures \ref{fig:amp} and \ref{fig:scale}. If \texttt{diff_degree} is greater than an integer \texttt{n}, this slice is \texttt{2n} times differentiable at \texttt{x=0}. It turns out that this means realizations will be \texttt{n} times differentiable.

It's natural to ask what happens when \texttt{diff_degree} isn't an integer. There is such a thing as \citetitle[http://en.wikipedia.org/wiki/Fractional_calculus]{fractional calculus}, which deals with things like taking half a derivative. There are several definitions of fractional derivatives and integrals, and I don't know the connections between them and the \texttt{diff_degree} parameter (please email me if you do).  Stein \cite{stein} discusses the connection briefly. See also Miller and Ross \cite{fraccalc}.

At any rate, it's safe to say that \texttt{diff_degree} is a roughness index that can be interpreted as a degree of differentiability when it is an integer. Figure \ref{fig:diffdegree} illustrates the effects of changing this parameter:
\begin{description}
    \item[0 (not shown):] \texttt{C(x,y)=1} if \texttt{y=x}, \texttt{0} if $\texttt{x}\ne \texttt{y}$. Not only are realizations not differentiable, they're not even continuous. \texttt{f(x)} is an independent normal random variable for each value of \texttt{x}.
    \item[.2:] \texttt{C(x,0)} is not even one time differentiable at its peak, where \texttt{x=0}. Realizations are very rough, but continuous.
    \item[.5:] If \texttt{diff_degree} is just larger than \texttt{.5}, \texttt{C(x,0)} is barely differentiable at \texttt{x=0}. Realizations, however, aren't differentiable; their roughness is comparable to trajectories of Brownian particles. When \texttt{diff_degree} is equal to \texttt{.5}, \texttt{matern} is equivalent to another covariance function, \texttt{pow_exp}, with extra argument \texttt{pow=1}.
    \item[1:] If \texttt{diff_degree} is just larger than \texttt{1}, \texttt{C(x,0)} is barely twice differentiable at \texttt{x=0}, and realizations are just barely differentiable.
    \item[1.4:] The value from figures \ref{fig:amp}, \ref{fig:cov} and \ref{fig:scale} is shown for comparison.
    \item[2:] If \texttt{diff_degree} is just larger than \texttt{2}, \texttt{C(x,0)} is barely four times differentiable at \texttt{x=0}, and realizations are barely twice differentiable.
    \item[10:] Realizations are very smooth. As \texttt{diff_degree} approaches infinity, \texttt{matern} gets closer to \texttt{gaussian}. Realizations from GPs with Gaussian covariances are infinitely differentiable. In fact, if \texttt{diff_degree} is larger than 10 \texttt{matern} simply calls \texttt{gaussian}, because it's much faster. Please email me if this is a problem.
\end{description}
  
\begin{figure}
    \centering
        \epsfig{file=figs/d2a1s1.pdf,width=8cm} 
        % \epsfig{file=figs/d4a1s1.pdf,width=4cm}       
        \epsfig{file=figs/d5a1s1.pdf,width=8cm}         
        % \epsfig{file=figs/d8a1s1.pdf,width=4cm}       
        \epsfig{file=figs/d10a1s1.pdf,width=8cm}        
        \epsfig{file=figs/d14a1s1close.pdf,width=8cm}       
        \epsfig{file=figs/d20a1s1.pdf,width=8cm}        
        \epsfig{file=figs/d100a1s1.pdf,width=8cm}       
    \caption{Matern draws for various \texttt{diff_degree} parameters. In increasing orders of smoothness: \texttt{.2}, \texttt{.5}, (equivalent to \texttt{pow_exp} with \texttt{pow=1}, roughness similar to Brownian motion), \texttt{1}, \texttt{1.4} (the examples given so far), \texttt{2}, \texttt{10} (nearly equivalent to \texttt{gaussian}).}
    \label{fig:diffdegree}
\end{figure}

\subsection{Suggestions for further experimentation} 
\label{sec:experiment} 


\begin{itemize}
    \item Change the parameters of the Mat\`ern covariance function, and try to guess how realizations will look.
    \item Differentiate realizations numerically. Recall that 
    \begin{eqnarray*}
        \frac{df}{dx}=\lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h},
    \end{eqnarray*}
    so you can take an approximate `numerical derivative' by evaluating the fraction on the right hand side with $h$ equal to a small number.
    \item Replace \texttt{matern.euclidean} in \file{examples/cov_params.py} with the Euclidean version of one of the other covariance functions and experiment with the parameters. See Banerjee \cite{banerjee} for their interesting properties.
    \item Replace \function{zero_fun} in \file{cov_params.py} with a nontrivial mean function and repeat the preceding.   
\end{itemize}

\section{Nonparametric regression: observing Gaussian processes}\label{sec:observing} 

\begin{figure}
    \centering
        \epsfig{file=figs/obs.pdf,width=8cm}
        \epsfig{file=figs/cond.pdf,width=8cm}
    \caption{The output of {\sffamily `examples/observations.py'}: the observed GP with \texttt{obs_V = .002} (left) and \texttt{obs_V = 0} (right). Note that in the conditioned case, the $\pm$ 1 SD envelope shrinks to zero at the points where the observations were made, and all realizations pass through the observed values. Compare these plots to those in figure \ref{fig:realizations}.}
    \label{fig:obs}
\end{figure}

Consider the following common statistical situation: You decide on a GP prior for an unknown function $f$, then you observe the value of $f$ at $N$ input points $[o_0\ldots o_{N-1}]$, possibly with uncertainty. If the observation error is normally distributed, it turns out that $f$'s posterior distribution given the new information is another Gaussian process, with new mean and covariance functions. 

The probability model that represents this situation is as follows:
\begin{equation}
    \label{regprior}
    \left.\begin{array}{l}
        \textup{data}_i \stackrel{\tiny{\textup{ind}}}{\sim} \textup{N}(f(o_i), V_i)\\
        f \sim \textup{GP}(M,C)\\
    \end{array}\right\}\Rightarrow f|\textup{data} \sim \textup{GP}(M_o, C_o).
\end{equation}
This package provides a function called \function{observe} that imposes normally-distributed observations on Gaussian process distributions. This function converts $f$'s prior to its posterior by transforming $M$ and $C$ in equation \ref{regprior} to $M_o$ and $C_o$:

The following code imposes the observations
\begin{eqnarray*}
    \texttt{f(-.5) = 3.1}, & \texttt{V=.002}\\
    \texttt{f(.5) = 2.9}, & \texttt{V=.002}
\end{eqnarray*}
on the GP distribution defined in \file{mean.py} and \file{cov.py}:
\verbatiminput{../../examples/gp/observation.py} 

The function \function{observe} takes a covariance \code{C} and a mean \code{M} as arguments, and essentially tells them that their realizations' values on \code{obs_mesh} have been observed to be \code{obs_vals} with variance \code{obs_V}. If \code{obs_V} is \code{None}, \function{observe} assumes that the observation precision was infinite; that is, that the realizations' values on \code{obs_mesh} were observed with no uncertainty. Making (or pretending to make) observations with infinite precision is sometimes called \emph{conditioning}, and can be a valuable tool for modifying GP priors; for example, if a rate function is known to be zero when a population's size is zero.

The output of the code is shown in figure \ref{fig:obs}, along with the output with \code{obs_V=None}. Compare these to the analogous figure for the unobserved GP, figure \ref{fig:realizations}. The covariance after observation is visualized in figure \ref{fig:obscov}. The covariance `tent' has been pressed down at points where $\texttt{}\approx \pm \texttt{.5}$ and/or $\texttt{y}\approx\pm \texttt{.5}$, which are the values where the observations were made.

\begin{figure}
    \centering
        \epsfig{file=figs/obscov.pdf,width=10cm}
    \caption{The covariance function from {\sffamily `examples/observation.py'} after observation. Compare this with the covariance function before observation, visualized in figure \ref{fig:cov} }
    \label{fig:obscov}
\end{figure}

\subsection{Example: Salmonid stock-recruitment functions}\label{sub:MMKregression}
Munch, Kottas and Mangel \cite{mmk} use Gaussian process priors to infer various \emph{stock-recruitment (SR) functions}. An important concept in fishery science, SR functions relate the size of a fish stock to the number or biomass of recruits to the fishery each year. In other words, they relate population size or biomass to number or biomass of new fish produced. The authors argue that model uncertainty is endemic in stock-recruitment theory, and that in this situation GP priors are a sensible alternative to particular functional forms.

We don't have the tools yet to fully duplicate Munch, Kottas and Mangel' results; that will have to wait for chapter \ref{cha:adv}. However, we can fit a simpler version of their model now by using the \function{observe} function. Specifically, we'll fit the data in figure 6 of their paper \cite{mmk}, which is for three salmonids: chum (\emph{Onchorhynchus keta}), pink (\emph{Onchorhynchus gorbuscha}) and sockeye (\emph{Onchorhynchus nerka}). 

The code is in the script \file{examples/gp/more_examples/MMKsalmon/regression.py}. The script begins by importing the \class{salmon} class from \file{salmon.py} in the same directory.
The \class{salmon} class does the following:
\begin{itemize}
    \item Reads data from a csv file.
    \item Creates a GP prior with a Mat\`ern covariance function and a linear mean function. The parameters are chosen fairly arbitrarily at this stage. In chapter \ref{cha:adv}, we'll look at how to place priors on these parameters and infer them along with the unknown function itself.
    
    Note also that I've specified the prior using the data; for instance, the \texttt{scale} parameter depends on the maximum observed abundance. Some would consider this cheating.
    \item `Observes' the unknown function's value to be zero at the origin with no uncertainty. No matter what the data are, every draw from the posterior will have \texttt{f(0) = 0}. This isn't really an observation, it's just a convenient way to incorporate the knowledge that if there is no stock, there will be no recruitment.
    \item Provides a \method{plot} method, which just plots the posterior envelope, data and three realizations from the posterior.
\end{itemize}
% The code in \file{examples/more_examples/MKMsalmon/salmon.py} is shown here: 
% \verbatiminput{../../examples/gp/more_examples/MKMsalmon/salmon.py}

\begin{figure}
    \centering
        \epsfig{file=figs/MMKchumreg.pdf,width=10cm}
        \epsfig{file=figs/MMKsockeyereg.pdf,width=10cm}
        \epsfig{file=figs/MMKpinkreg.pdf,width=10cm}                
    \caption{Fits to the stock-recruitment data in Munch, Kottas and Mangel' \cite{mmk} Figure 6 using a simple nonparametric regression.}
    \label{fig:MMKregression}
\end{figure}

The main script, \file{examples/more_examples/MKMsalmon/salmon.py}, creates three \class{salmon} instances called \texttt{chum}, \texttt{pink} and \texttt{sockeye}, then imposes the data for each species on its prior to obtain its posterior. The observation variance I used was chosen fairly arbitrarily like the prior parameters, and we'll look at inferring it in chapter \ref{cha:adv} also. Finally, each species' \method{plot} method is called. Output is shown in figure \ref{fig:MMKregression}.
% The code in \file{examples/more_examples/MKMsalmon/regression.py} is shown here:
% \verbatiminput{../../examples/gp/more_examples/MKMsalmon/regression.py}

To reiterate, there are some major drawbacks to this simple model. The observation variance may not actually be known, and we may not be comfortable specifying a single value for each of the prior parameters. Because of these considerations, Munch, Kottas and Mangel opt for a more sophisticated statistical model that has to be fit using MCMC. We will follow them in section \ref{sub:MMKMCMC}. 

\section{Higher-dimensional GPs}\label{sec:highdim} 

In addition to functions of one variable such as $f(x)$, this package supports Gaussian process priors for functions of many variables such as $f(\mathbf{x})$, where $\mathbf{x}=[x_0\ldots x_{n-1}]$. This is useful for modeling dynamical or biological functions of many variables as well as for spatial statistics.

Any time you pass an array into a \texttt{Mean}, \texttt{Covariance} or \texttt{Realization}'s init method or evaluate one of these objects on an array, the convention is that the array's last index iterates over spatial dimension. To evaluate a covariance C on the ordered pairs \texttt{(0,1)}, \texttt{(2,3)}, \texttt{(4,5)} and \texttt{(6,7)}, you could pass in the following two-dimensional array:
\begin{verbatim}
[[0,1]
 [2,3]
 [4,5]
 [6,7]]
\end{verbatim}
or the following three-dimensional array:
\begin{verbatim}
[[[0,1]
  [2,3]],

  [4,5]
  [6,7]]]
\end{verbatim}
Either is fine, since in both the last index iterates over elements of the ordered pairs.

The exception to this rule is one-dimensional input arrays. The array
\begin{verbatim}
[0, 1, 2, 3, 4, 5, 6, 7]
\end{verbatim}
is interpreted as an array of eight one-dimensional values, whereas the array
\begin{verbatim}
[[0, 1, 2, 3, 4, 5, 6, 7]]
\end{verbatim}
is interpreted as a single eight-dimensional value according to the convention above.

Means and covariances learn their spatial dimension the first time they are called or observed. Some covariances, such as those specified in geographic coordinates, have an intrinsic spatial dimension. Realizations inherit their spatial dimension from their means and covariances when possible, otherwise they learn it the first time they are called. If one of these objects is subsequently called with an input of a different dimension, it raises an error.

\subsection{Covariance function bundles and coordinate systems}
The examples so far, starting with \file{examples/cov.py}, have used the covariance function \texttt{matern.euclidean}. This function is an attribute of the \texttt{matern} object, which is an instance of class \class{covariance_function_bundle}.

Instances of \class{covariance_function_bundle} have three attributes, \texttt{euclidean}, \texttt{geo_deg} and \texttt{geo_rad}, which correspond to standard coordinate systems:
\begin{itemize}
    \item \texttt{euclidean}: $n$-dimensional Euclidean coordinates.
    \item \texttt{geo_deg}: Geographic coordinates (longitude, latitude) in degrees, with radius 1.
    \item \texttt{geo_rad}: Geographic coordinates (longitude, latitude) in radians, with radius 1.
\end{itemize}
Note that you can effectively change the radius of the geographic coordinate systems using the \texttt{scale} parameter.

Covariance function bundles are described in more detail in section \ref{sec:usercov}.

\subsection{Multithreading GP operations}
This package can use multi-core systems to speed up two kinds of computations:
\begin{itemize}
	\item filling in covariance matrices,
	\item linear algebra for observing GP's or drawing realizations.
\end{itemize}
If you've built numpy against multithreaded linear algebra libraries, all the linear algebra will be parallelized automatically. The functions contained in covariance function bundles (section \ref{sec:usercov}), which include all the covariance functions distributed with this package, take an argument called \texttt{n_threads} in addition to their parameters. The covariance function in \file{examples/gp/cov.py} can be given 4 threads as follows:
\begin{verbatim}
	C = Covariance(eval_fun = matern.euclidean, diff_degree = 1.4, amp = .4, scale = 1., n_threads = 4)
\end{verbatim}

On a quad-core system, evaluating this covariance function on large input vectors will use all the cores. Note that there's no point setting \texttt{n_threads} to 5 on a quad-core system, because only four cores are available.

 
\subsection{A geostatistical example}\label{sub:geostat}
\begin{figure}
    \centering
        \epsfig{file=figs/elevmean.pdf, width=8cm}
        \epsfig{file=figs/elevvar.pdf, width=8cm}
    \caption{The posterior mean and variance surfaces for the elevation example. Elevation is measured in meters. Computing these surfaces is also called `\citetitle[http://en.wikipedia.org/wiki/Kriging]{kriging}.' The posterior variance is small in the neighborhood of observations, but large in regions where no observations were made. Note that this posterior distribution is conditional on a relatively sparse dataset, which notably misses some of the highest points in Europe.}
    \label{fig:elev}
\end{figure}
\begin{figure}
    \centering
        \epsfig{file=figs/elevdraw0.pdf, width=8cm}
        \epsfig{file=figs/elevdraw1.pdf, width=8cm}
    \caption{Two realizations from the posterior distribution of elevation. Elevation is measured in meters.}
    \label{fig:elevreal}
\end{figure}
Higher-dimensional usage is demonstrated in the folder \file{examples/more_examples/Elevation}. File \file{getdata.py} extracts latitude, longitude and elevation data from the csv file in that folder and manipulates them into the array format described in section \ref{sec:highdim}. The data are from the Carbon Dioxide Information Analysis Center's \citetitle[http://cdiac.ornl.gov/epubs/ndp/ndp026d/ndp026d.html]{Cloud Climatology for Land Stations Worldwide, 1971-1996}. File \file{makemap.py} interfaces with \citetitle[www.matplotlib.org]{matplotlib}'s \texttt{basemap} toolkit, which draws maps. File \file{regression.py} actually implements the regression.
% \verbatiminput{../../examples/gp/more_examples/Elevation/regression.py}  

The output of \file{regression.py} is shown in figures \ref{fig:elev} and \ref{fig:elevreal}. Figure \ref{fig:elev} shows the posterior mean and variance of the elevation. Finding these surfaces is also known as `\citetitle[http://en.wikipedia.org/wiki/Kriging]{kriging}.' Note that it's not possible to visualize the full covariance matrix, because it's now a function of four variables. The surface in figure \ref{fig:elev} is equivalent to the diagonal of the surface in figure \ref{fig:obs}.

Figure \ref{fig:elevreal} shows two realizations from the posterior distribution of the elevation surface. They are much rougher than their mean surface. Evaluating realizations is much more expensive than evaluating the observed mean or evaluating the observed covariance with one argument.


\section{Basis covariances}\label{sec:basis}

\begin{figure}[htbp]
    \centering
        \epsfig{file=figs/basiscov.pdf,width=10cm}
        \caption{Three realizations of an observed Gaussian process whose covariance is an instance of \class{BasisCovariance}. The basis in this case is function \function{fourier_basis} from module \module{cov_funs}. 25 basis functions are used.}
    \label{fig:basiscov}
\end{figure}

It's possible to create random functions from linear combinations of finite sets of basis functions $\{e\}$ with random coefficients $\{c\}$:
\begin{eqnarray*}
    f(x) = M(x) + \sum_{i_0=0}^{n_0-1}\ldots \sum_{i_{N-1}=0}^{n_{N-1}-1} c_{i_1\ldots i_{N-1}} e_{i_1\ldots i_{N-1}}(x), &
    \{c\}\sim \textup{some distribution}. 
\end{eqnarray*}
If the distribution is multivariate normal with mean zero, $f$ is a Gaussian process with mean $M$ and covariance defined by
\begin{eqnarray*}
    C(x,y)=\sum_{i_0=0}^{n_0-1}\ldots \sum_{i_{N-1}=0}^{n_{N-1}-1} \sum_{j_0=0}^{n_0-1}\ldots \sum_{j_{N-1}=0}^{n_{N-1}-1} e_{i_0\ldots i_{N-1}}(x) e_{j_1\ldots j_{N-1}}(x) K_{i_0\ldots i_{N-1}, j_1\ldots j_{N-1}},
\end{eqnarray*}
where $K$ is the covariance of the coefficients $c$.

Particularly successful applications of this general idea (shown in one dimension) are:
\begin{description}
    \item[Random Fourier series:] $e_i(x) = \sin(i\pi x/L)$ or $\cos(i\pi x/L)$, for instance \cite{spanos}.
    \item[Gaussian process convolutions:] $e_i(x) = \exp(-(x-\mu_n)^2)$, for instance \cite{convolution}.
    \item[B-splines:] $e_i(x) = $ a polynomial times an interval indicator. See \citetitle[http://en.wikipedia.org/wiki/Basis_B-spline]{Wikipedia}'s article.
\end{description}
Such representations can be very efficient when there are many observations in a low-dimensional space, but are relatively inflexible in that they generally produce realizations that are infinitely differentiable. In some applications, this tradeoff makes sense.

This package supports basis representations via the \class{BasisCovariance} class:
\begin{verbatim}
    C = BasisCovariance(basis, cov, **basis_params)
\end{verbatim}
The arguments are:
\begin{description}
    \item[\texttt{basis}:] Must be an array of functions, of any shape. Each basis function will be evaluated at \texttt{x} with the extra parameters. The basis functions should obey the same calling conventions as mean functions: return values should have shape \code{x.shape[:-1]} unless \texttt{x} is one-dimensional, in which case return values should be of the same shape as \texttt{x}. Note that each function should take the entire input array as an argument.
    \item[\texttt{cov}:] An array whose shape is either:
        \begin{itemize}
            \item Of the same shape as \texttt{basis}. In this case the coefficients are assumed independent, and \texttt{cov[i[0],...,i[N-1]]} (an $N$-dimensional index) simply gives the prior variance of the corresponding coefficient.
            \item Of shape \texttt{basis.shape * 2}, using Python's convention for tuple multiplication. In this case \texttt{cov[i[0],...,i[N-1], j[0],...,j[N-1]]} (a $2N$-dimensional index) gives the covariance of $c_{i_0\ldots i_{N-1}}$ and $c_{j_1\ldots j_{N-1}}$.
        \end{itemize} 
        Internally, the basis array is ravelled and this covariance tensor is reshaped into a matrix; I have made the input convention this way because it seems easier to keep track of which covariance value corresponds to which coefficients. The covariance tensor must be symmetric (\texttt{cov[i[0],...,i[N-1], j[0],...,j[N-1]]} $=$ \texttt{cov[j[0],...,j[N-1], i[0],...,i[N-1]]}), and positive semidefinite when reshaped to a matrix.
    \item[\texttt{basis_params}:] Any extra parameters required by the basis functions.
\end{description}

\section{Separable bases}

Many bases, such as Fourier series, can be decomposed into products of functions as follows:
\begin{eqnarray*}
    e_{i_0\ldots i_{N-1}}(x) = e^0_{i_0}(x)\ldots e^{N-1}_{i_{N-1}}(x)
\end{eqnarray*}
Basis covariances constructed using such bases can be represented more efficiently using \texttt{SeparableBasisCovariance} objects. These objects are constructed just like \texttt{BasisCovariance} objects, but instead of an $n_0\times \ldots \times n_{N-1}$ array of basis functions they take a nested lists of functions as follows:
\begin{verbatim}
    basis = [ [e[0][0], ... ,e[0][n[0]-1]]
                       ...
              [e[N-1][0], ... ,e[N-1][n[N-1]-1]] ].
\end{verbatim}
For an $N$-dimensional Fourier basis, each of the \texttt{e}'s would be a sine or cosine; frequency would increase with the second index. As with \texttt{BasisCovariance}, each basis needs to take the entire input array \texttt{x} and \texttt{basis_params} as arguments. See \texttt{fourier_basis} in \texttt{RandomRealizations/cov_funs/bases.py} for an example.

\section{Example} 

Once created, a \class{BasisCovariance} or \class{SeparableBasisCovariance} object behaves just like a \class{Covariance} object, but it and any \texttt{Mean} and \texttt{Realization} objects associated with it will take advantage of the efficient basis representation in their internal computations. An example of \class{SeparableBasisCovariance} usage is given in \file{examples/basis_cov.py}, shown below. Compare its output in figure \ref{fig:basiscov} to that of \file{examples/observation.py}. 
\verbatiminput{../../examples/gp/basiscov.py}

% chapter basics (end)


\chapter{Tutorial II: More advanced topics}\label{cha:adv} % (fold)

\section{The Gaussian process and the multivariate normal distribution} 

The Gaussian process generalizes the multivariate normal distribution from vectors to functions, like the multivariate normal distribution generalizes the univariate normal distribution from scalars to vectors. The progression is as follows:
\begin{equation}
    \begin{array}{ll}
        y\sim\textup N(\mu,V): & \textup{$y$, $\mu$, $V$ are scalars}\\\\
        \vec y\sim\textup N(\vec \mu,C): & \textup{$\vec y$ and $\vec \mu$ are vectors, $C$ is a matrix}\\\\
        f\sim\textup{GP}(M, C): & \textup{$f$ and $M$ are functions of one variable, $C$ is a function of two variables}
    \end{array}
\end{equation}

One of the nice things about all Gaussian distributions (parameterized by mean and covariance) is that they're easy to marginalize. For example, each element of a vector with a multivariate normal distribution has a univariate normal distribution:
\begin{eqnarray*}
    \vec y\sim\textup N(\vec \mu,C)\\
    \Rightarrow \vec y_i\sim\textup N(\vec \mu_i,C_{i,i}),
\end{eqnarray*}
and any subvector of a vector with a multivariate normal distribution has a multivariate normal distribution:
\begin{eqnarray*}
    \vec y\sim\textup N(\vec \mu,C)\\
    \Rightarrow \vec y_{i_1\ldots i_2}\sim\textup N(\vec \mu_{i_1\ldots i_2},C_{i_1\ldots i_2,i_1\ldots i_2}).
\end{eqnarray*}

This marginalizability applies to GP's as well. If $\vec x$ is a vector of values,
\begin{equation}
    \begin{array}{l}
        f\sim\textup{GP}(M, C)\\\\
        \Rightarrow f(\vec x) \sim\textup N(M(\vec x), C(\vec x,\vec x)).
    \end{array}
\end{equation}
In other words, any evaluation of a Gaussian process realization has a multivariate normal distribution. Its mean is the corresponding evaluation of the associated mean function, and its covariance is the corresponding evaluation of the associated covariance function. You can probably start to see why this fact is important for working with GPs on a computer. For the details of this package's implementation, see the algorithm documentation available from \citetitle[code.google.com/p/random-realizations]{code.google.com/p/random-realizations}.

\subsection{Observations}
As mentioned earlier, if $f$ has a GP prior and normally-distributed observations of $f$ are made at a finite set of values, $f$'s posterior is a Gaussian process also:
\begin{eqnarray*}
    \left.\begin{array}{l}
        d_i \stackrel{\tiny{\textup{ind}}}{\sim} \textup{N}(f(o_i), V_i)\\
        f \sim \textup{GP}(M,C)
    \end{array}\right\}\Rightarrow f|d \sim \texttt{GP}(M_o, C_o).    
\end{eqnarray*}
Denoting by $o$ the array of observation values $[o_0\ldots o_{N-1}]$ and $V$ the array of observation variances $[V_0\ldots V_{N-1}]$, $M_o(x)$ and $C_o(x,y)$ are as follows for arbitrary input vectors $x$ and $y$:
\begin{eqnarray*}
    M_o(x) = M(x) + C(x,o)[C(o,o) + V]^{-1}(f(o)-M(o))\\
    C_o(x,y) = C(x,y) - C(x,o)[C(o,o) + V]^{-1}C(o,y).
\end{eqnarray*}

\subsection{Low-rank observations} 

If $C(o,o)+V$ is singular, some elements of $f(o)$ can be computed from others with no uncertainty. In other words, there exists a partition $[o_*, o_{**}]$ of $o$ and corresponding partition of the data such that $C(o_*,o_*)+V$ is full-rank and
\begin{eqnarray*}
    f(o_{**}) = M_{o_*}(o_{**}),
\end{eqnarray*}
where $M_{o_*}$ can be computed from the formula above.

In such cases, this package's strategy is to observe $f$ at a subvector $o_*$ of $o$, such that $C(o_*,o_*)+V$ is full-rank but if any elements were added to $o_*$ it would become nearly singular. The function \function{predictive_check} optionally checks the remaining data values $d_{**}$ against $M_{o_*}(o_{**})$, and raises an error if the two aren't equal up to a user-defined threshold. 

This threshold is parameter \code{relative_precision} from \texttt{Covariance}'s init method, multiplied by the largest value on the diagonal of $C(o_*,o_*)+V$; intuitively, the maximal variance of $f(o_{**})$ given $f(o_*)$ is a multiple of the maximal variance of $f(o_*)$.


\section{Incorporating Gaussian processes in larger probability models with PyMC}\label{sec:PyMC} 

This section will show you how to build and fit statistical models that go beyond simple nonparametric regression.

\subsection{The \class{GP} class}
\class{GP} is a subclass of the PyMC \class{Stochastic} class whose value attribute is a \class{Realization} instance. Its init method takes the following arguments:
\begin{description}
    \item[$M$:] A mean object or mean-valued deterministic variable.
    \item[$C$:] A covariance object or covariance-valued deterministic variable.
    \item[\texttt{mesh=None}:] An array or an array-valued variable.
    \item[\texttt{init_mesh_vals=None}:] An optional vector of initial values for the value attribute's evaluation on \texttt{mesh}.
    \item[\texttt{mesh_eval_isdata=False}:] If \texttt{True}, the value attribute's evaluation on \texttt{mesh} is fixed at \texttt{init_mesh_vals}.
    \item[\texttt{doc, name, trace, cache_depth, verbose}:] Optional arguments that are passed directly to \code{Stochastic.\_\_init\_\_}. See PyMC's documentation.
\end{description}

\texttt{GP} instances have a log-probability attribute like any PyMC stochastic variable, but there is an important difference: If the value attribute is a realization called $f$, the log-probability attribute gives $p(f\texttt{(mesh))}|M,C)$. In other words, the log-probability attribute only cares about $f$'s evaluation on \texttt{mesh}. The reason is simple: it would be expensive and difficult to assign something like a log-density to entire realizations in most cases. 

\subsection{The \class{GPMetropolis} and \class{GPParentMetropolis} step methods}

The a price of the `cop-out' of computing \texttt{GP}s' log-probabilities based only on their evaluation on a mesh is that the Metropolis-Hastings algorithm doesn't apply. \texttt{GPMetropolis} and \texttt{GPParentMetropolis} employ a strategy that will be described informally here.

Although they're contained in the same object, think about splitting the value $f$ of a \texttt{GP} into two pieces: an array $f$\texttt{(mesh)}, and a function $\tilde f$ defined on all values not in \texttt{mesh}. Denote the parents of the \class{GP} by $P$ and the children by $K$.

\subsubsection{GPMetropolis} 
The Metropolis-Hastings acceptance ratio for a proposed value $f_p$ can be written as follows:
\begin{eqnarray*}
    \frac{p(K|\tilde f_p, f_p(\mathtt{mesh}))\ p(\tilde f_p|f_p(\mathtt{mesh}), P)\ p(f_p(\mathtt{mesh}) | P)\ q(\tilde f|f(\mathtt{mesh}))\ q(f(\mathtt{mesh}))}{p(\mathtt{K}|\tilde f, f(\mathtt{mesh}))\ p(\tilde f|f(\mathtt{mesh}), P)\ p(f(\mathtt{mesh}) | P)\ q(\tilde f_p|f(\mathtt{mesh}))\ q(f_p(\mathtt{mesh}))},
\end{eqnarray*}
where $q$ denotes proposal densities. We want to avoid computing all terms with $\tilde f$ or $\tilde f_p$ in the consequent position:
\begin{eqnarray*}
    p(\tilde f_p|f_p(\texttt{mesh}), P),\\ q(\tilde f|f(\texttt{mesh})),\\ p(\tilde f|f(\texttt{mesh}), P),\\ q(\tilde f_p|f(\texttt{mesh})),
\end{eqnarray*}
but all other terms are fine. We can make these terms cancel by choosing our proposal distribution as follows:
\begin{eqnarray*}
    q(\tilde f_p|f(\texttt{mesh})) = p(\tilde f_p|f_p(\texttt{mesh}), P).
\end{eqnarray*}
In other words, if we propose $\tilde f$ from its prior distribution conditional on $f(\texttt{mesh})$ and its parents whenever we propose $f(\texttt{mesh})$, we don't have to worry about computing the intractable terms. This is what \class{GPMetropolis} does. 

\class{GPMetropolis} reports its competence to handle \texttt{GP} instances as \texttt{3}, so it will be chosen as the default handler. Its init method takes the following arguments:
\begin{description}
    \item[\texttt{stoch}:] The \class{GPMetropolis} instance to be handled.
    \item[\texttt{scale=.1}:] $f(\texttt{mesh})$ will be proposed from a random-walk multivariate normal distribution with covariance equal to \texttt{C(mesh, mesh * scale * _asf)}, where C is the covariance-valued parent and \texttt{_asf} is the adaptive scaling factor, which is updated when \texttt{self.tune()} is called.
    \item[\texttt{verbose = 0}:] An integer from 0 to 3 indicating the preferred verbosity level.
\end{description}

\subsubsection{GPParentMetropolis} 
Similarly, the Metropolis-Hastings acceptance ratio for a proposed value $P_p$ of the parents \emph{and} a proposed value $\tilde f_p$ for the function off the mesh is as follows:
\begin{eqnarray*}
    \frac{p(K|\tilde f_p)\ p(\tilde f_p|f(\texttt{mesh}), P_p)\ p(f(\texttt{mesh}) | P_p)\ q(\tilde f_p|f(\texttt{mesh}),f_p, P_p)\ q(P)}{p(K|\tilde f)\ p(\tilde f|f(\texttt{mesh}), P)\ p(f(\texttt{mesh}) | P)\ q(\tilde f_p|f(\texttt{mesh}),f)\ q(P_p)}
\end{eqnarray*}
By choosing the same proposal distribution for $\tilde f$ as above, we again avoid having to compute the intractable terms. In other words, every time a value is proposed for a \class{GP}'s parent, a value must be proposed for the \class{GP}  conditional on its value's evaluation on its mesh, and the prior probability of the \class{GP}'s children must be included in the acceptance ratio. 

To implement this strategy, \class{GPParentMetropolis} wraps an instance of a \class{Metropolis} subclass. It replaces its host class's \texttt{propose} and \texttt{reject} methods with special methods that call the host's native methods, and then propose and reject values for $\tilde f$ conditional on \texttt{f(mesh)}. \class{GPParentMetropolis} also adds the \texttt{GP}'s children to its host method's children, so that the likelihood term $p(K|\tilde f)$ is included in the acceptance ratio.

\class{GPParentMetropolis} reports its competence to handle parents of \class{GP} instances as \texttt{3}, so it will be chosen as the default handler. However, its init method will choose the host method by checking the step method registry for a method that would be competent to handle the parent if it had ordinary children. Its init method takes the following arguments: 
\begin{description}
    \item[\texttt{stochastic}:] The stochastic variable to handle. Must be a parent of a \class{GP} instance.
    \item[\texttt{scale = 1.}:] This parameter will be passed to the host method's init method.
    \item[\texttt{verbose = 0}:] An integer from 0 to 3 indicating the preferred verbosity level.
\end{description}

This scheme works if $K$ depends on $P$ and if $P$ has children other than the \class{GP}, though these possibilites aren't included in the rejection ratio above. 

\subsubsection{Choosing a mesh} A useful mental model for these step methods is as follows: The mesh points of a \class{GP} instance are the points where you `grab' its value to propose it. If the variable's mesh is \texttt{None}, its value will be proposed from its prior, and rejection rates are likely to be quite large. If the mesh is too dense, on the other hand, computation of the log-probability will be expensive (it scales as the cube of the number of points in the mesh). If the mesh is so dense that \texttt{C(mesh, mesh)} is numerically singular, an error will be raised in the init method. This continuum is illustrated in figure \ref{fig:meshpropose}. Finding the happy medium will require some experimentation.

Another important point to bear in mind is that if the \texttt{GP}'s children depend on its value only via its evaluation on the mesh, the likelihood terms $p(K|\tilde f_p)$ and $p(K|\tilde f)$ will cancel. In other words, if the mesh is chosen so that $p(K|f)=p(K|f(\texttt{mesh}))$ then the proposed value of $\tilde f$ will have no bearing on the acceptance probability of the proposed value of \texttt{f(mesh)} or of the parents P. Such a mesh choice will generally improve the acceptance rate.

\begin{figure}
    \centering
        \epsfig{file=figs/nomeshpropose.pdf,width=9cm}
        \epsfig{file=figs/lightmeshpropose.pdf,width=9cm}
        \epsfig{file=figs/densemeshpropose.pdf,width=9cm}        
    \caption{Several possible proposals of $\tilde f$ (curves) given proposed values for $f(\texttt{mesh})$ (heavy dots) with no mesh (top), a sparse mesh (middle), and a dense mesh (bottom). Proposal distributions' envelopes are shown as shaded regions, with means shown as broken lines. With no mesh, $\tilde f$ is being proposed from its prior and the acceptance rate will be very low. A denser mesh permits a high degree of control over $\tilde f$, but computing the log-probability will be more expensive.}
    \label{fig:meshpropose}
\end{figure}


\subsection{The \class{GPNormal} step method}
As we've already seen in other contexts, a \class{GP}'s full conditional distribution is a Gaussian process if its Markov blanket is described by the following probability model:
\begin{eqnarray*}
    K_i |f \sim \textup{N}(f(o_i), V_i) & i=0\ldots n-1\\
    f|M,C\sim\textup{GP}(M,C)
\end{eqnarray*}
In other words, each of its children $K_i$ is normally distributed with variance $V_i$ and mean equal to $f(o_i)$, where $o_i$ is an arbitrary mesh. 

In this case, a \texttt{GP} instance $f$ can be handled by the Gibbs step method \class{GPNormal}, which will have much better mixing properties that \class{GPMetropolis}. Its init method takes the following parameters:
\begin{description}
    \item[$f$:] The \texttt{GP} instance to be handled.
    \item[\texttt{obs_mesh:}] An array or array-valued variable giving the observation mesh $o$.
    \item[\texttt{obs_V}:] An array or array-valued variable giving the observation variance $V$. 
    \item[\texttt{obs_vals}:] An array or array-valued variable giving the concatenation of the values of the children $K_i$.
\end{description}

\class{GPNormal} doesn't register itself, so it will never be assigned as a default handler. This may change eventually.

\class{GPNormal} doesn't care about $f$'s mesh, because it never has to evaluate $f$'s log-probability. However, a good choice of mesh is important because it will used by the \class{GPParentMetropolis} instances that handle $f$'s mean and covariance parameters.  

\subsection{Example: A simple extension of nonparametric regression}\label{sub:BasicMCMC}
The files \file{examples/PyMCModel.py} and \file{examples/MCMC.py} show \class{GP} and the three step methods listed above in action. The probability model created by \file{examples/PyMCModel.py} is illustrated as a directed acyclic graph in figure \ref{fig:unobservedModel}. If you comment the last line, $f$ will be handled by a default \class{GPMetropolis} instance. If you leave it uncommented, $f$ will be handled by the \class{GPNormal} instance \texttt{S}.  

The main script \file{examples/MCMC.py} is mostly devoted to plotting, and its output is shown in figure \ref{fig:MCMCOutput} for both the \class{GPMetropolis} and \class{GPNormal} step methods. Both files are a bit long, so I won't quote them here. 
% \verbatiminput{../../examples/gp/PyMCModel.py}

\begin{figure}
    \centering
        \epsfig{file=figs/unobservedModel.pdf, width=10cm}
    \caption{The PyMC-generated directed acyclic graph representation of the extended nonparametric regression model created by \textsf{`examples/PyMCModel.py'} . Ellipses represent \class{Stochastic} objects (variables whose values are unknown even if their parents' values are known), triangles represent \class{Deterministic} objects (variables whose values can be determined if their parents' values are known), and rectangles represent \class{Stochastic} objects with the \member{isdata} flag set to \member{True} (data). Arrows point from parent to child, and arrow labels show the name assigned to the parent by the child. For instance, \class{C} considers \class{C_amp} to be its `\class{amp}' parameter.}
    \label{fig:unobservedModel}
\end{figure}

\begin{figure}
    \centering
        \epsfig{file=figs/gibbsSamples.pdf,width=10cm}
        \epsfig{file=figs/metroSamples.pdf,width=10cm}          
    \caption{The output of {\sffamily `examples/MCMC.py'} using the \class{GPNormal} (top) and \class{GPMetropolis} (bottom) step methods. Note that the Metropolis samples take several iterations to `burn in' to their dense support, whereas the Gibbs samples jump there more or less immediately. Both the Metropolis and Gibbs samples' jumping distribution is modulated by the value of the prior parameters, which are updated by \class{GPParentMetropolis} step methods.}
    \label{fig:MCMCOutput}
\end{figure}

\subsection{Example: Munch, Kottas and Mangel's stock-recruitment study}\label{sub:MMKMCMC}

\begin{figure}
    \centering
        \epsfig{file=figs/MKMsalmon.pdf, width=10cm}
    \caption{A PyMC-generated directed acyclic graph representation of the probability model in (\ref{eqn:MMKModel}), which is implemented in file \textsf{`examples/more_examples/MKMSalmon/salmon_sampler.py'}. A very similar probability model was used by Munch, Kottas and Mangel to infer stock-recruitment functions for three salmonid species. The label `\texttt{pink.SR}' indicates that this particular model corresponds to the pink salmon (\emph{Onchorhynchus gorbuscha}) data. Note that two coordinate transformations are implemented using PyMC deterministic variables. This technique can help lazy programmers avoid transforming priors by hand, and in less trivial cases it can save computation by caching the transformed parameters.}
    \label{fig:MMKsalmonmodel}
\end{figure}

\begin{figure}
    \centering
        \epsfig{file=figs/pinkfpost.pdf, width=10cm}
    \caption{The posterior of the stock-recruitment function for pink salmon (\emph{Onchorhynchus gorbuscha}). The data are shown as heavy black dots. The centered 95\%, 50\% and 25\% posterior probability intervals are shown as shaded regions. Three draws from the posterior are plotted.}
    \label{fig:pinkfpost}
\end{figure}

\begin{figure}
    \centering
        \epsfig{file=figs/pinkbeta0post.pdf, width=5cm}
        \epsfig{file=figs/pinkbeta1post.pdf, width=5cm}        
        \epsfig{file=figs/pinkamppost.pdf, width=5cm}        
        \epsfig{file=figs/pinkdiffdegreepost.pdf, width=5cm}        
        \epsfig{file=figs/pinkscalepost.pdf, width=5cm}        
        \epsfig{file=figs/pinkVpost.pdf, width=5cm}        
    \caption{The marginal posterior distributions of the mean and covariance parameters of the stock-recruitment function for pink salmon (\emph{Onchorhynchus gorbuscha}).}
    \label{fig:pinkparams}
\end{figure}

We now have the tools to duplicate Munch, Kottas and Mangel's \cite{mmk} stock-recruitment results. The full probability we'll use follows. It is like Munch, Kottas and Mangel's probability model, but we will use a Mat\`ern covariance function. Here \texttt{SR} is the stock-recruitment function, C is its covariance and M is its mean.
\begin{equation}
    \label{eqn:MMKModel}
    \begin{array}{ll}
        \texttt{frye}[i] \stackrel{\tiny{\textup{ind}}}{\sim} \textup{N}(exp(\mathtt{SR}(\log(\mathtt{abundance}[i]))), V),& i=1\ldots n\\
        \texttt{SR}\sim \textup{GP}(M,C)& \\
        M:x\rightarrow \beta_0+\beta_1 \log(x)&\\
        C:x,y\rightarrow \texttt{matern.euclidean}(x,y;\ \texttt{diff_degree},\texttt{amp},\texttt{scale})&\\
        V,\ \beta_0,\ \beta_1,\ \texttt{diff_degree},\ \texttt{amp},\ \texttt{scale} \sim \texttt{priors}.&
    \end{array}
\end{equation} 
The `priors' can be found by reading their paper (except the prior on \texttt{diff_degree}, which I chose based on what seems reasonable to me) or by reading the file \file{examples/more_examples/MMKSalmon/salmon_sampler.py}. This file contains a PyMC \class{Sampler} subclass called \class{SalmonSampler}, which reads in the data, creates a probability model incorporating the data, and provides some plotting methods.

Note that this model differs from the model in section \ref{sub:MMKregression} in that we're putting a Gaussian process prior on the stock-recruitment function in log-log space, so conditioning its value at zero isn't an option. 

The probability model \class{SalmonSampler} creates is visualized as a directed acyclic graph in figure \ref{fig:MMKsalmonmodel}. Munch, Kottas and Mangel specify priors for $\texttt{amp}^{-2}$ and $V^{-1}$, and I've used PyMC deterministic variables to conveniently implement the transformations rather than changing variables manually. The posterior distribution of \texttt{SR} for the pink salmon (\emph{Onchorhynchus gorbuscha}) is shown in figure \ref{fig:pinkfpost}, and the posterior of the mean and covariance parameters for the same species are shown in figure \ref{fig:pinkparams}.

% \subsection{Example: Ellner, Seifu and Smith's blowfly study}\label{sub:ESSMCMC}
% 
% \textbf{I haven't been able to get this one to mix yet.} The model is:
% \begin{eqnarray*}
%     \textup{data}_t \stackrel{\tiny{\textup{ind}}}{\sim}(A_t, V)\\
%     A_t = B(A_{t-\tau})\psi_{t-\tau} - D(A_{t-1})\phi_{t-1}, & t>\tau\\
%     B \sim \textup{GP}(sx\exp(-rx), C_B) \\
%     D \sim \textup{GP}(mx, C_D) \\
%     \psi_t \stackrel{\tiny{\textup{iid}}}{\sim}\textup{lognorm}(\mu_\phi,V_\phi) \\
%     \phi_t \stackrel{\tiny{\textup{iid}}}{\sim}\textup{lognorm}(\mu_\phi,V_\phi)\\    
%     C_B = \textup{Mat\`ern}(\sigma_B,\nu_B,\phi_B) \\
%     C_D = \textup{Mat\`ern}(\sigma_D,\nu_D,\phi_D)   \\  
%     A_{1\ldots\tau},r,s,m,\textup{covariance parameters},\mu_\phi,\mu_\psi,V_\phi,V_\psi,V \sim \textup{priors},
% \end{eqnarray*}
% and I assumed $\tau$ fixed for purposes of the demo. The DAG schematic is shown in figure \ref{fig:ESSblowflymodel}. 
% 
% What happens is the following: when $B$ and $D$ have mean zero, the model burns in and mixes badly. When they have the mean functions parametrized above, the amplitude of their covariance params get very small; the model is preferring the simpler parametric version, since it can explain the data as well as the nonparametric version. This would make a great demo for both the importance of a flexible mean function and the natural parsimony of Bayesian statistics, but I can't even get the parametric version mixing and don't have time for week-long runs. We'll see what happens with this.
% 
% \begin{figure}
%     \centering
%         \epsfig{file=figs/ESSblowfly.pdf,width=15cm}
%     \caption{The model for Ellner, Seifu and Smith's \cite{ess} blowfly data.}
%     \label{fig:ESSblowflymodel}
% \end{figure}
% 
% % \begin{figure}
% %     \centering
% %         \epsfig{file=figs/ESSBPosterior.pdf,width=10cm}
% %         \epsfig{file=figs/ESSDPosterior.pdf,width=10cm}        
% %     \caption{caption}
% %     \label{fig:ESSBD}
% % \end{figure}
% % 
% % \begin{figure}
% %     \centering
% %         \epsfig{file=figs/ESSmuphiPosterior.pdf,width=7cm}
% %         \epsfig{file=figs/ESSVphiPosterior.pdf,width=7cm}
% %         \epsfig{file=figs/ESSmupsiPosterior.pdf,width=7cm}
% %         \epsfig{file=figs/ESSVpsiPosterior.pdf,width=7cm}        
% %     \caption{caption}
% %     \label{fig:ESSphipsi}
% % \end{figure}
% % 
% % \begin{figure}
% %     \centering
% %         \epsfig{file=figs/ESSblowflyfit.pdf,width=15cm}
% %     \caption{caption}
% %     \label{fig:ESSfit}
% % \end{figure}
% 

\section{Extending the covariance functions: Writing your own, using alternate coordinate systems, building in anisotropy and nonstationarity}\label{sec:usercov} 
This section will give you a brief tour of the \module{cov_funs} module and point out plugs where you can add functionality that isn't included in this package. 

\bigskip
Note that you don't need to understand this section; any covariance function that satisfies the calling convention described in section \ref{subsub:cov} will work. The \texttt{cov_funs} module is convenient, but it is admittedly complicated.

\bigskip
The \class{covariance_function_bundle} class includes versions of each covariance function using several different distance metrics. For example, the Euclidean version of the Mat\`ern covariance function is \texttt{matern.euclidean} and the Gaussian covariance function in geographic coordinates (in radians) is \texttt{gaussian.geo_rad}. 

In addition, it has two attributes that are included for extensibility: the \texttt{raw} attribute, which exposes the basic Fortran implementation of each function, and \texttt{add_distance_metric} method, which combines the \texttt{raw} attribute with distance metrics. This section will describe these attributes in more detail.

Class \texttt{covariance_function_bundle}'s init method takes three arguments: 
\begin{description}
	\item[\texttt{cov_fun_name}] The name of the covariance function.
	\item[\texttt{cov_fun_module}] The name of the module in which the covariance function can be found. This must be somewhere on your \texttt{PYTHONPATH}.
	\item[\texttt{extra_cov_params}] A dictionary whose keys are the extra parameters the covariance function takes, and whose values are brief sentences explaining the roles of those parameters. This dictionary will be used to generate the docstring of the bundle.
\end{description}
The names of the function and module are used rather than the function object itself in order to allow covariance objects to be pickled. This, in turn, allows covariance-valued stochastic variables to be stored in PyMC's \texttt{pickle} and \texttt{hdf52} database backends.


\subsection{The components of a covariance function bundle}

\subsubsection{The actual covariance functions}\label{sub:distances}
The covariance functions contained in a bundle are named by the distance metric they use. The \texttt{matern} bundle, for instance, contains the following covariance functions:
\begin{itemize}
	\item \texttt{matern.euclidean}
	\item \texttt{matern.geo_rad}
	\item \texttt{matern.geo_deg}
	\item \texttt{matern.aniso_geo_rad}
	\item \texttt{matern.aniso_geo_deg}
	\item \texttt{matern.paniso_geo_rad}	
	\item \texttt{matern.paniso_geo_deg}
\end{itemize}

Each covariance function takes at least two arguments, $x$ and $y$, which are two-dimensional arrays in which the first index iterates over separate points and the second iterates over coordinates as in section \ref{sub:geostat}. For the geographic distance functions, the first coordinate is longitude and the second is latitude. Covariance functions also take arguments \texttt{amp} and \texttt{scale}. Finally, they take optional arguments \texttt{n_threads}, which sets the maximum number of threads available for the evaluation of the output matrix, and \texttt{symm}, which indicates whether $x$ and $y$ the same array. 

Some of the covariance functions take extra arguments for the distance metric and/or actual covariance function. For example, \texttt{matern.aniso_geo_rad} takes extra arguments \texttt{inc} and \texttt{ecc} for the distance function and \texttt{diff_degree} for the covariance function.

When \texttt{matern.euclidean}, for example, is called with input locations $x$ and $y$, the following events place:
\begin{itemize}
	\item The output matrix is allocated.
	\item The output matrix is filled with the Euclidean distances between the elements of $x$ and the elements of $y$.
	\item The output matrix is overwritten with the covariances between the elements of $x$ and the elements of $y$.
\end{itemize}
The last two steps will be executed in parallel if \texttt{n_threads} is greater than 1 and the size of the output matrix is sufficiently large.

The easiest way to deal with different coordinate systems or to build in anisotropy and nonstationarity using a deformation approach \cite{sampson} is to write your own distance function and add it to the covariance function by calling the \texttt{add_distance_metric} method. See \ref{sub:add_distance_metric} for information on how to use your distance function with an existing covariance function, or with one of your own devising. See \ref{sub:user_ccs} for the calling conventions required from distance functions.  

\subsubsection{The \member{raw} attribute}\label{sub:raw}
The \member{raw} attribute of each bundle is the underlying covariance function. These functions take distance matrices as arguments and overwrite them in-place as covariance matrices. If you write your own raw covariance function, it should conform to the standard described in section \ref{sub:user_ccs}.

The following raw functions are implemented in Fortran in \file{isotropic_cov_funs.f} and wrapped as covariance function bundles in \module{cov_funs}. Here $t$ denotes a single element of a distance matrix, $|x_i-y_j|$. Each function is equal to $1$ when $t=0$.
\begin{description}
    \item[\texttt{matern($\nu,t$)}:] The argument \texttt{diff_degree} is written in the following formula as $\nu$ for readability. $K_\nu$ is a modified Bessel function of the third kind of order $\nu$.
    \begin{eqnarray*}
        \frac{(2\sqrt{\nu}t)^\nu}{2^{\nu-1}\Gamma(\nu)}K_\nu(2\sqrt{\nu}t)
    \end{eqnarray*}
    \item[\texttt{quadratic($\texttt{phi},t$)}:]
    \begin{eqnarray*}
        1-\frac{t^2}{1+\phi t^2}
    \end{eqnarray*}
    \item[\texttt{gaussian($t$)}:]
    \begin{eqnarray*}
        e^{-t^2}
    \end{eqnarray*}
    \item[\texttt{pow_exp(\texttt{pow},$t$)}:] The argument \texttt{pow} is written as $p$.
    \begin{eqnarray*}
        e^{-|t|^p}
    \end{eqnarray*}
    \item[\texttt{sphere(t)}:]
    \begin{eqnarray}
        \left\{
        \begin{array}{ll}
            1-\frac{3}{2}t+\frac{1}{2}t^3& t\le 1\\
            0 & t > 1
        \end{array} \right.
    \end{eqnarray}
\end{description}

See \citetitle[http://www.statsnetbase.com/ejournals/books/book_summary/summary.asp?id=1285]{Banerjee et al.} \cite{banerjee} , \citetitle[http://www.leg.ufpr.br/mbgbook/]{Diggle and Ribeiro} \cite{diggle} and \citetitle[http://books.google.com/books?id=5n_XuL2Wx1EC&dq=stein+some+theory+for+kriging&printsec=frontcover&source=web&ots=829kgTWuC6&sig=gFu5_Gg4_eZ4yzFT4BBWamSanBA#PPP1,M1]{Stein} \cite{stein}for more discussion of the properties of these covariance functions. The init method of \texttt{covariance_function_bundle} takes just one argument, a raw covariance function.

\subsubsection{The \texttt{add_distance_metric} method and the \texttt{apply_distance} function}\label{sub:add_distance_metric}
\texttt{apply_distance} takes two arguments, a distance function and a raw covariance function, and returns a covariance function suitable for wrapping in a \class{Covariance} object. 

It endows the new function with a docstring that combines the raw covariance function's information with the distance function's information. It's possible to include information about extra parameters in the auto-generated docstring. To wrap a raw covariance function with a distance function called \texttt{my_dist_fun} with extra parameters called \texttt{eta} and \texttt{gamma} you could add explanations to \texttt{my_dist_fun} before calling \texttt{apply_distance}:
\begin{verbatim}
my_dist_fun.extra_params = {'eta': 'The eta parameter.', 
                            'gamma': 'The gamma parameter.'}
\end{verbatim}
The new parameters will be included in the docstring.

Covariance functions generated by \texttt{apply_distance} take the following arguments:
\begin{itemize}
    \item Input arrays $x$ and $y$. These high-level covariance functions are more forgiving than the distance functions; $x$ and $y$ do not actually need to be two-dimensional, the only requirement is that their last index must iterate over coordinates as in section \ref{sub:geostat}. The exception to this rule is if $x$ and $y$ are only one dimensional, in which case they are assumed to have only one coordinate.
    \item Scaling arguments \texttt{amp} and \texttt{scale}. Parameter \texttt{amp} is the standard deviation of $f(x)$ for arbitrary $x$, regardless of coordinate system, before observation (the prior standard deviation). Parameter \texttt{scale} controls the lengthscale of decay of the correlation, which controls the wiggliness of $f$. It effectively multiplies distances, so that large values yield quick decay and more wiggliness. These scaling arguments are sometimes referred to as the `sill' and `range' parameters.
    \item Extra arguments for the covariance and/or distance functions.
\end{itemize}

The \texttt{add_distance_metric} method of \class{covariance_function_bundle} is just a thin wrapper for the function \texttt{apply_distance}, which is in \file{cov_utils.py}. Since \class{covariance_function_bundle} objects already have a raw covariance function, this method only needs a distance function. 



\subsection{Calling conventions for new covariance and distance functions}
\label{sub:user_ccs}

User-defined covariance and distance functions should always take the following arguments:
\begin{description}
	\item[$C$] A Fortran-contiguous (column major) array of dimension (\texttt{x.shape[0]},\texttt{y.shape[0]}). This should be overwritten in-place.
	\item[$x$, $y$] Arrays of input locations. These will be regularized: they will be two-dimensional, with the first index iterating over points and the second over coordinates.
	\item[\texttt{cmin}$=0$, \texttt{cmax}$=-1$] Optional arguments. If non-default values are provided, only the slice \texttt{C[:,cmin:cmax]} of $C$ should be overwritten.
	\item[\texttt{symm=False}] An optional argument indicating whether $x$ and $y$ are the same array. If \texttt{True}, $C$ will be square and only the upper triangle of $C$ should be overwritten. 
\end{description}
They can take any other arguments they need, of course. 

If parallel computation is desired, covariance and distance functions must release the \citetitle[http://www.python.org/doc/1.5.2/api/threads.html]{global interpreter lock}. That means they have to be written in Fortran or C. \citetitle[http://cens.ioc.ee/projects/f2py2e/]{f2py} extensions can release the global interpreter lock by simply including the statement \texttt{cf2py threadsafe}; other types of extensions should call the Python C-API macros \texttt{Py_BEGIN_ALLOW_THREADS} and \texttt{Py_END_ALLOW_THREADS}.

The high-level covariance functions produced by \texttt{apply_distance} will be more forgiving than these low-level distance and covariance functions; $x$ and $y$ will not actually need to be two-dimensional, the only requirement is that their last index must iterate over coordinates as in section \ref{sub:geostat}. The exception to this rule is if $x$ and $y$ are only one dimensional, in which case they are assumed to have only one coordinate. These high-level functions will regularize $x$ and $y$ before passing them into your distance and covariance functions.

User-supplied covariance functions should be expressed in their simplest forms: their amplitude and input scalings (`sill' and `range', when applicable) should be set to 1. No `nugget' parameter should be used. Numerically singular covariances are helpful when using \texttt{Covariance} objects, because they have low-rank Cholesky factors that can be used for efficient computation. The effect of a nugget can be obtained by adding iid normal variables to realizations. \texttt{FullRankCovariance} objects take a nugget argument regardless of their underlying covariance functions.

Distance functions should be symmetric, meaning the distance from point $x$ to point $y$ is the same as the distance from $y$ to $x$. Since raw covariance functions overwrite distance matrices, the output of an \texttt{apply_distance}-generated covariance function will be symmetric if its distance function is symmetric.

To be a valid covariance function, an \texttt{apply_distance}-generated covariance function $C$ must be positive semidefinite, meaning $C(x,x)$ is a positive semidefinite matrix for any mesh $x$. The formal test for positive semidefiniteness is Bochner's theorem \cite{stein}, but you can also just check the eigenvalues of $C(x,x)$ for a variety of meshes $x$.



\subsection{Summary}\label{sec:cookbook}
\begin{description}
    \item[To use a new functional form:] Write a new raw covariance function and wrap it in a \class{covariance_function_bundle} object.
    \item[To use a new coordinate system:] Write a new distance function and use the \texttt{add_distance_metric} method of an existing \class{covariance_function_bundle}.
    \item[To build in anisotropy/ nonstationarity using a deformation approach] like that of Sampson and Guttorp \cite{sampson}, you can either write a new distance function and use \texttt{add_distance_metric} or you can actually implement the deformation before passing in the $x$ and $y$ arguments, possibly using PyMC \class{Deterministics}.
    \item[To use a new functional form \emph{and} a new distance function:] Wrap the covariance function as a \class{covariance_function_bundle} and apply the \texttt{add_distance_metric} method to the distance function, or just apply \texttt{apply_distance} to both of them at once.
\end{description}

The easiest way to implement a more advanced extension like that of Paciorek and Schervish \cite{pachische} , is to write a new wrapper like \texttt{apply_distance}. You'll probably still be able to take advantage of the raw covariance functions, but not the distance functions. If you do this and don't mind sharing your work, please email me.

% chapter adv (end)


\chapter{Numerical utilities}\label{cha:numerics} % (fold)

More detail on these algorithms, as well as the internal workings of the mean, covariance and realization objects and the observe function, are given in the algorithm documentation.

In addition to numpy's linear algebra support, this package uses some Fortran subroutines wrapped using \citetitle[http://cens.ioc.ee/projects/f2py2e/]{f2py}. They require \citetitle[http://www.netlib.org/blas/]{BLAS} and \citetitle[http://www.netlib.org/lapack/]{LAPACK} libraries (eg, \citetitle[http://math-atlas.sourceforge.net/]{ATLAS}) on your system, and the \file{setup.py} script will attempt to find these. If you don't have optimized BLAS and LAPACK installed, it's a good idea to install them; this package and numpy in general will be much faster. Some operating systems (such as Mac OS X) ship with optimized BLAS and LAPACK libraries included.

\section{Low-level linear algebra utilities}
The following low-level linear algebra utility functions are found in the Fortran files \file{linalg_utils.f} and \file{incomplete_chol.f} :
\begin{description}

    \item[\function{dtrsm_wrap(a,b,uplo='U',transa='N',alpha=1.)}:] A wrapper for the BLAS routine \citetitle[http://www.netlib.org/blas/dtrsmf.f]{DTRSM}, which solves triangular systems \texttt{$\alpha$a$^{op}$ x = b} in-place (that is, \texttt{b} is overwritten by $x$). 
    \begin{itemize}
        \item If \texttt{uplo='U'}, \texttt{a} is assumed to be an upper triangle; if \texttt{uplo='L'}, \texttt{a} is assumed to be a lower triangle.
        \item If \texttt{transa='T'}, \texttt{$\alpha$a.T*x = b} is solved. If \texttt{transa='N'}, \texttt{$\alpha$a*x = b} is solved.
    \end{itemize}
    
    \item[\function{dtrmm_wrap(a,b,uplo='U',transa='N',alpha=1.)}:] A wrapper for the BLAS routine \citetitle[http://www.netlib.org/blas/dtrsmf.f]{DTRMM}, which does the triangular matrix multiplication \texttt{$\alpha$a$^{op}$x = b} in-place (that is, \texttt{b} is overwritten by $x$).
    \begin{itemize}
        \item If \texttt{uplo='U'}, \texttt{a} is assumed to be an upper triangle; if \texttt{uplo='L'}, \texttt{a} is assumed to be a lower triangle.
        \item If \texttt{transa='T'}, \texttt{$\alpha$a.T*x} is computed. If \texttt{transa='N'}, \texttt{$\alpha$a*x} is computed.
    \end{itemize}
    
    \item[\function{info = dpotrf_wrap(a)}:] A wrapper for the LAPACK routine \citetitle[http://www.netlib.org/lapack/double/dpotrf.f]{DPOTRF}, which tries to overwrite positive-definite matrix \texttt{a} with an upper-triangular Cholesky factor in-place. If the return value \texttt{info} is positive, \texttt{a} is not positive definite and the algorithm has failed.
    
    \item[\function{U, m, piv = ichol(diag, reltol, rowfun)}:] An implementation of the incomplete Cholesky decomposition, based on a port of one of the functions in the \citetitle[http://www.kyb.tuebingen.mpg.de/bs/people/seeger/]{chol_incomplete} package by Matthias Seeger. 
    
The arguments are:
    \begin{description}
        \item[\texttt{diag}:] The diagonal of an \texttt{n} by \texttt{n} covariance matrix C.
        \item[\texttt{reltol}:] If the ratio of the \texttt{i}'th pivot to the maximum pivot is found to be less than \texttt{reltol}, the \texttt{i}'th pivot is assumed to be zero.
        \item[\texttt{rowfun}:] A Python function. The call \function{rowfun(i,p,rowvec)} should perform the update \texttt{rowvec[i,i+1:]=C[i,p[i+1:]]} in-place.
    \end{description}

The outputs are:
    \begin{description}
        \item[M:] The rank of C. Note that M$\le$\texttt{n}.
        \item[\texttt{piv}:] A length-\texttt{n} vector of pivots.
        \item[\texttt{U}:] An M-by-\texttt{n} upper-triangular matrix that satisfies \texttt{U[:,argsort(piv)].T * U[:,argsort(piv)] = C}.
    \end{description}

Because the full matrix $C$ does not need to be computed ahead of time, the algorithm is able to factor $C$ in $O(m^2 n)$ operations \cite{incompchol}. The algorithm is implemented in Fortran, but a Python version would be as follows:

\begin{verbatim}

def swap(vec,i,j):
    temp = vec[i]
    vec[i] = vec[j]
    vec[j] = temp

def ichol(diag, reltol, rowfun):

    piv = arange(n)
    U = zeros((n,n),dtype=float).view(matrix)
    rowvec = zeros(n,dtype=float)

    for i in range(n):
        l = diag[i:].argmax()
        maxdiag = diag[l]
    
        if maxdiag < reltol:
            m=i
            return U[:m,:], m, piv

        swap(diag,i,l)
        swap(p,i,l)
    
        temp = U[:i,i]
        U[:i,i] = U[:i,l]
        U[:i,l] = temp
    

        U[i,i] = sqrt(diag[i])
        rowvec[i:] = C[i,piv[i+1:]]
    
        if i > 0:
            rowvec -= U[:i,i].T * U[:i,i+1:]
        
        U[i,i+1:] = rowvec[i+1:] / U[i,i]
        diag[i+1:] -= U[i,i+1:].view(ndarray) ** 2
    
    m=n
    return U, m, piv
\end{verbatim}

Function \function{ichol} is wrapped by the \class{Covariance} method \method{cholesky}.

\item[\function{m, piv = ichol_continue(U, diag, reltol, rowfun, piv)}:]
This function computes the Cholesky factorization of an \texttt{n} by \texttt{n} covariance matrix C from the factor of its upper-left \texttt{n}$_*$ by \texttt{n}$_*$ submatrix C$_*$. Its input arguments are as follows:
\begin{description}
    \item[\texttt{U}:] Unlike \function{ichol}, this function overwrites a matrix in-place. Suppose the Cholesky factor of C$_*$, \texttt{U}$_*$, is of rank M$_*$. On input, \texttt{U} must be an \texttt{[m + (n-n$_*$)]}-by-\texttt{n} matrix arranged like this:
    \begin{eqnarray*}
        \left[
        \begin{array}{ccc}
            \texttt{U}_*\texttt{[:,:m}_*\texttt{]} & \texttt{U}_*\texttt{[:,:m}_*\texttt{].T.I C[:m}_*\texttt{,n}_*\texttt{:]} & \texttt{U}_*\texttt{[:,m}_*\texttt{:]}\\
            \texttt{0}&\texttt{0}&\texttt{0}
        \end{array}
        \right]
    \end{eqnarray*}
On exit, \texttt{U} will be an M-by-\texttt{n} upper-triangular matrix that satisfies \texttt{U[:,argsort(piv)].T * U[:,argsort(piv)]=C}.
    \item[\texttt{piv}:] Denote by \texttt{piv}$_*$ the pivot vector associated with \texttt{U}$_*$ On input, \texttt{piv} must be a length-\texttt{n} vector laid out like this:
    \begin{eqnarray*}
        \begin{array}{ccc}
            [\texttt{piv}_*\texttt{[:m}_*\texttt{]} & \texttt{arange(n-n$_*$)} & \texttt{piv}_*\texttt{[m}_*\texttt{:]}]
        \end{array}
    \end{eqnarray*}
    \item[\texttt{diag}:] The length \texttt{n-n}$_*$ diagonal of \texttt{C[n$_*$:,n$_*$]}.
    \item[\texttt{rowfun}:] The call \function{rowfun(i,p,rowvec)} should perform the update \texttt{rowvec[i,i+1:]} \texttt{=} \texttt{C[i,p[i+1:]]} in-place.
\end{description}

    The input parameter \texttt{reltol}, as well as the output parameters \texttt{piv} and M and the updated matrix \texttt{U}, should be interpreted just like their counterparts in \function{ichol}. 
    
    The algorithm is just like the algorithm in ichol, but the index \texttt{i} iterates over \texttt{range(m$_*$,m$_*$+n-n$_*$)} (and the index of \texttt{diag} is downshifted appropriately).
    
    \function{ichol_continue} is wrapped by the \class{Covariance} method \method{continue_cholesky}, so it should rarely be necessary to call it directly. 
    
    \item[\function{U, m, piv = ichol_full(c, reltol)}:] Just like \texttt{ichol}, but instead of a diagonal and a `get-row' function a full covariance matrix C is required as an input.
    
    \item[\function{U, m, piv = ichol_full(basis, nug, reltol)}:] Just like \texttt{ichol}, but the following arguments are required:
    \begin{description}
        \item[\texttt{basis}:] The evaluation of a basis function on the mesh, multiplied by the Cholesky factor of the coefficients' covariance matrix. This is itself a square root of the covariance matrix, but running it through \texttt{ichol_basis} allows for observations with nonzero variance and essentially pivots for better accuracy even in the zero-variance case.
        \item[\texttt{nug}:] A vector that will effectively be added to the diagonal of the covariance matrix.
    \end{description}
\end{description}

\section{High-level linear algebra utilities}
The following high-level linear algebra utilities are found in \file{GPutils.py}:
\begin{description}

    \item[\function{x = trisolve(A,b,uplo='U',transa='N',inplace=False)}:] Solves the triangular system \texttt{$\alpha$a$^{op}$ x = b} using \function{dtrsm_wrap} and returns $x$. If \texttt{A} is found to be singular, an error is raised. 
    \begin{itemize}
        \item \texttt{A} is assumed upper-triangular if \code{uplo='U'} and lower-triangular if \code{uplo='L'}.
        \item If \texttt{transa='T'}, \texttt{$\alpha$a.T*x = b} is solved. If \texttt{transa='N'}, \texttt{$\alpha$a*x = b} is solved.
        \item If \texttt{inplace=True}, \texttt{b} is overwritten with $x$ in-place and returned. If \texttt{inplace=False}, \texttt{b} is not modified.
    \end{itemize}

    \item[\function{b = trimult(A,x,uplo='U',transa='N',inplace=False)}:] Does the triangular multiplication \texttt{$\alpha$a$^{op}$x = b} using \function{dtrmm_wrap} and returns \texttt{b}.
    \begin{itemize}
        \item \texttt{A} is assumed upper-triangular if \code{uplo='U'} and lower-triangular if \code{uplo='L'}.
        \item If \texttt{transa='T'}, \texttt{$\alpha$a.T8x} is computed. If \texttt{transa='N'}, \texttt{$\alpha$a*x} is computed.
        \item If \texttt{inplace=True}, $x$ is overwritten with \texttt{b} in-place and returned. If \texttt{inplace=False}, $x$ is not modified.
    \end{itemize}    
\end{description}
        
    % \item[\function{observe(M, C, obs_mesh, obs_vals, obs_V, lintrans, cross_validate = True)}:] This function calls \texttt{C.observe}, calls \texttt{M.observe} with the output, and then if \texttt{cross_validate=True} calls \texttt{predictive_check}. 
    
%     If the return of \texttt{predictive_check} value is \texttt{False}, the values of some observations can be predicted with negligible uncertainty from the values of others, but they don't match their predicted values; in other words, the data are extremely improbable. A \texttt{ZeroProbability} exception is raised with some helpful suggestions if this is the case.
%     
%     \item[\function{OK = predictive_check(obs_vals, obs_mesh, M, posdef_indices, tolerance)} :] Checks the value of M evaluated at \texttt{obs_mesh} sliced at the complement of \texttt{posdef_indices} against the corresponding \texttt{obs_vals}. If any of the differences are greater than \texttt{tolerance}, \texttt{False} is returned. Otherwise \texttt{True} is returned.
% \end{description}
% \section{Object internals}\label{sec:internals}
% 
% The methods of \class{Mean}, \class{Covariance} and \class{Realization} will be described here. 
% 
% \section{Covariance}\label{sec:covarianceInt}
% \begin{description}
%     
%     \item[\method{cholesky(x, apply_pivot = True, observed=True)}:] Returns an incomplete Cholesky factor of \texttt{C(x,x)}. The other arguments' meanings are as follows:
%     \begin{description}
%         \item[\texttt{apply_pivot}:] If \texttt{True}, the return value is a matrix \texttt{U} such that \texttt{U.T*U = C(x,x)}. If \texttt{False}, the return value is a dictionary. Element \texttt{'pivots'} is a vector of pivots returned by function \function{ichol}, and element \texttt{'U'} is the matrix \texttt{sig} returned by \function{ichol}.
%         \item[\texttt{observed}:] If \texttt{True}, the matrix \texttt{C(x,x)} is obtained conditional on observations that have been made involving C. If \texttt{False}, the matrix is obtained without regard to those observations.
%     \end{description}
%     
%         \item[\method{continue_cholesky(x, x_old, chol_dict_old[, apply_pivot, observed])}:] Returns an incomplete Cholesky factor of \texttt{C(concatenate(x_old,x), concatenate(x_old,x))}. Arguments \texttt{apply_pivot} and \texttt{observed} are the same as for \texttt{cholesky}. \texttt{chol_dict_old} should be the incomplete Cholesky factorization of \texttt{C(x_old, x_old)} in the form of a dictionary such as the one produced by \texttt{C.cholesky}. 
% 
%         This method returns either a matrix or a dictionary depending on the value of \texttt{apply_pivot}. See \texttt{cholesky}.
% 
% 
%         \item[\method{relevant_slice, obs_mesh_new, U_for_draw = observe(obs_mesh, obs_V=0.)}:] All subsequent calls will be made under the assumption that observations of the random field at inputs \texttt{obs_mesh} have been made with variance \texttt{obs_V}. This method updates the following attributes of C:
%         \begin{itemize}
%             \item \texttt{observed}: This flag is set to \texttt{True}.
%             \item \texttt{obs_U} and \texttt{obs_piv}: The output of \texttt{self.cholesky(obs_mesh, apply_pivot=False, observed=False)}.
%             \item \texttt{obs_mesh}: The mesh on which self has been observed, sliced by \texttt{obs_piv[:m]}, where M is the rank of \texttt{obs_U}.
%             \item \texttt{obs_V}: The variances associated with the observations, sliced as \texttt{obs_mesh} is.
%             \item \texttt{obs_len}: The length of \texttt{obs_V}.
%         \end{itemize}
%         
%         The return values are:
%         \begin{itemize}
%             \item \texttt{relevant_slice}: The indices included in the incomplete Cholesky factorization. These correspond to the values of \texttt{obs_mesh} that determine the other values, but not one another.
%             \item \texttt{obs_mesh_new}: \texttt{obs_mesh} sliced according to \texttt{relevant_slice}.
%             \item \texttt{U_for_draw}: An upper-triangular Cholesky factor of \texttt{self}'s evaluation on \texttt{obs_mesh} conditional on all previous observations.
%         \end{itemize}
%         
% 
%     
%     \item[\method{__call__(x[, y, observed])}:] 
% 
%     \begin{itemize}
%         \item If only one argument $x$ is provided, \texttt{C(x,x) = V(f(x))} will be returned conditional on any observations that have been made.
%         \begin{itemize}
%             \item If C is unobserved or \texttt{observed = False}, its underlying function C is evaluated at each element of $x$. The \texttt{i}'th element of the return value is \texttt{c(x_i,x_i)}.
%             \item If C is observed and \texttt{observed = True}, its underlying function C is evaluated at each element of $x$. The \texttt{i}'th element of the return value is 
%             \begin{eqnarray*}
%                 \texttt{c(x_i,x_i) - (U.T.I c(o,x_i)).T (U.T.I c(o,x_i))},
%             \end{eqnarray*}
%             where \texttt{o} is \code{self.obs_mesh} and \texttt{U} is \texttt{self.obs_U}, both of which were provided by \function{observe}.
%             \end{itemize}
%                 
%         \item If two arguments \texttt{(x,y)} are provided, \texttt{C(x,y)} will be returned conditional on any observations that have been made. 
%         \begin{itemize}
%             \item If C is unobserved, \texttt{c(x,y)} will be returned where C is the underlying function. If $x$ and $y$ are references to the same array, only half the matrix will actually be computed; the other half will be filled in.
%         \item If C is observed, the return value will be
%         \begin{eqnarray*}
%             \texttt{c(x,y) - (U.T.I c(o,x)).T (U.T.I c(o,y))}
%         \end{eqnarray*}
%         where again \texttt{o} is \code{self.obs_mesh} and \texttt{U} is \texttt{self.obs_U}.
%         \end{itemize}
%         
%     \end{itemize}
%     \end{description}
% 
% \section{BasisCovariance}\label{sec:basisCovariance}
% \class{BasisCovariance} has methods and attributes that act just like those of \texttt{Covariance}, but of course the internal computations are different and tend to be faster.
% 
% \section{Mean}\label{sec:meanInt}
% \begin{description}
%     \item[\method{observe(C, obs_mesh_new, obs_vals_new)}:] All subsequent calls will be made under the assumption that observations of the random field at inputs \texttt{obs_mesh} have been made with variance \texttt{obs_V}. Assumes that \texttt{C(obs_mesh_new, obs_mesh_new)} is positive definite; the \emph{function} \function{observe} slices \texttt{obs_vals} according to \texttt{Covariance.observe}'s output value \texttt{relevant_slice} before passing it to \texttt{Mean.observe}. This method updates the following attributes of M:
%     \begin{itemize}
%         \item \texttt{self.C}: The covariance of the random field (which has self as mean) that was observed. Several of $C$'s attributes are used by this method.
%         \item \texttt{self.obs_U}: \texttt{C.obs_U}.        
%         \item \texttt{self.obs_mesh}: \texttt{C.obs_mesh}.
%         \item \texttt{obs_V}: \texttt{C.obs_V}.
%         \item \texttt{obs_len}: \texttt{C.obs_len}.
%         \item \texttt{self.dev}: \texttt{obs_mesh - self.__call__(obs_mesh, observed=False)}.
%         \item \texttt{self.reg_vec}: \texttt{self.obs_U[:,:m].T*self.dev}, where M is the rank of \texttt{self.obs_U}.
%     \end{itemize}
%     
%     Note that \texttt{Mean.observe} will not cross-validate; if inconsistent observation values are used, it will simply ignore them. You'll need to call \texttt{predictive_check} yourself if you want cross-validation.
% 
%     \item[\method{__call__(x[, observed])}:] 
%     \begin{itemize}
%         \item If M has not been observed or \texttt{observed = False}, the return value of \texttt{M(x)} will just be \texttt{m(x)} where M is the underlying mean function. 
%         \item If M has been observed and \texttt{observed = True}, the return value will be 
%         \begin{eqnarray*}
%             \texttt{c(x,o) U.I r},
%         \end{eqnarray*}
%         where \texttt{r} is \code{self.reg_vec}, \texttt{U} is \code{self.obs_U} and C is \code{self.cov_fun}, all of which were provided by \function{observe}. 
% 
%     \end{itemize}
%      
% \end{description}

% \section{Realization}\label{sec:realInt}
% A realization f maintains the following attributes:
% \begin{itemize}
%     \item \texttt{x_sofar}: The values at which self has already been evaluated.
%     \item \texttt{f_sofar}: Self's evaluation at \texttt{x_sofar}.
%     \item \texttt{M_internal}: An observed copy of self's mean.
%     \item \texttt{C_internal}: An observed copy of self's covariance. 
% \end{itemize}
% 
% When the evaluation \texttt{f(x)} is requested, the following happen:
% \begin{enumerate}
%     \item \texttt{x_sofar} is searched for elements of $x$. If any are found, the corresponding values of f aren't recomputed. Call the remaining elements \texttt{x_new}.
%     \item \texttt{C_internal} is observed at \texttt{x_new} with \texttt{obs_V=0}.
%     \item The output value \texttt{U_for_draw} is used to generate a normal random variable \texttt{f_new} with mean \texttt{M_internal(x_new)} and covariance \texttt{C_internal(x_new, x_new)}.
%     \item \texttt{M_internal} is observed with \texttt{obs_mesh = x_new}, \texttt{obs_vals = f_new}, \texttt{obs_taus = Inf}.
%     \item \texttt{f_new} is appropriately combined with f's evaluation at previously computed values of $x$ and the result is returned.
% \end{enumerate}
% 
% If a realization's covariance is an instance of \texttt{BasisCovariance}, the behavior is different. At instantiation, the realization will draw coefficients for the basis terms; calls to the realization will simply be handled by calling the basis.

% chapter numerics (end)

\chapter{Wishlist}\label{cha:wishlist}
\section{Features}
\begin{itemize}
    \item Linear transformations.
    \begin{itemize}
        \item Create new GP's from old GP's via linear transformations. Maybe try to make a \class{LinearOperator} class.
        \item Observe linear transformations of evaluations of GP.
    \end{itemize}
    \item Observations can be dependent (\texttt{obs_V} can be square rather than a diagonal).
    \item Specific support for anisotropy.
    \item Specific support for spline covariances.
    \item Specific support for vector-valued GPs (cokriging).
\end{itemize}

\section{Optimizations}
\begin{itemize}
    \item Markov random field covariances, which take advantage of the sparseness of the covariance for speed.
    \item Speed up realization calls with just one value. All the array manipulation really bogs this case down, and this makes dynamical/autoregressive applications slow.
    \begin{itemize}
        \item There isn't a clear bottleneck, unfortunately. It might be necessary to write all the call methods in f2py or Pyrex.
    \end{itemize}
\item \function{ichol} and friends:
\begin{itemize}
    \item Don't allocate full-rank memory up-front.
    \item Use a symbolic Cholesky decomposition to save some work.
    \item Increase granularity of BLAS calls to take better advantage of threaded libraries.
    \item Distributed memory.
\end{itemize}
\end{itemize}

\section{Other} 
\begin{itemize}
    \item Clean up the return values from \class{Covariance.observe}.
    \item Try to figure out relative performance of incomplete Cholesky w/low-rank covariances to basis covariances, and relative performance of sparse Cholesky to Markov Random Field covariances.
\end{itemize}

% section new_features (end)

% \chapter{Quick reference}\label{cha:reference}

\nocite{*} 
\bibliographystyle{plain} 
\bibliography{gp} 

\end{document}
