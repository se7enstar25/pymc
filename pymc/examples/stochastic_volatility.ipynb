{
 "metadata": {
  "name": "stochastic_volatility"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.pylab import *\n",
      "import numpy as np\n",
      "from pymc import  *\n",
      "from pymc.distributions.timeseries import *\n",
      "\n",
      "from scipy.sparse import csc_matrix\n",
      "from scipy import optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Asset prices have time-varying volatility (variance of day over day `returns`). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21.\n",
      "\n",
      "$$ \\sigma \\sim Exponential(50) $$\n",
      "\n",
      "$$ \\nu \\sim Exponential(.1) $$\n",
      "\n",
      "$$ s_i \\sim Normal(s_{i-1}, \\sigma^{-2}) $$\n",
      "\n",
      "$$ log(\\frac{y_i}{y_{i-1}}) \\sim t(\\nu, 0, exp(-2 s_i)) $$\n",
      "\n",
      "Here, $y$ is the daily return series and $s$ is the latent log volatility process."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Build Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we load some daily returns of the S&P 500."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 400\n",
      "returns = np.genfromtxt(\"data/SP500.csv\")[-n:]\n",
      "returns[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([-0.00637 , -0.004045, -0.02547 ,  0.005102, -0.047733])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specifying the model in pymc mirrors its statistical specification. \n",
      "\n",
      "However, it is easier to sample the scale of the log volatility process innovations, $\\sigma$, on a log scale, so we create it using `TransformedVar` and use `logtransform`. `TransformedVar` creates one variable in the transformed space and one in the normal space. The one in the transformed space (here $\\text{log}(\\sigma) $) is the one over which sampling will occur, and the one in the normal space is the one to use throughout the rest of the model.\n",
      "\n",
      "It takes a variable name, a distribution and a transformation to use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = Model()\n",
      "with model: \n",
      "    sigma, log_sigma = model.TransformedVar('sigma', Exponential.dist(1./.02, testval = .1),\n",
      "                                            logtransform)\n",
      "\n",
      "    nu = Exponential('nu', 1./10)\n",
      "\n",
      "    s = GaussianRandomWalk('s', sigma**-2, shape = n)\n",
      "\n",
      "    r = T('r', nu, lam = exp(-2*s), observed = returns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Fit Model\n",
      "\n",
      "To get a decent scaling matrix for the Hamiltonian sampler, we find the Hessian at a point. The method `Model.d2logpc` gives us a `Theano` compiled function that returns the matrix of 2nd derivatives.\n",
      "\n",
      "However, the 2nd derivatives for the degrees of freedom parameter, `nu`, are negative and thus not very informative and make the matrix non-positive definite, so we replace that entry with a reasonable guess at the scale. The interactions between `log_sigma`/`nu` and `s` are also not very useful, so we set them to zero.\n",
      "\n",
      "The Hessian matrix is also sparse, so we can get faster sampling by using a sparse scaling matrix. If you have `scikits.sparse` installed, convert the Hessian to a csc matrixs by uncommenting the appropriate line below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this model, the full maximum a posteriori (MAP) point is degenerate and has infinite density. However, if we fix `log_sigma` and `nu` it is no longer degenerate, so we find the MAP with respect to the volatility process, 's', keeping `log_sigma` and `nu` constant at their default values. \n",
      "\n",
      "We use L-BFGS because it is more efficient for high dimensional functions (`s` has n elements)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with model:\n",
      "    start = find_MAP(vars=[s], fmin = optimize.fmin_l_bfgs_b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We do a short initial run to get near the right area, then start again using a new Hessian at the new starting point to get faster sampling due to better scaling. We do a short run since this is an interactive example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with model: \n",
      "    h = guess_scaling(start)\n",
      "    step = NUTS(model.vars, h)\n",
      "    trace = sample(200, step, start, trace = model.vars + [sigma]) \n",
      "\n",
      "    # Start next run at the last sampled position.\n",
      "    start2 = trace.point(-1)\n",
      "    guess_scaling(start2)\n",
      "    step = NUTS(model.vars, h)\n",
      "    trace = sample(2000, step, trace=trace) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "PositiveDefiniteError",
       "evalue": "Scaling is not positive definite. Simple check failed. Diagonal contains negatives. Check indexes [  5   6   7   8  12  13  14  15  17  18  19  20  22  24  25  27  28  30\n  32  33  35  36  43  45  47  48  49  50  52  53  54  55  56  57  58  61\n  62  64  67  68  70  71  73  79  81  85  87  88  92  94  96  99 100 101\n 103 104 107 109 110 111 112 114 119 120 121 122 126 127 129 130 132 133\n 135 136 140 142 146 152 154 157 158 159 160 162 164 166 167 168 173 174\n 175 177 178 181 184 185 186 192 193 194 196 197 198 199 200 207 209 210\n 211 213 215 217 219 220 221 223 225 229 232 235 236 239 240 241 242 248\n 252 253 254 256 257 258 263 266 269 271 277 278 282 284 287 288 289 290\n 294 296 300 304 308 310 312 313 314 316 317 318 325 326 327 328 329 330\n 331 333 334 341 342 343 346 348 350 354 357 362 364 366 368 369 370 372\n 373 374 375 379 381 383 387 389 394 395 396 399]",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPositiveDefiniteError\u001b[0m                     Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-241e87783422>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguess_scaling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvars\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Start next run at the last sampled position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/john/Documents/workspace/pymc/pymc/step_methods/nuts.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vars, C, step_scale, is_cov, state, Emax, target_accept, gamma, k, t0, model)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_scale\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m4.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpotential\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquad_potential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_cov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_cov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/john/Documents/workspace/pymc/pymc/step_methods/quadpotential.py\u001b[0m in \u001b[0;36mquad_potential\u001b[1;34m(C, is_cov, as_cov)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mQuadPotential_SparseInv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mpartial_check_positive_definite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_cov\u001b[0m \u001b[1;33m!=\u001b[0m  \u001b[0mas_cov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/john/Documents/workspace/pymc/pymc/step_methods/quadpotential.py\u001b[0m in \u001b[0;36mpartial_check_positive_definite\u001b[1;34m(C)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         raise PositiveDefiniteError(\n\u001b[1;32m---> 58\u001b[1;33m             \"Simple check failed. Diagonal contains negatives\", i)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mPositiveDefiniteError\u001b[0m: Scaling is not positive definite. Simple check failed. Diagonal contains negatives. Check indexes [  5   6   7   8  12  13  14  15  17  18  19  20  22  24  25  27  28  30\n  32  33  35  36  43  45  47  48  49  50  52  53  54  55  56  57  58  61\n  62  64  67  68  70  71  73  79  81  85  87  88  92  94  96  99 100 101\n 103 104 107 109 110 111 112 114 119 120 121 122 126 127 129 130 132 133\n 135 136 140 142 146 152 154 157 158 159 160 162 164 166 167 168 173 174\n 175 177 178 181 184 185 186 192 193 194 196 197 198 199 200 207 209 210\n 211 213 215 217 219 220 221 223 225 229 232 235 236 239 240 241 242 248\n 252 253 254 256 257 258 263 266 269 271 277 278 282 284 287 288 289 290\n 294 296 300 304 308 310 312 313 314 316 317 318 325 326 327 328 329 330\n 331 333 334 341 342 343 346 348 350 354 357 362 364 366 368 369 370 372\n 373 374 375 379 381 383 387 389 394 395 396 399]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figsize(12,6)\n",
      "title(str(s))\n",
      "plot(trace[s][::10].T,'b', alpha=.03);\n",
      "xlabel('time')\n",
      "ylabel('log volatility')\n",
      "\n",
      "figsize(12,6)\n",
      "traceplot(trace, model.vars[:-1]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "1. Hoffman & Gelman. (2011). [The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo](http://arxiv.org/abs/1111.4246). "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}