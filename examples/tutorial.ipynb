{
 "metadata": {
  "name": "tutorial"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.pylab import *\n",
      "from pymc import *\n",
      "import numpy as np \n",
      "from numpy.random import normal, beta\n",
      "import theano \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model\n",
      "-----\n",
      "We consider the following generative model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xtrue = normal(scale = 2., size = 1)\n",
      "ytrue = normal(loc = np.exp(xtrue), scale = 1, size = (2,1))\n",
      "zdata = normal(loc = xtrue + ytrue, scale = .75, size = (2, 20))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we observe `zdata` but not `xtrue` or `ytrue`, so we want to come up with posterior distributions for x and y."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Build Model\n",
      "-----------\n",
      "The `Model` encapsulates a statistical model. It has very simple internals: just a list of unobserved variables (`Model.vars`) and a list of factors which go into computing the posterior density (`Model.factors`) (see model.py for more).\n",
      "\n",
      "The `Var` and `Data` method add unobserved and observed random variables to our model respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = Model()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `Var` method adds an unobserved random variable to the model. `Var` needs the name and prior distribution for the random variable, and optionally the shape of the parameter. \n",
      "It returns a Theano variable which represents (the value of) the random variable we have added to the model.\n",
      "\n",
      "The `Var` method is also very simple (see model.py), it creates a Theano variable, adds it to the model\n",
      "list and adds the likelihood factor to the model's factor list.\n",
      "\n",
      "The distribution classes (see distributions/continuous.py), such as `Normal(mu, tau)` below, take some parameters and have a `logp(value)` method for calculating the likelihood."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with model:\n",
      "    x = Normal('x', mu = 0., tau = 1)\n",
      "    y = Normal('y', mu = exp(x), tau = 2.**-2, shape = (2,1))\n",
      "    z = Normal('z', mu = x + y, tau = .75**-2, observed = zdata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "exp",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-6-1081871c5864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'z'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.75\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: exp"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `Data` method adds an observed random variable to the model. It functions similar to `Var`. `Data` takes the observed data, and the distribution for that data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "   \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fit Model\n",
      "---------\n",
      "We need a starting point for our sampling. The `find_MAP` function finds the maximum a posteriori point (MAP), which is often a good choice for starting point. `find_MAP` uses an optimization algorithm to find the local maximum of the log posterior."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = find_MAP(model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Points in parameter space are represented by dictionaries with parameter names as they keys and the value of the parameters as the values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use Hamiltonian Monte Carlo (HMC) to sample from the posterior as implemented by the `HamiltonianMC` step method class. HMC requires an (inverse) covariance matrix to scale its proposal points. So first we pick one. It is helpful if it approximates the true (inverse) covariance matrix. For distributions which are somewhat normal-like, the hessian matrix (matrix of 2nd derivatives of the log posterior) close to the MAP will approximate the inverse covariance matrix of the posterior.\n",
      "\n",
      "The `approx_hess(model, point)` function works similarly to the find_MAP function and returns the hessian at a given point."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = approx_hess(model, start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we build our step method. `HamiltonianMC` takes a model object, a set of variables it should update and a scaling matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "step = HamiltonianMC(model, model.vars, h)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `sample` function takes a number of steps to sample, a step method, a starting point. It returns a trace, the final state of \n",
      "the step method and the seconds that sampling took."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace, state, t = sample(3e3, step, start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To use more than one sampler, use a CompoundStep which takes a list of step methods. \n",
      "\n",
      "The trace object can be indexed by the variables returning an array with the first index being the sample index\n",
      "and the other indexes the shape of the parameter. Thus the shape of trace[x].shape == (ndraw, 2,1).\n",
      "\n",
      "Pymc3 provides `traceplot` a simple plot for a trace."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(trace[x])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(trace[y][:,:,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "traceplot(trace)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}